{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a97551",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#64e3a1; font-family: arial black; color:#000000; font-size: 300%; text-align: center;\">Importing Dependencies </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "5cdda43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import random \n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a4f64c",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#64e3a1; font-family: arial black; color:#000000; font-size: 300%; text-align: center;\">Web Scraping the AI news from Google Reseach </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am fetching a single URL and returning HTML text\n",
    "def fetch_page(url, session=None, sleep_range=(1, 3)):\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "        \n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    }\n",
    "\n",
    "    resp = session.get(url, headers=headers, timeout=20)\n",
    "    resp.raise_for_status() \n",
    "\n",
    "    # Be polite to the server\n",
    "    time.sleep(random.uniform(*sleep_range))\n",
    "    return resp.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be3a46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below method returns True if the URL looks like a real article page, False if it's a year page, label orcategory page\n",
    "\n",
    "def is_real_article(url: str) -> bool:\n",
    "    parsed = urlparse(url)\n",
    "    path = parsed.path  # e.g. \"/blog/reducing-ev-range-anxiety-how-a-simple-ai-model-can-help/\"\n",
    "\n",
    "    if not path.startswith(\"/blog/\"):\n",
    "        return False\n",
    "    if \"/blog/label/\" in path:\n",
    "        return False\n",
    "    slug = path.rstrip(\"/\").split(\"/\")[-1]\n",
    "    if re.fullmatch(r\"\\d{4}\", slug):\n",
    "        return False\n",
    "\n",
    "    # Typical article slugs contain at least one hyphen\n",
    "    if \"-\" not in slug:\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a Google Research Blog listing page HTML, extracting absolute URLs of individual blog posts\n",
    "def extract_article_links(listing_html, base=\"https://research.google\"):\n",
    "    soup = BeautifulSoup(listing_html, \"html.parser\")\n",
    "    links = []\n",
    "    seen = set()\n",
    "\n",
    "    # Grab all hrefs, I will filter with is_real_article\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "        full_url = urljoin(base, href)\n",
    "\n",
    "        if not is_real_article(full_url):\n",
    "            continue\n",
    "\n",
    "        if full_url not in seen:\n",
    "            seen.add(full_url)\n",
    "            links.append(full_url)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70b6906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Here, I am extracting title and main text content from a Google Research blog post.\n",
    "def parse_article(html, url=None):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    h1 = soup.find(\"h1\")\n",
    "    title = h1.get_text(strip=True) if h1 else (url or \"Untitled\")\n",
    "    article_tag = soup.find(\"article\")\n",
    "    if article_tag is None:\n",
    "        article_tag = soup.find(\"main\") or soup\n",
    "\n",
    "    paragraphs = []\n",
    "    for p in article_tag.find_all(\"p\"):\n",
    "        text = p.get_text(\" \", strip=True)\n",
    "        if text:\n",
    "            paragraphs.append(text)\n",
    "\n",
    "    if paragraphs:\n",
    "        date_pattern = r\"^[A-Z][a-z]+ \\d{1,2}, \\d{4}$\"\n",
    "        if re.match(date_pattern, paragraphs[0]):\n",
    "            paragraphs = paragraphs[1:]\n",
    "\n",
    "    content = \"\\n\\n\".join(paragraphs)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"link\": url,\n",
    "        \"content\": content\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7133d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below function collects real article links from listing pages until we reach max_articles or there are no more.\n",
    "# Scrapper visits each article and extract title and content then save to CSV: title, link, and content\n",
    "\n",
    "def scrape_google_research_blog(\n",
    "    base_url: str = \"https://research.google/blog/\",\n",
    "    max_articles: int = 50,\n",
    "    max_pages: int = 50,\n",
    "    output_csv: str = \"google_research_blog_articles_latest200_clean.csv\",\n",
    "    sleep_range=(1, 3),\n",
    "):\n",
    "\n",
    "    session = requests.Session()\n",
    "    all_links_in_order = []\n",
    "\n",
    "    page = 1\n",
    "    while len(all_links_in_order) < max_articles and page <= max_pages:\n",
    "        if page == 1:\n",
    "            url = base_url\n",
    "        else:\n",
    "            url = f\"{base_url}?page={page}\"\n",
    "\n",
    "        print(f\"\\nFetching listing page {page}: {url}\")\n",
    "        try:\n",
    "            listing_html = fetch_page(url, session=session, sleep_range=sleep_range)\n",
    "            page_links = extract_article_links(listing_html)\n",
    "            if not page_links:\n",
    "                print(\"  No real article links found on this page - stopping.\")\n",
    "                break\n",
    "\n",
    "            print(f\"  Found {len(page_links)} real article links on this page.\")\n",
    "            all_links_in_order.extend(page_links)\n",
    "        except Exception as e:\n",
    "            print(f\"  !! Error fetching listing page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    # Deduplicate while preserving order\n",
    "    deduped_links = list(dict.fromkeys(all_links_in_order))\n",
    "    if len(deduped_links) > max_articles:\n",
    "        deduped_links = deduped_links[:max_articles]\n",
    "\n",
    "    print(f\"\\nTotal unique real-article URLs collected: {len(deduped_links)}\")\n",
    "\n",
    "    articles = []\n",
    "    for i, url in enumerate(deduped_links, start=1):\n",
    "        print(f\"[{i}/{len(deduped_links)}] Fetching article: {url}\")\n",
    "        try:\n",
    "            html = fetch_page(url, session=session, sleep_range=sleep_range)\n",
    "            article_data = parse_article(html, url=url)\n",
    "            articles.append(article_data)\n",
    "        except Exception as e:\n",
    "            print(f\"  !! Error fetching/parsing article: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(articles, columns=[\"title\", \"link\", \"content\"])\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nSaved {len(df)} articles to: {output_csv}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching listing page 1: https://research.google/blog/\n",
      "  Found 12 real article links on this page.\n",
      "\n",
      "Fetching listing page 2: https://research.google/blog/?page=2\n",
      "  Found 13 real article links on this page.\n",
      "\n",
      "Fetching listing page 3: https://research.google/blog/?page=3\n",
      "  Found 13 real article links on this page.\n",
      "\n",
      "Fetching listing page 4: https://research.google/blog/?page=4\n",
      "  Found 13 real article links on this page.\n",
      "\n",
      "Total unique real-article URLs collected: 48\n",
      "[1/48] Fetching article: https://research.google/blog/reducing-ev-range-anxiety-how-a-simple-ai-model-predicts-port-availability/\n",
      "[2/48] Fetching article: https://research.google/blog/real-time-speech-to-speech-translation/\n",
      "[3/48] Fetching article: https://research.google/blog/generative-ui-a-rich-custom-visual-interactive-user-experience-for-any-prompt/\n",
      "[4/48] Fetching article: https://research.google/blog/separating-natural-forests-from-other-tree-cover-with-ai-for-deforestation-free-supply-chains/\n",
      "[5/48] Fetching article: https://research.google/blog/a-new-quantum-toolkit-for-optimization/\n",
      "[6/48] Fetching article: https://research.google/blog/differentially-private-machine-learning-at-scale-with-jax-privacy/\n",
      "[7/48] Fetching article: https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/\n",
      "[8/48] Fetching article: https://research.google/blog/ds-star-a-state-of-the-art-versatile-data-science-agent/\n",
      "[9/48] Fetching article: https://research.google/blog/forecasting-the-future-of-forests-with-ai-from-counting-losses-to-predicting-risk/\n",
      "[10/48] Fetching article: https://research.google/blog/exploring-a-space-based-scalable-ai-infrastructure-system-design/\n",
      "[11/48] Fetching article: https://research.google/blog/accelerating-the-magic-cycle-of-research-breakthroughs-and-real-world-applications/\n",
      "[12/48] Fetching article: https://research.google/blog/toward-provably-private-insights-into-ai-use/\n",
      "[13/48] Fetching article: https://research.google/blog/streetreaderai-towards-making-street-view-accessible-via-context-aware-multimodal-ai/\n",
      "[14/48] Fetching article: https://research.google/blog/how-we-are-building-the-personal-health-coach/\n",
      "[15/48] Fetching article: https://research.google/blog/google-earth-ai-unlocking-geospatial-insights-with-foundation-models-and-cross-modal-reasoning/\n",
      "[16/48] Fetching article: https://research.google/blog/a-verifiable-quantum-advantage/\n",
      "[17/48] Fetching article: https://research.google/blog/a-pictures-worth-a-thousand-private-words-hierarchical-generation-of-coherent-synthetic-photo-albums/\n",
      "[18/48] Fetching article: https://research.google/blog/teaching-gemini-to-spot-exploding-stars-with-just-a-few-examples/\n",
      "[19/48] Fetching article: https://research.google/blog/solving-virtual-machine-puzzles-how-ai-is-optimizing-cloud-computing/\n",
      "[20/48] Fetching article: https://research.google/blog/using-ai-to-identify-genetic-variants-in-tumors-with-deepsomatic/\n",
      "[21/48] Fetching article: https://research.google/blog/coral-npu-a-full-stack-platform-for-edge-ai/\n",
      "[22/48] Fetching article: https://research.google/blog/xr-blocks-accelerating-ai-xr-innovation/\n",
      "[23/48] Fetching article: https://research.google/blog/speech-to-retrieval-s2r-a-new-approach-to-voice-search/\n",
      "[24/48] Fetching article: https://research.google/blog/a-collaborative-approach-to-image-generation/\n",
      "[25/48] Fetching article: https://research.google/blog/introducing-interactive-on-device-segmentation-in-snapseed/\n",
      "[26/48] Fetching article: https://research.google/blog/ai-as-a-research-partner-advancing-theoretical-computer-science-with-alphaevolve/\n",
      "[27/48] Fetching article: https://research.google/blog/the-anatomy-of-a-personal-health-agent/\n",
      "[28/48] Fetching article: https://research.google/blog/towards-better-health-conversations-research-insights-on-a-wayfinding-ai-agent-based-on-gemini/\n",
      "[29/48] Fetching article: https://research.google/blog/afrimed-qa-benchmarking-large-language-models-for-global-health/\n",
      "[30/48] Fetching article: https://research.google/blog/time-series-foundation-models-can-be-few-shot-learners/\n",
      "[31/48] Fetching article: https://research.google/blog/deep-researcher-with-test-time-diffusion/\n",
      "[32/48] Fetching article: https://research.google/blog/sensible-agent-a-framework-for-unobtrusive-interaction-with-proactive-ar-agents/\n",
      "[33/48] Fetching article: https://research.google/blog/making-llms-more-accurate-by-using-all-of-their-layers/\n",
      "[34/48] Fetching article: https://research.google/blog/learn-your-way-reimagining-textbooks-with-generative-ai/\n",
      "[35/48] Fetching article: https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/\n",
      "[36/48] Fetching article: https://research.google/blog/speculative-cascades-a-hybrid-approach-for-smarter-faster-llm-inference/\n",
      "[37/48] Fetching article: https://research.google/blog/smarter-nucleic-acid-design-with-nucleobench-and-adabeam/\n",
      "[38/48] Fetching article: https://research.google/blog/accelerating-scientific-discovery-with-ai-powered-empirical-software/\n",
      "[39/48] Fetching article: https://research.google/blog/how-googles-ai-can-help-transform-health-professions-education/\n",
      "[40/48] Fetching article: https://research.google/blog/a-scalable-framework-for-evaluating-health-language-models/\n",
      "[41/48] Fetching article: https://research.google/blog/from-massive-models-to-mobile-magic-the-tech-behind-youtube-real-time-generative-ai-effects/\n",
      "[42/48] Fetching article: https://research.google/blog/securing-private-data-at-scale-with-differentially-private-partition-selection/\n",
      "[43/48] Fetching article: https://research.google/blog/beyond-billion-parameter-burdens-unlocking-data-synthesis-with-a-conditional-generator/\n",
      "[44/48] Fetching article: https://research.google/blog/enabling-physician-centered-oversight-for-amie/\n",
      "[45/48] Fetching article: https://research.google/blog/achieving-10000x-training-data-reduction-with-high-fidelity-labels/\n",
      "[46/48] Fetching article: https://research.google/blog/insulin-resistance-prediction-from-wearables-and-routine-blood-biomarkers/\n",
      "[47/48] Fetching article: https://research.google/blog/highly-accurate-genome-polishing-with-deeppolisher-enhancing-the-foundation-of-genomic-research/\n",
      "[48/48] Fetching article: https://research.google/blog/mle-star-a-state-of-the-art-machine-learning-engineering-agents/\n",
      "\n",
      "Saved 48 articles to: data/google_research_blog_articles.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reducing EV range anxiety: How a simple AI mod...</td>\n",
       "      <td>https://research.google/blog/reducing-ev-range...</td>\n",
       "      <td>Kostas Kollias, Research Scientist, Google Res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Real-time speech-to-speech translation</td>\n",
       "      <td>https://research.google/blog/real-time-speech-...</td>\n",
       "      <td>Karolis Misiunas, Research Engineer, Google De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Generative UI: A rich, custom, visual interact...</td>\n",
       "      <td>https://research.google/blog/generative-ui-a-r...</td>\n",
       "      <td>Yaniv Leviathan, Google Fellow, Dani Valevski,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Separating natural forests from other tree cov...</td>\n",
       "      <td>https://research.google/blog/separating-natura...</td>\n",
       "      <td>Maxim Neumann, Research Engineer, Google DeepM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A new quantum toolkit for optimization</td>\n",
       "      <td>https://research.google/blog/a-new-quantum-too...</td>\n",
       "      <td>Stephen Jordan and Noah Shutty, Research Scien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Reducing EV range anxiety: How a simple AI mod...   \n",
       "1             Real-time speech-to-speech translation   \n",
       "2  Generative UI: A rich, custom, visual interact...   \n",
       "3  Separating natural forests from other tree cov...   \n",
       "4             A new quantum toolkit for optimization   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://research.google/blog/reducing-ev-range...   \n",
       "1  https://research.google/blog/real-time-speech-...   \n",
       "2  https://research.google/blog/generative-ui-a-r...   \n",
       "3  https://research.google/blog/separating-natura...   \n",
       "4  https://research.google/blog/a-new-quantum-too...   \n",
       "\n",
       "                                             content  \n",
       "0  Kostas Kollias, Research Scientist, Google Res...  \n",
       "1  Karolis Misiunas, Research Engineer, Google De...  \n",
       "2  Yaniv Leviathan, Google Fellow, Dani Valevski,...  \n",
       "3  Maxim Neumann, Research Engineer, Google DeepM...  \n",
       "4  Stephen Jordan and Noah Shutty, Research Scien...  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling the function\n",
    "df_google_research = scrape_google_research_blog(\n",
    "    base_url=\"https://research.google/blog/\",\n",
    "    max_articles=50,\n",
    "    output_csv=\"data/google_research_blog_articles.csv\",\n",
    ")\n",
    "\n",
    "len(df_google_research), df_google_research['content'].nunique()\n",
    "df_google_research.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "db18949e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yossi Matias, Vice President & Head of Google Research\\n\\nFrom earth science to genomics to quantum, we share the latest scientific breakthroughs from Google Research and how today’s powerful AI tools and platforms are accelerating innovation.\\n\\nLast week at our flagship Research@ event in Mountain View, we shared some of Google Research’s latest announcements, from understanding earth to advancements in genomics to advancements in quantum computing. Working collaboratively with colleagues across the company, our teams drive breakthrough research and accelerate real-world solutions for products, businesses, science and society. As research comes to reality, we uncover new research opportunities, driving innovation further and faster. I call this powerful, cyclical relationship between research and real-world impact the magic cycle of research .\\n\\nThis cycle is accelerating significantly these days, propelled by more powerful models, new agentic tools that help accelerate scientific discovery, and open platforms and tools. We see this momentum across domains .\\n\\nLink to Youtube Video\\n\\nAt Research@MTV last week, we highlighted three of our latest breakthroughs: Google Earth AI, DeepSomatic, and Quantum Echoes.\\n\\nEarth AI is a powerful collection of geospatial AI models and reasoning designed to address critical global challenges; it gives users an unprecedented level of understanding about what is happening across the planet.\\n\\nFor years we’ve been developing state-of-the-art geo-spatial AI models including floods , wildfires , cyclones , air quality , pollen , weather nowcasting and long range forecasting , agriculture , population dynamics , AlphaEarth Foundations and mobility . These models, developed by teams across Google, are already helping millions of people worldwide and we keep making progress. We have just expanded access to our new Remote Sensing Foundations and new global Population Dynamics Foundations. And we can now share that our riverine flood models — expanded over the years to cover 700 million people in 100 countries — now provide forecasts covering over 2B people in 150 countries for significant riverine flood events.\\n\\nEarth AI is a Google-wide program building on our long-standing efforts. Our latest research updates to Earth AI integrate and synthesize these vast amounts of real-world imagery, population and environmental data. Using LLMs and their reasoning capabilities, the Earth AI geospatial reasoning agent can understand nuanced concepts and discover correlations across multiple datasets and models. This agent allows users to ask complex questions and receive answers in plain language, making Earth AI capabilities accessible even to non-experts. Users can quickly generate insights from business logic use cases and supply chain management to crisis resilience and international policy.\\n\\nIn our evaluations, Geospatial Reasoning Agent improved responses over baseline models that did not have access to Earth AI models and tools. We share the results in our research blog and our technical report .\\n\\nGoogle Earth with Gemini capabilities will soon be powered by our Earth AI imagery models, enabling users to search for objects in satellite imagery. Plus, our powerful models are now available to trusted testers on Google Cloud. And we continue to hear from our partners about diverse important use cases, including testimonials from Give Directly , McGill and Partners , Cooper/Smith , WPP , WHO AFRO , Planet Labs and Airbus .\\n\\nDeepSomatic , published in Nature Biotechnology , is our newest of many AI tools designed to help the scientific community and health practitioners.\\n\\nDeepSomatic builds on 10 years of genomics research at Google. Since 2015, we’ve been building models like DeepConsensus and DeepVariant to help us better understand the genome. With these models, we’ve helped map human and non-human genomes and used this information to inform our understanding of disease.\\n\\nSome cancers have complex genetic signatures that may make them targets for tailored treatments based on their specific mutations. So, we asked ourselves if we could sequence the genomes of these cancerous cells more precisely. The result, DeepSomatic, is our new open-source AI-powered tool to help scientists and doctors make sense of genetic variants in cancer cells.\\n\\nThe model works by first turning genetic sequencing data into a set of images and then using a convolutional neural network to differentiate between the reference genome, the non-cancer germline variants in that individual, and the cancer-caused somatic variants in the tumor.\\n\\nIdentifying cancer variants could potentially lead to brand-new therapies, and it could help clinicians decide between treatments such as chemotherapy and immunotherapy. Our partners at Children’s Mercy are using it to pinpoint how and why a particular form of cancer affects a patient in order to create personalized cures.\\n\\nDeepSomatic follows other breakthroughs which share the same goal of using AI to help fight cancer. We also just released a 27 billion parameter foundation model for single-cell analysis, C2S-Scale , in collaboration with Google DeepMind. This builds upon our work from earlier this year , in collaboration with Yale, and recently generated a novel hypothesis about cancer cellular behavior. With more clinical tests, this may reveal a promising new pathway for developing therapies to fight cancer.\\n\\nTo accelerate the next exponential wave of scientific discovery, we’re looking to our strategic, long-term investment in quantum computing.\\n\\nOur foundation rests on decades of research, leading to our hardware milestone on the Willow chip in late 2024. This work is supported by Michel Devoret, our Chief Scientist of Quantum Hardware, who together with with former Quantum AI hardware lead John Martinis, and John Clarke of the University of California, Berkeley, became 2025 Physics Nobel Laureates for their research in the 1980s that laid the groundwork for today's superconducting qubits.\\n\\nNow we’ve announced a new verifiable quantum advantage , published in the cover of Nature . Our “ Quantum Echoes ” algorithm runs on our Willow chip 13,000 times faster than the best classical algorithm on one of the world’s fastest supercomputers. It offers a new way to explain interactions between atoms in a real world molecule observed using nuclear magnetic resonance spectroscopy . This is the world’s first algorithm to demonstrate verifiable quantum advantage and points towards practical applications of quantum computing that are beyond the capabilities of classical computers.\\n\\nQuantum computing has the potential to meaningfully advance drug design and help make fusion energy a reality. And given our latest breakthrough, we’re optimistic that we’ll start to see real-world applications within five years.\\n\\nLink to Youtube Video\\n\\nFireside Chat about Quantum AI with James Manyika and Hartmut Neven.\\n\\nWe also shared some of the work across various domains where teams are driving breakthrough research and accelerating real-world solutions. The breadth and depth of the opportunities is ever increasing. Here are a few recent examples.\\n\\nAI co-scientist is a multi-agent AI system built as a virtual scientific collaborator to help scientists generate novel hypotheses and research proposals, and to accelerate scientific and biomedical discoveries. Our new AI-powered empirical software system, a Gemini-backed coding agent, helps scientists write expert-level empirical software. It accelerates the historically slow task of creating custom software to evaluate and iteratively improve scientific hypotheses. This opens the door to a future where scientists can easily, rapidly, and systematically investigate hundreds or thousands of potential solutions to the problems that motivate their research.\\n\\nAMIE , a conversational medical AI agent, demonstrates clinical reasoning and communication on par with primary care physicians in both multimodal and multi-visit settings . As we explore how AMIE may translate to real-world environments, we are testing it under physician oversight , including in a partnership with Beth Israel Deaconess Medical Center to evaluate AMIE with real-world patients.\\n\\nMedGemma , part of our Health AI Developer Foundations ( HAI-DEF ) collection, is Google's most capable open model for multimodal medical comprehension. Since launch MedGemma and HAI-DEF have >1M downloads and >40K unique users.\\n\\nWe continue advancing our research on factuality and grounding for LLMs, including studying how LLMs convey uncertainty , assessing whether LLMs encode more factual knowledge in their parameters than they express in their outputs, and more. We expand to multimodal content - for example, Time-Aligned Captions and our contrastive sequential video diffusion method focus on making scenes in videos visually consistent, helping improve the quality of our image and video models.\\n\\nImproving the efficiency of LLMs remains a high priority goal across the industry. Building on our speculative decoding work which enabled substantial efficiency gains without any compromise on quality, we keep seeing many new approaches , such as our recent speculative cascades . We keep advancing other techniques for efficiency and for energy innovation techniques.\\n\\nAlgorithmic research contributes to new Ads model connecting advertisers to customers, continued research on our large-scale optimisations, enhancements to Google Maps routing and improved voice search in India. Privacy research includes recent advances such as confidential federated analytics , differentially private synthetic data and provably private insights into AI use . We are making progress on TimesFM which has hundreds of millions of queries per month in BigQuery alone, and recently introduced a novel approach using in-context fine-tuning .\\n\\nWe keep exploring new ways to improve learning and education, building on our earlier work on LearnLM , such as Learn Your Way to improve learning efficacy. And we keep exploring AI innovations such as the use of diffusion models for real-time game engines , which inspire new horizons for simulating immersive world environments.\\n\\nLink to Youtube Video\\n\\nAt Research@ Mountain View, Yossi Matias joins Alex Kantrowitz on the Big Technology Podcast to discuss our research efforts in areas like cancer treatment and Quantum.\\n\\nThe magic cycle of research is quickly gaining momentum. This is propelled by more powerful models, by agentic tools like the AI co-scientist and AI-based expert-level empirical software that help accelerate scientific discovery, and open platforms and tools like MedGemma , HAI-DEF and DeepSomatic . Innovation today is happening at unprecedented speed.\\n\\nThe latest advancements point to a world where AI is not just a tool, but an essential partner and collaborator. This partnership is already taking shape in tangible ways, empowering researchers, engineers, healthcare workers, and educators. With humans at the steering wheel, we can leverage AI to bring new ideas to life and take on the challenges that matter most.\\n\\nThis fusion of human ingenuity with the powerful capabilities of AI will fuel further innovation and accelerate its impact for people at a global scale, defining a new era of scientific discovery for the benefit of everyone, everywhere.\\n\\nNovember 18, 2025\\n\\nNovember 13, 2025\\n\\nNovember 13, 2025\""
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_google_research['content'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c0cce",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#64e3a1; font-family: arial black; color:#000000; font-size: 300%; text-align: center;\">Web scraping the AI news from TechCrunch</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d62f255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below method Fetches a single URL and return its HTML text. \n",
    "def fetch_page(url, session=None, sleep_range=(1, 3)):\n",
    "    if session is None:\n",
    "        session = requests.Session()\n",
    "        \n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    }\n",
    "\n",
    "    resp = session.get(url, headers=headers, timeout=20)\n",
    "    resp.raise_for_status() \n",
    "\n",
    "    # Be polite to being blocked\n",
    "    time.sleep(random.uniform(*sleep_range))\n",
    "    return resp.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "4ae7e2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE_URL_RE = re.compile(r\"^/(\\d{4})/(\\d{2})/(\\d{2})/\")\n",
    "\n",
    "def is_article_url(url: str) -> bool:\n",
    "    \"\"\"\n",
    "    True if URL looks like a TechCrunch article, e.g.\n",
    "    https://techcrunch.com/2025/10/31/some-slug/\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    if parsed.netloc not in (\"techcrunch.com\", \"www.techcrunch.com\"):\n",
    "        return False\n",
    "    \n",
    "    return bool(ARTICLE_URL_RE.match(parsed.path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below method extract absolute URLs of individual article pages\n",
    "def extract_article_links(listing_html, base=\"https://techcrunch.com\"):\n",
    "    soup = BeautifulSoup(listing_html, \"html.parser\")\n",
    "    links = []\n",
    "    seen = set()\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"].strip()\n",
    "        full_url = urljoin(base, href)\n",
    "\n",
    "        if not is_article_url(full_url):\n",
    "            continue\n",
    "\n",
    "        if full_url not in seen:\n",
    "            seen.add(full_url)\n",
    "            links.append(full_url)\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below method finds URL of the 'Next' page on a TechCrunch tag listing page and returns absolute URL or None if there is no next page\n",
    "def find_next_page_url(listing_html, base=\"https://techcrunch.com\"):\n",
    "    soup = BeautifulSoup(listing_html, \"html.parser\")\n",
    "\n",
    "    link = soup.find(\"a\", rel=\"next\") #Prefer next if present\n",
    "    if link and link.get(\"href\"):\n",
    "        return urljoin(base, link[\"href\"])\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True): #Fallback\n",
    "        if a.get_text(strip=True).lower() == \"next\":\n",
    "            return urljoin(base, a[\"href\"])\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below method extract title and main article content from a TechCrunch article page\n",
    "\n",
    "def parse_article(html, url=None):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    h1 = soup.find(\"h1\") # Title\n",
    "    title = h1.get_text(strip=True) if h1 else (url or \"Untitled\")\n",
    "\n",
    "    texts = [t.strip() for t in soup.stripped_strings if t.strip()]\n",
    "\n",
    "    try:\n",
    "        i_title = texts.index(title)\n",
    "    except ValueError:\n",
    "        i_title = 0\n",
    "    end = len(texts)\n",
    "    for j in range(i_title + 1, len(texts)):\n",
    "        if texts[j] == \"Topics\":\n",
    "            end = j\n",
    "            break\n",
    "\n",
    "    # Skip a couple of metadata lines right after the title\n",
    "    start = min(i_title + 3, end)\n",
    "\n",
    "    content_blocks = texts[start:end]\n",
    "\n",
    "    content = \"\\n\\n\".join(content_blocks)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"link\": url,\n",
    "        \"content\": content\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87336575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am scrapping the TechCrunch AI tag pages, following 'Next' until we collect 'max_articles' unique article URLs.\n",
    "# For each article, fetch and parse title, clean content. Then, save to CSV: title, link, content.\n",
    "\n",
    "\n",
    "def scrape_techcrunch_ai_tag(\n",
    "    start_url: str = \"https://techcrunch.com/tag/artificial-intelligence/\",\n",
    "    max_articles: int = 50,\n",
    "    output_csv: str = \"techcrunch_ai_articles_clean.csv\",\n",
    "    sleep_range=(1, 3),\n",
    "):\n",
    "\n",
    "    session = requests.Session()\n",
    "    all_article_urls = []\n",
    "    current_url = start_url\n",
    "\n",
    "    # 1) Collect enough article URLs\n",
    "    while len(all_article_urls) < max_articles and current_url:\n",
    "        print(f\"\\nFetching listing page: {current_url}\")\n",
    "        try:\n",
    "            listing_html = fetch_page(current_url, session=session, sleep_range=sleep_range)\n",
    "        except Exception as e:\n",
    "            print(f\"  !! Error fetching listing page: {e}\")\n",
    "            break\n",
    "\n",
    "        page_links = extract_article_links(listing_html)\n",
    "        print(f\"  Found {len(page_links)} article links on this page.\")\n",
    "        all_article_urls.extend(page_links)\n",
    "\n",
    "        if len(all_article_urls) >= max_articles:\n",
    "            break\n",
    "\n",
    "        next_url = find_next_page_url(listing_html)\n",
    "        if not next_url:\n",
    "            print(\"  No next page found - stopping pagination.\")\n",
    "            break\n",
    "\n",
    "        current_url = next_url\n",
    "\n",
    "    # Deduplicate \n",
    "    deduped_urls = list(dict.fromkeys(all_article_urls))\n",
    "    if len(deduped_urls) > max_articles:\n",
    "        deduped_urls = deduped_urls[:max_articles]\n",
    "\n",
    "    print(f\"\\nTotal unique article URLs collected: {len(deduped_urls)}\")\n",
    "\n",
    "    # Fetching each article\n",
    "    articles = []\n",
    "    for i, url in enumerate(deduped_urls, start=1):\n",
    "        print(f\"[{i}/{len(deduped_urls)}] Fetching article: {url}\")\n",
    "        try:\n",
    "            html = fetch_page(url, session=session, sleep_range=sleep_range)\n",
    "            article_data = parse_article(html, url=url)\n",
    "            articles.append(article_data)\n",
    "        except Exception as e:\n",
    "            print(f\"  !! Error fetching/parsing article: {e}\")\n",
    "\n",
    "    # Saving the result\n",
    "    df = pd.DataFrame(articles, columns=[\"title\", \"link\", \"content\"])\n",
    "    df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"\\nSaved {len(df)} articles to: {output_csv}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c84b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching listing page: https://techcrunch.com/tag/artificial-intelligence/\n",
      "  Found 42 article links on this page.\n",
      "\n",
      "Fetching listing page: https://techcrunch.com/tag/artificial-intelligence/page/2/\n",
      "  Found 41 article links on this page.\n",
      "\n",
      "Total unique article URLs collected: 50\n",
      "[1/50] Fetching article: https://techcrunch.com/2025/10/31/meta-bought-1-gw-of-solar-this-week/\n",
      "[2/50] Fetching article: https://techcrunch.com/2025/08/26/how-one-ai-startup-is-helping-rice-farmers-battle-climate-change/\n",
      "[3/50] Fetching article: https://techcrunch.com/2025/08/20/harvard-dropouts-to-launch-always-on-ai-smart-glasses-that-listen-and-record-every-conversation/\n",
      "[4/50] Fetching article: https://techcrunch.com/2025/08/20/meta-to-add-100-mw-of-solar-power-from-u-s-gear/\n",
      "[5/50] Fetching article: https://techcrunch.com/2025/08/04/perplexity-accused-of-scraping-websites-that-explicitly-blocked-ai-scraping/\n",
      "[6/50] Fetching article: https://techcrunch.com/2025/06/04/obvios-stop-sign-cameras-use-ai-to-root-out-unsafe-drivers/\n",
      "[7/50] Fetching article: https://techcrunch.com/2025/06/02/breakneck-data-center-growth-challenges-microsofts-sustainability-goals/\n",
      "[8/50] Fetching article: https://techcrunch.com/2025/05/27/gridcare-thinks-more-than-100-gw-of-data-center-capacity-is-hiding-in-the-grid/\n",
      "[9/50] Fetching article: https://techcrunch.com/2025/05/22/meta-adds-another-650-mw-of-solar-power-to-its-ai-push/\n",
      "[10/50] Fetching article: https://techcrunch.com/2025/04/01/who-are-climate-conscious-consumers-not-who-youd-expect-says-northwind-climate/\n",
      "[11/50] Fetching article: https://techcrunch.com/2025/03/30/data-centers-love-solar-heres-a-comprehensive-guide-to-deals-over-100-megawatts/\n",
      "[12/50] Fetching article: https://techcrunch.com/2025/03/20/nvidia-thinks-ai-can-solve-electrical-grid-problems-caused-by-ai/\n",
      "[13/50] Fetching article: https://techcrunch.com/2025/03/20/solar-notches-another-win-as-microsoft-adds-475-mw-to-power-its-ai-data-centers/\n",
      "[14/50] Fetching article: https://techcrunch.com/2025/03/11/geothermal-could-power-nearly-all-new-data-centers-through-2030/\n",
      "[15/50] Fetching article: https://techcrunch.com/2025/02/25/elevenlabs-is-now-letting-authors-create-and-publish-audiobooks-on-its-own-platform/\n",
      "[16/50] Fetching article: https://techcrunch.com/2025/02/13/data-center-tweaks-could-unlock-76-gw-of-new-power-capacity-in-the-u-s/\n",
      "[17/50] Fetching article: https://techcrunch.com/2025/02/11/youtube-ai-updates-to-include-expansion-of-auto-dubbing-age-identifying-tech-and-more/\n",
      "[18/50] Fetching article: https://techcrunch.com/2025/02/07/self-inspection-raises-3m-for-its-ai-powered-vehicle-inspections/\n",
      "[19/50] Fetching article: https://techcrunch.com/2025/01/31/meta-turns-to-solar-again-in-its-data-center-building-boom/\n",
      "[20/50] Fetching article: https://techcrunch.com/2025/01/27/how-to-switch-off-apple-intelligence-on-your-iphone-ipad-and-mac/\n",
      "[21/50] Fetching article: https://techcrunch.com/2025/01/08/gridwares-boxes-literally-listen-to-power-lines-to-find-outages/\n",
      "[22/50] Fetching article: https://techcrunch.com/2025/01/02/2025-will-be-the-year-climate-tech-learns-to-love-ai/\n",
      "[23/50] Fetching article: https://techcrunch.com/2024/12/20/heres-the-full-list-of-49-us-ai-startups-that-have-raised-100m-or-more-in-2024/\n",
      "[24/50] Fetching article: https://techcrunch.com/2024/12/14/what-are-ai-world-models-and-why-do-they-matter/\n",
      "[25/50] Fetching article: https://techcrunch.com/2024/12/14/why-marc-andreessen-was-very-scared-after-meeting-with-the-biden-administration-about-ai/\n",
      "[26/50] Fetching article: https://techcrunch.com/2024/12/13/exxon-cant-resist-the-ai-power-gold-rush/\n",
      "[27/50] Fetching article: https://techcrunch.com/2024/12/06/if-you-can-make-this-ai-bot-fall-in-love-you-could-win-thousands-of-dollars/\n",
      "[28/50] Fetching article: https://techcrunch.com/2024/12/06/google-photos-launches-a-2024-recap-for-a-look-back-at-this-years-memories/\n",
      "[29/50] Fetching article: https://techcrunch.com/2024/12/05/amp-robotics-raises-91m-to-build-more-robot-filled-waste-sorting-facilities/\n",
      "[30/50] Fetching article: https://techcrunch.com/2024/12/04/meta-jumps-aboard-the-nuclear-powered-data-center-bandwagon/\n",
      "[31/50] Fetching article: https://techcrunch.com/2025/11/27/andurils-autonomous-weapons-stumble-in-tests-and-combat-wsj-reports/\n",
      "[32/50] Fetching article: https://techcrunch.com/2025/11/25/why-hold-forever-investors-are-snapping-up-venture-capital-zombies/\n",
      "[33/50] Fetching article: https://techcrunch.com/2025/11/24/altman-describes-openais-forthcoming-ai-device-as-more-peaceful-and-calm-than-the-iphone/\n",
      "[34/50] Fetching article: https://techcrunch.com/2025/11/24/openai-learned-the-hard-way-that-cameo-trademarked-the-word-cameo/\n",
      "[35/50] Fetching article: https://techcrunch.com/2025/11/24/anthropic-releases-opus-4-5-with-new-chrome-and-excel-integrations/\n",
      "[36/50] Fetching article: https://techcrunch.com/2025/11/24/us-banks-scramble-to-assess-data-theft-after-hackers-breach-financial-tech-firm/\n",
      "[37/50] Fetching article: https://techcrunch.com/2025/11/23/chatgpt-told-them-they-were-special-their-families-say-it-led-to-tragedy/\n",
      "[38/50] Fetching article: https://techcrunch.com/2025/11/24/aws-is-spending-50b-build-ai-infrastructure-for-the-us-government/\n",
      "[39/50] Fetching article: https://techcrunch.com/2025/11/24/doge-days-are-over-as-trump-disbands-elon-musks-team-of-federal-cost-cutters/\n",
      "[40/50] Fetching article: https://techcrunch.com/2025/11/25/spotify-to-raise-us-prices-in-first-quarter-of-next-year-report-says/\n",
      "[41/50] Fetching article: https://techcrunch.com/2025/02/28/tech-layoffs-2024-list/\n",
      "[42/50] Fetching article: https://techcrunch.com/2025/01/28/chatgpt-everything-to-know-about-the-ai-chatbot/\n",
      "[43/50] Fetching article: https://techcrunch.com/2024/11/19/physical-ai-startup-brightai-bootstraps-to-80m-in-revenue/\n",
      "[44/50] Fetching article: https://techcrunch.com/2024/10/31/buddy-ai-is-using-ai-and-gaming-to-help-children-learn-english-as-a-second-language/\n",
      "[45/50] Fetching article: https://techcrunch.com/2024/10/30/clashing-approaches-to-combat-ais-perpetual-bullst-machine/\n",
      "[46/50] Fetching article: https://techcrunch.com/2024/10/29/ashton-kutchers-sound-ventures-backs-fei-fei-lis-world-labs-in-continued-ai-push/\n",
      "[47/50] Fetching article: https://techcrunch.com/2024/10/29/ashton-kutcher-explains-why-hes-betting-on-ai-but-not-trying-to-pick-a-winner/\n",
      "[48/50] Fetching article: https://techcrunch.com/2024/10/24/after-selling-anchor-to-spotify-co-founders-reunite-to-build-ai-educational-startup-oboe/\n",
      "[49/50] Fetching article: https://techcrunch.com/2024/10/24/smashing-an-ai-powered-app-for-online-readers-launches-to-the-public/\n",
      "[50/50] Fetching article: https://techcrunch.com/2024/10/23/vcs-love-using-the-ai-meeting-notepad-granola-so-they-gave-it-20m/\n",
      "\n",
      "Saved 50 articles to: data/techcrunch_ai_articles_clean.csv\n",
      "50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meta bought 1 GW of solar this week</td>\n",
       "      <td>https://techcrunch.com/2025/10/31/meta-bought-...</td>\n",
       "      <td>Meta signed three deals this week to procure n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How one AI startup is helping rice farmers bat...</td>\n",
       "      <td>https://techcrunch.com/2025/08/26/how-one-ai-s...</td>\n",
       "      <td>Fixing climate change is no small task — just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harvard dropouts to launch ‘always on’ AI smar...</td>\n",
       "      <td>https://techcrunch.com/2025/08/20/harvard-drop...</td>\n",
       "      <td>9:00 AM PDT · August 20, 2025\\n\\nTwo former Ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meta to add 100MW of solar power from US gear</td>\n",
       "      <td>https://techcrunch.com/2025/08/20/meta-to-add-...</td>\n",
       "      <td>Meta signed a deal yesterday with solar develo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Perplexity accused of scraping websites that e...</td>\n",
       "      <td>https://techcrunch.com/2025/08/04/perplexity-a...</td>\n",
       "      <td>AI startup Perplexity is crawling and scraping...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                Meta bought 1 GW of solar this week   \n",
       "1  How one AI startup is helping rice farmers bat...   \n",
       "2  Harvard dropouts to launch ‘always on’ AI smar...   \n",
       "3      Meta to add 100MW of solar power from US gear   \n",
       "4  Perplexity accused of scraping websites that e...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://techcrunch.com/2025/10/31/meta-bought-...   \n",
       "1  https://techcrunch.com/2025/08/26/how-one-ai-s...   \n",
       "2  https://techcrunch.com/2025/08/20/harvard-drop...   \n",
       "3  https://techcrunch.com/2025/08/20/meta-to-add-...   \n",
       "4  https://techcrunch.com/2025/08/04/perplexity-a...   \n",
       "\n",
       "                                             content  \n",
       "0  Meta signed three deals this week to procure n...  \n",
       "1  Fixing climate change is no small task — just ...  \n",
       "2  9:00 AM PDT · August 20, 2025\\n\\nTwo former Ha...  \n",
       "3  Meta signed a deal yesterday with solar develo...  \n",
       "4  AI startup Perplexity is crawling and scraping...  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling function to scrap the website and save data as CSV\n",
    "df_tc_ai = scrape_techcrunch_ai_tag(\n",
    "    start_url=\"https://techcrunch.com/tag/artificial-intelligence/\",\n",
    "    max_articles=50,\n",
    "    output_csv=\"data/techcrunch_ai_articles_clean.csv\",\n",
    ")\n",
    "\n",
    "print(len(df_tc_ai))\n",
    "df_tc_ai.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "78cbded3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count       50.000000\n",
       " mean      6438.380000\n",
       " std       9843.552998\n",
       " min       1064.000000\n",
       " 25%       3214.750000\n",
       " 50%       4042.500000\n",
       " 75%       5388.750000\n",
       " max      63460.000000\n",
       " Name: content, dtype: float64,\n",
       " 50)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tc_ai['content'].str.len().describe(), df_tc_ai['content'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eec4f2",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#64e3a1; font-family: arial black; color:#000000; font-size: 300%; text-align: center;\">Text Preprocessing</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "bc45c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_google_research = pd.read_csv('data/google_research_blog_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "95cbd1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tc_ai = pd.read_csv('data/techcrunch_ai_articles_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6459c85e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note!</b> \n",
    "\n",
    "Displaying the complete article to check for unwanted/redundant scrapped data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "b9edadd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing EV range anxiety: How a simple AI model predicts port availability\n",
      "Kostas Kollias, Research Scientist, Google Research\n",
      "\n",
      "We developed a unique model to predict the probability with which an EV charging port will be available at a certain station within a certain amount of minutes from the current time, which helps EV drivers plan their trips efficiently while minimizing waiting time at the charging stations.\n",
      "\n",
      "The transition to electric vehicles (EVs) is accelerating globally, bringing with it the critical need for a reliable and robust charging infrastructure. While building out more physical charging stations is an important step, an equally important task is maximizing the efficiency of this infrastructure and minimizing \"range anxiety”, a term used to describe an EV driver’s fear of running out of battery before reaching their destination or the nearest available charging station. These concerns led us to design an approach for EV routing that reduces range anxiety by integrating charging stations into the navigational route based on the battery level and destination.\n",
      "\n",
      "This week we announced a new lightweight, highly efficient prediction model that can answer the core question, “ What is the probability that an EV charging port will be available at a specific station a certain number of minutes from now? ” We found that the most sophisticated model isn't always the best solution. By co-designing the model and the deployment infrastructure, we were able to create a highly effective prediction system based on a simple linear regression approach. This model’s simplicity is its strength, allowing it to rely on easily accessible features while still achieving performance improvements over a strong baseline. Our work demonstrates that combining intuitive real-world logic with machine learning can deliver significant operational and user experience benefits.\n",
      "\n",
      "Our goal was to maximize predictive power while minimizing the feature set (i.e., the specific, measurable data points the model uses to make a prediction) to ensure speed and low-latency deployment. After testing various architectures, including a decision tree and a simple neural network, a straightforward linear regression model proved to be the most performant and robust for this specific task.\n",
      "\n",
      "We trained the model using real-time availability data from charging networks to calculate the true number of available charging ports within a certain number of minutes from the current observation time using criteria for model features and weights. We uniformly sampled ports from two distinct regions (CA and Germany). Larger stations were more likely to be included in the training set because they see more traffic than isolated ports and more closely reflect real-world usage.\n",
      "\n",
      "The model uses the hour of the day as a key piece of information (a \"feature\"). It treats each hour (or hour range) separately. For example, \"9 AM\" is one feature, and \"5 PM\" is another.\n",
      "\n",
      "The \"weights\" are the specific numerical values that the linear regression algorithm learns during training. These numbers dictate how much each specific hour of the day affects the final prediction.\n",
      "\n",
      "These “hour feature weights\" are the model's learned coefficients that quantify the predictable rate of EV port occupancy change for every hour of the day. Essentially, the model learns to express the difference between the current number of available ports and the future number of available ports as a function of the hour feature weights.\n",
      "\n",
      "The feature weights learned for each hour of the day are particularly insightful because they directly represent the rate at which port occupancy changes. As illustrated by the chart below, there are clear, predictable trends tied to driver schedules:\n",
      "\n",
      "Feature weights for each hour for the 30 minute horizon. They correspond to the rate at which port occupancy changes at each 30 minute bucket.\n",
      "\n",
      "Feature weights for each hour for the 60 minute horizon. They correspond to the rate at which port occupancy changes at each 60 minute bucket.\n",
      "\n",
      "Note that the model only differentiates from the current state when the change rate is significant (e.g., rush hour) or the station is large (more ports amplify the predicted change), which are intuitively the correct times to issue an updated prediction.\n",
      "\n",
      "Our evaluation was designed to be rigorous and representative of real-world usage. For both the 30-minute and 60-minute time horizons, we evaluated predictions on 100 randomly selected stations, sampling their occupancy status 48 times daily (every 30 minutes) for a full week.\n",
      "\n",
      "The model was benchmarked against a remarkably strong baseline: the \"Keep Current State\" approach. This baseline simply assumes that the number of available ports a certain number of minutes ( H ) in the future will be exactly the same as the current number.\n",
      "\n",
      "While simple, this baseline is very hard to beat, especially over short horizons. For example, our data showed that on the US East Coast, never more than 10% of ports change their availability state within a 30-minute block. Since most of the time the state doesn't change, the simplest prediction — no change — is correct most of the time, making the task of adding predictive value extremely difficult.\n",
      "\n",
      "We focused on two key metrics to measure the model’s accuracy for predicting the exact number of free ports: mean squared error (MSE) and mean absolute error (MAE). A ratio of MSE/MAE ≥ 1 free port measures the accuracy of the most critical binary task for the user: “Will I find at least one free port (Yes/No)?”\n",
      "\n",
      "The evaluation confirmed that the linear regression model provides crucial gains over the strong \"Keep Current State\" baseline, primarily by correctly identifying the infrequent, yet vital, moments of high occupancy turnover.\n",
      "\n",
      "We sampled test instances from among stations with at least 6 ports with horizons of 30 to 60 minutes, a realistic set of cases for charging in urban environments. We evaluated the model for the task of predicting the availability of at least one port in a station. This evaluation focused on the station profile and time of day when the model would differentiate from the baseline, namely large stations at times of significant rates of change.\n",
      "\n",
      "The table below presents the fraction of time in which we provide a wrong prediction (which is equivalent to the MAE for this problem) for the times of highest change (8am and 8pm).\n",
      "\n",
      "Comparison of error rates on the availability of at least one free port (30 to 60-Minute Horizon).\n",
      "\n",
      "In summary, deploying the regression model allows us to reduce the number of bad predictions by approximately 20% in morning peak times and by approximately 40% in evening peak times.\n",
      "\n",
      "Further examinations revealed that while the shape of the change rate curve (when ports fill vs. when they empty) is similar across regions, the magnitude of the change is distinct enough to warrant separate models. For instance, training the model separately for regions like California and Germany yielded better performance than pooling all data together, suggesting that it’s necessary to account for unique regional EV usage patterns.\n",
      "\n",
      "We have successfully developed and deployed a lightweight, linear regression model that effectively predicts EV charging port availability. By focusing on simplicity, speed, and co-designing the model with the existing infrastructure, we bypassed the complexity and latency associated with more detailed, but often unscalable, approaches.\n",
      "\n",
      "The resulting model provides a crucial predictive advantage over a strong \"Keep Current State\" baseline, particularly during high-traffic periods. This capability translates directly into an improved user experience: reduced anxiety, smarter routing decisions, and a better overall experience that supports the continued growth of electric mobility. Future work will focus on extending the prediction horizons to provide even greater value for long-distance travel planning.\n",
      "\n",
      "We thank our collaborators Achir Ramadhan, Sreenivas Gollapudi, Shubham Gupta, Ilya Eyzerman, and Ivan Kuznetsov.\n",
      "\n",
      "November 19, 2025\n",
      "\n",
      "November 18, 2025\n",
      "\n",
      "November 13, 2025\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(df_google_research.loc[idx, 'title'])\n",
    "print(df_google_research.loc[idx, 'content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dba835",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note!</b> \n",
    "\n",
    "* I will remove Author name (Unnecessary in my case). So, I will remove 2nd line in general?! I'll verify this by checking other articles.  \n",
    "* Also I plan to remove, 'We thank our collaborators.....' at the end of the article along with unwanted dates. I'll check other articles to see if the same pattern exists over there too!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "42651501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differentially private machine learning at scale with JAX-Privacy\n",
      "Borja Balle, Staff Research Scientist, Google DeepMind, and Ryan McKenna, Senior Research Scientist, Google Research\n",
      "\n",
      "We announce the release of JAX-Privacy 1.0, a library for differentially private machine learning on the high-performance computing library, JAX.\n",
      "\n",
      "From personalized recommendations to scientific advances, AI models are helping to improve lives and transform industries. But the impact and accuracy of these AI models is often determined by the quality of data they use. Large, high-quality datasets are crucial for developing accurate and representative AI models, however, they must be used in ways that preserve individual privacy.\n",
      "\n",
      "That’s where JAX and JAX-Privacy come in. Introduced in 2020, JAX is a high-performance numerical computing library designed for large-scale machine learning (ML). Its core features — including automatic differentiation , just-in-time compilation , and seamless scaling across multiple accelerators — make it an ideal platform for building and training complex models efficiently. JAX has become a cornerstone for researchers and engineers pushing the boundaries of AI. Its surrounding ecosystem includes a robust set of domain-specific libraries, including Flax , which simplifies the implementation of neural network architectures, and Optax , which implements state-of-the-art optimizers.\n",
      "\n",
      "Built on JAX, JAX-Privacy is a robust toolkit for building and auditing differentially private models. It enables researchers and developers to quickly and efficiently implement differentially private (DP) algorithms for training deep learning models on large datasets, and provides the core tools needed to integrate private training into modern distributed training workflows. The original version of JAX-Privacy was introduced in 2022 to enable external researchers to reproduce and validate some of our advances on private training . It has since evolved into a hub where research teams across Google integrate their novel research insights into DP training and auditing algorithms.\n",
      "\n",
      "Today, we are proud to announce the release of JAX-Privacy 1.0 . Integrating our latest research advances and re-designed for modularity, this new version makes it easier than ever for researchers and developers to build DP training pipelines that combine state-of-the-art DP algorithms with the scalability provided by JAX.\n",
      "\n",
      "For years, researchers have turned to DP as the gold standard for quantifying and bounding privacy leakage. DP guarantees that the output of an algorithm is nearly the same whether or not a single individual (or example) is included in the dataset.\n",
      "\n",
      "While the theory of DP is well-established, its practical implementation in large-scale ML can be a challenge. The most common approach, differentially private stochastic gradient descent (DP-SGD), requires customized batching procedures, per-example gradient clipping, and the addition of carefully calibrated noise. This process is computationally intensive and can be difficult to implement correctly and efficiently, especially at the scale of modern foundation models.\n",
      "\n",
      "JAX-Privacy enables researchers and developers to train and fine-tune foundation models on private data using state-of-the-art differentially private algorithms in a scalable and efficient way thanks to its primitive building blocks for gradient clipping and correlated noise generation, both of which work effectively in distributed environments.\n",
      "\n",
      "Existing frameworks have made strides, but they often fall short in scalability or flexibility. Our work has consistently pushed the boundaries of private ML, from pioneering new DP algorithms to developing sophisticated auditing techniques . We needed a tool that could keep pace with our research — a library that was not only correct and efficient but also designed from the ground up to handle the parallelism and complexity of state-of-the-art models.\n",
      "\n",
      "JAX's functional paradigm and powerful transformations, like vmap (for automatic vectorization) and shard_map (for single-program multiple-data parallelization), provided a strong foundation. By building on JAX, we could create a library that was parallelism-ready out-of-the-box, supporting the training of large-scale models across multiple accelerators and supercomputers. JAX-Privacy is the culmination of this effort, a time-tested library that has powered internal production integrations and is now being shared with the broader community.\n",
      "\n",
      "JAX-Privacy simplifies the complexities of DP by providing a suite of carefully engineered components:\n",
      "\n",
      "JAX-Privacy implements a variety of foundational tools for clipping, noise addition, batch selection, accounting, and auditing that can be combined in various ways to construct end-to-end DP training plans.\n",
      "\n",
      "One of the most exciting aspects of JAX-Privacy is its practical application. The library is designed to support modern ML frameworks used for pre-training and fine-tuning LLMs. A notable example is our recent use of JAX-Privacy building blocks in the training of VaultGemma , the world's most capable differentially private LLM.\n",
      "\n",
      "With this open-source release, we want to enable developers to easily fine-tune large models with just a few lines of code via the popular Keras framework. In particular, we include fully-functional examples for fine-tuning models in the Gemma family , a collection of open models built by Google DeepMind based on Gemini. These examples demonstrate how to apply JAX-Privacy to tasks like dialogue summarization and synthetic data generation, showing that this library can deliver state-of-the-art results even when working with the most advanced models.\n",
      "\n",
      "By simplifying the integration of DP, JAX-Privacy empowers developers to build privacy-preserving applications from the ground up, whether they are fine-tuning a chatbot for a healthcare application or a model for personalized financial advice. It lowers the barrier to entry for privacy-preserving ML and makes powerful, responsible AI more accessible.\n",
      "\n",
      "We are excited to share JAX-Privacy with the research community. This release is the result of years of dedicated effort and represents a significant contribution to the field of privacy-preserving ML. We hope that by providing these tools, we can enable a new wave of research and innovation that benefits everyone.\n",
      "\n",
      "We will continue to support and develop the library, incorporating new research advances and responding to the needs of the community. We look forward to seeing what you build using JAX-Privacy. Check out the repository on GitHub or the PIP package to start training privacy-preserving ML models today.\n",
      "\n",
      "JAX-Privacy includes contributions from: Leonard Berrada, Robert Stanforth, Brendan McMahan, Christopher A. Choquette-Choo, Galen Andrew, Mikhail Pravilov, Sahra Ghalebikesabi, Aneesh Pappu, Michael Reneer, Jamie Hayes, Vadym Doroshenko, Keith Rush, Dj Dvijotham, Zachary Charles, Peter Kairouz, Soham De, Samuel L. Smith, Judy Hanwen Shen.\n",
      "\n",
      "November 21, 2025\n",
      "\n",
      "November 19, 2025\n",
      "\n",
      "November 13, 2025\n"
     ]
    }
   ],
   "source": [
    "idx = 5 #6th article\n",
    "print(df_google_research.loc[idx, 'title'])\n",
    "print(df_google_research.loc[idx, 'content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16656d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Logical Explanation:</b> \n",
    "\n",
    "* I have found out that last 3 lines in the Google Research articles are redundant and 2nd line as it gives information about Author.   \n",
    "* Unfortunately, there is not fixed pattern with the naming to credit author but pattern does exists in terms of last 3 lines are useless!  \n",
    "* I'll remove these lines (In some articles credit to author and other helpers are not given in the last 4th line but before that like see below example! Just to be safe I'll remove last 3 lines for now!)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b9fdcd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI as a research partner: Advancing theoretical computer science with AlphaEvolve\n",
      "Ansh Nagda, Student Researcher, and Abhradeep Thakurta, Staff Research Scientist, Google DeepMind, and Prabhakar Raghavan, Chief Technologist, Google\n",
      "\n",
      "We invoke AlphaEvolve, an LLM-based coding agent, to find and verify combinatorial structures that improve results on the hardness of approximately solving certain optimization problems.\n",
      "\n",
      "Recently, large language models (LLMs) have demonstrated surprising capabilities in competitive mathematics and competitive programming , demonstrating world-leading performance across both of these fields. However, their successes in mathematical discovery — proving novel theorems or uncovering new combinatorial structures — have been relatively few (with some notable exceptions [ 1 , 2 , 3 ]). Since mathematics and theoretical computer science demand absolute correctness [94fb54] , any AI-based method that makes mathematical discovery must either have a proof of correctness that can be confirmed computationally (without any human involvement), or have a domain-expert human in the loop to certify correctness.\n",
      "\n",
      "In our recent paper, “ Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory ”, we demonstrate how an LLM-powered coding agent can help discover new mathematical structures that push the boundaries of our understanding of complexity theory (a sub-field of theoretical computer science). Our work utilizes AlphaEvolve , a system developed at Google DeepMind that uses LLMs to iteratively evolve code. By employing a feedback loop, AlphaEvolve began with populations of code snippets, evaluated the structures produced by the code snippets, and used an LLM to morph the most successful snippets toward better solutions. This approach led to new results in two distinct areas of complexity theory: 1) improving the state-of-the-art for the limit on our ability to approximate the outcome (i.e., the \"inapproximability\") of the maximum cut problem for 4 slices (which we define as the MAX-4-CUT problem ), and 2) tightening the bounds on the average-case hardness of certifying properties of random graphs .\n",
      "\n",
      "AI-assisted mathematical research can operate in the following modes:\n",
      "\n",
      "Our work falls in the second category, where we obtain better proof elements using AlphaEvolve that can be automatically verified by a computer program.\n",
      "\n",
      "A fundamental challenge in using AI for theoretical computer science research lies in the universal nature of the problems studied. An AI system might find a solution to a specific instance of a problem — say, the optimal route for a traveling salesman visiting 50 specific cities. However, computer scientists often seek theorems that hold true universally for all problem instances and sizes (denoted as ∀n).\n",
      "\n",
      "How can we use AlphaEvolve to prove a universal statement? The answer lies in a technique known as \"lifting\" (see image below). If a proof is viewed as a long string, then one can take a chunk of the proof (corresponding to a certain finite structure), and evolve it to support a stronger universal statement, while keeping the interface to the rest of the proof intact. The advantage of this approach is that to certify overall correctness, one needs to only certify the correctness of the finite structure that has been evolved.\n",
      "\n",
      "Lifting: Morphing finite structures using AI, while keeping the interface to the broader proof intact.\n",
      "\n",
      "In complexity theory, researchers often use established proof frameworks that rely on the existence of specific, highly optimized finite structures. If a better structure can be found, the entire proof framework \"lifts\" this improvement to a better universal result.\n",
      "\n",
      "A key example of this is a \" gadget reduction .\" To prove that a target problem is computationally hard (intractable), researchers try to map a known intractable source problem to it, hence demonstrating that the target problem is at least as hard as the source problem. A gadget is a recipe for locally transforming a small piece of the source problem into a piece of the target problem. These gadgets are finite structures, and finding the optimal gadget is a painstaking process often done by hand.\n",
      "\n",
      "By tasking AlphaEvolve with finding better gadgets, we were able to discover structures far more complex than those previously known. These finite discoveries, when plugged into the existing mathematical frameworks, immediately yield new universal theorems in complexity theory.\n",
      "\n",
      "We applied this methodology to the MAX-k-CUT problem. Given a graph (a network of nodes and edges), the goal is to partition the nodes into k distinct sets such that the number of edges crossing between different sets is maximized. This is a classic intractable ( NP-hard ) problem, meaning we do not expect to find efficient algorithms that solve it exactly. Therefore, we focused on approximation algorithms — those that efficiently find solutions guaranteed to be close to the optimum.\n",
      "\n",
      "The crucial question is: what is the limit of approximation?\n",
      "\n",
      "For MAX-4-CUT (partitioning into four sets), the previous best-known result proved that it is NP-hard to approximate the solution within a factor of 0.9883 . AlphaEvolve was deployed to search for a new gadget reduction to MAX-4-CUT.\n",
      "\n",
      "The system discovered an intricate gadget involving 19 variables (nodes) with a complex weighting scheme (some connections having up to 1429 times the weight of others). This discovery established a new inapproximability bound of 0.987.\n",
      "\n",
      "This improvement may seem incremental, but in the mature field of hardness of approximation, such advances often require significant new techniques or combinatorial insights.\n",
      "\n",
      "Gadget found by AlphaEvolve for the reduction to MAX-4-CUT.\n",
      "\n",
      "We also explored the hardness of problems on average , rather than in the worst case. Specifically, we studied the difficulty of certifying bounds on the MAX-2-CUT (as well as maximum independent set ) of sparse random graphs [3d54ef] . Recent work connected this problem to the existence of specific Ramanujan graphs — deterministic graphs that “look” like sparse random graphs. They conjectured that the existence of Ramanujan graphs with unnaturally large cuts implies it is computationally hard to certify the MAX-2-CUT of a random graph.\n",
      "\n",
      "Prior work used computer assistance to find such graphs on up to 10 nodes. Improving their results requires finding more extremal Ramanujan graphs on many more nodes, which are exceedingly difficult to find and verify. AlphaEvolve successfully navigated this vast search space, discovering Ramanujan graphs with even larger cuts on as many as 163 nodes.\n",
      "\n",
      "A 4-regular Ramanujan graph with large 2-cut found by AlphaEvolve.\n",
      "\n",
      "These discoveries significantly improved the lower bounds for average-case hardness. Furthermore, combined with new algorithmic progress (non-AI based), we were able to nearly settle the computational hardness of these questions, matching the upper and lower bounds to within the third decimal place.\n",
      "\n",
      "A critical distinction of this work is that the results come with proofs of correctness.\n",
      "\n",
      "When an LLM is prompted to generate a mathematical proof directly, it often produces a proof sketch or an argument that requires substantial human intervention to verify and complete. Hallucinations or subtle errors can render the output useless. As mentioned earlier, the standard for correctness in math is absolute.\n",
      "\n",
      "In contrast, the approach taken here uses AI to discover a structure within the proof, not the proof itself. The validity of the final theorem relies on two components: the correctness of the lifting framework, and the verification of the discovered structure. While the frameworks are sound, verifying the structures discovered by AlphaEvolve is computationally intensive.\n",
      "\n",
      "Remarkably, AlphaEvolve achieved a 10,000x speedup in the verification process by implementing sophisticated branch-and-bound strategies and system-level optimizations. This massive speedup was the key enabler for the research, allowing the system to explore much larger and more complex gadgets.\n",
      "\n",
      "Crucially, the final gadgets discovered were still verified using the original, brute-force algorithm, ensuring the absolute correctness of the theorems.\n",
      "\n",
      "While these initial research findings are far from conclusive, they suggest that AI is poised to become a helpful collaborator in mathematical discovery. We have observed the models in AlphaEvolve generate intricate mathematical objects that at times exhibit nascent reasoning capabilities. However, as we transition into an era where proofs may increasingly be attributed to AI, the crucial task of verification is set to become a significant bottleneck.\n",
      "\n",
      "We would like to thank Adam Zsolt Wagner, Swarat Chaudhuri, Pasin Manurangsi and Sushant Sachdeva for helping us during various stages of the project.\n",
      "\n",
      "In mathematics, a statement is definitively true or false, with no intermediate state possible. This stands in contrast to several other applications of AI, such as essay-writing or artistic creation, which have subjective standards of correctness and do not need to be correct in an absolute sense.\n",
      "\n",
      "A sparse random graph is generated by randomly adding edges between a pair of nodes, where each node is guaranteed to have exactly d neighbors for some small d .\n",
      "\n",
      "November 21, 2025\n",
      "\n",
      "November 19, 2025\n",
      "\n",
      "November 18, 2025\n"
     ]
    }
   ],
   "source": [
    "idx = 25 #6th article\n",
    "print(df_google_research.loc[idx, 'title'])\n",
    "print(df_google_research.loc[idx, 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1374a8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below method is_data_line return True if line looks like 'November 21, 2025'.\n",
    "# Method clean_google_article removes the first non-empty line (author information), and remove all trailing date lines.\n",
    "\n",
    "MONTH_NAMES = {\n",
    "    \"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "    \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"\n",
    "}\n",
    "\n",
    "def is_date_line(line: str) -> bool:\n",
    "    if not isinstance(line, str):\n",
    "        return False\n",
    "\n",
    "    # Normalize non-breaking spaces\n",
    "    line = line.replace(\"\\u00a0\", \" \").strip()\n",
    "    if not line:\n",
    "        return False\n",
    "\n",
    "    parts = line.split()\n",
    "    if len(parts) != 3:\n",
    "        return False\n",
    "\n",
    "    month, day_with_comma, year = parts\n",
    "\n",
    "    if month not in MONTH_NAMES:\n",
    "        return False\n",
    "    if not (year.isdigit() and len(year) == 4):\n",
    "        return False\n",
    "\n",
    "    # Day should be like \"21,\" so strip comma and check int\n",
    "    if not day_with_comma.endswith(\",\"):\n",
    "        return False\n",
    "    day_str = day_with_comma[:-1]\n",
    "    if not day_str.isdigit():\n",
    "        return False\n",
    "    day = int(day_str)\n",
    "    if not (1 <= day <= 31):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def clean_google_article(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    first_non_empty_idx = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip():\n",
    "            first_non_empty_idx = i\n",
    "            break\n",
    "\n",
    "    if first_non_empty_idx is not None:\n",
    "        lines.pop(first_non_empty_idx)\n",
    "\n",
    "    while lines:\n",
    "        tail = lines[-1]\n",
    "        # Also drop trailing empty lines\n",
    "        if not tail.strip():\n",
    "            lines.pop()\n",
    "            continue\n",
    "\n",
    "        if is_date_line(tail):\n",
    "            lines.pop()\n",
    "            continue\n",
    "\n",
    "        # Stop when last line is not a date line\n",
    "        break\n",
    "\n",
    "    # Strip spaces and drop empty lines in the middle\n",
    "    lines = [ln.strip() for ln in lines]\n",
    "    lines = [ln for ln in lines if ln]\n",
    "\n",
    "    cleaned = \"\\n\\n\".join(lines)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# Apply to  dataframe\n",
    "df_google_research['content'] = df_google_research['content'].apply(clean_google_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "57e29033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI as a research partner: Advancing theoretical computer science with AlphaEvolve\n",
      "We invoke AlphaEvolve, an LLM-based coding agent, to find and verify combinatorial structures that improve results on the hardness of approximately solving certain optimization problems.\n",
      "\n",
      "Recently, large language models (LLMs) have demonstrated surprising capabilities in competitive mathematics and competitive programming , demonstrating world-leading performance across both of these fields. However, their successes in mathematical discovery — proving novel theorems or uncovering new combinatorial structures — have been relatively few (with some notable exceptions [ 1 , 2 , 3 ]). Since mathematics and theoretical computer science demand absolute correctness [94fb54] , any AI-based method that makes mathematical discovery must either have a proof of correctness that can be confirmed computationally (without any human involvement), or have a domain-expert human in the loop to certify correctness.\n",
      "\n",
      "In our recent paper, “ Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory ”, we demonstrate how an LLM-powered coding agent can help discover new mathematical structures that push the boundaries of our understanding of complexity theory (a sub-field of theoretical computer science). Our work utilizes AlphaEvolve , a system developed at Google DeepMind that uses LLMs to iteratively evolve code. By employing a feedback loop, AlphaEvolve began with populations of code snippets, evaluated the structures produced by the code snippets, and used an LLM to morph the most successful snippets toward better solutions. This approach led to new results in two distinct areas of complexity theory: 1) improving the state-of-the-art for the limit on our ability to approximate the outcome (i.e., the \"inapproximability\") of the maximum cut problem for 4 slices (which we define as the MAX-4-CUT problem ), and 2) tightening the bounds on the average-case hardness of certifying properties of random graphs .\n",
      "\n",
      "AI-assisted mathematical research can operate in the following modes:\n",
      "\n",
      "Our work falls in the second category, where we obtain better proof elements using AlphaEvolve that can be automatically verified by a computer program.\n",
      "\n",
      "A fundamental challenge in using AI for theoretical computer science research lies in the universal nature of the problems studied. An AI system might find a solution to a specific instance of a problem — say, the optimal route for a traveling salesman visiting 50 specific cities. However, computer scientists often seek theorems that hold true universally for all problem instances and sizes (denoted as ∀n).\n",
      "\n",
      "How can we use AlphaEvolve to prove a universal statement? The answer lies in a technique known as \"lifting\" (see image below). If a proof is viewed as a long string, then one can take a chunk of the proof (corresponding to a certain finite structure), and evolve it to support a stronger universal statement, while keeping the interface to the rest of the proof intact. The advantage of this approach is that to certify overall correctness, one needs to only certify the correctness of the finite structure that has been evolved.\n",
      "\n",
      "Lifting: Morphing finite structures using AI, while keeping the interface to the broader proof intact.\n",
      "\n",
      "In complexity theory, researchers often use established proof frameworks that rely on the existence of specific, highly optimized finite structures. If a better structure can be found, the entire proof framework \"lifts\" this improvement to a better universal result.\n",
      "\n",
      "A key example of this is a \" gadget reduction .\" To prove that a target problem is computationally hard (intractable), researchers try to map a known intractable source problem to it, hence demonstrating that the target problem is at least as hard as the source problem. A gadget is a recipe for locally transforming a small piece of the source problem into a piece of the target problem. These gadgets are finite structures, and finding the optimal gadget is a painstaking process often done by hand.\n",
      "\n",
      "By tasking AlphaEvolve with finding better gadgets, we were able to discover structures far more complex than those previously known. These finite discoveries, when plugged into the existing mathematical frameworks, immediately yield new universal theorems in complexity theory.\n",
      "\n",
      "We applied this methodology to the MAX-k-CUT problem. Given a graph (a network of nodes and edges), the goal is to partition the nodes into k distinct sets such that the number of edges crossing between different sets is maximized. This is a classic intractable ( NP-hard ) problem, meaning we do not expect to find efficient algorithms that solve it exactly. Therefore, we focused on approximation algorithms — those that efficiently find solutions guaranteed to be close to the optimum.\n",
      "\n",
      "The crucial question is: what is the limit of approximation?\n",
      "\n",
      "For MAX-4-CUT (partitioning into four sets), the previous best-known result proved that it is NP-hard to approximate the solution within a factor of 0.9883 . AlphaEvolve was deployed to search for a new gadget reduction to MAX-4-CUT.\n",
      "\n",
      "The system discovered an intricate gadget involving 19 variables (nodes) with a complex weighting scheme (some connections having up to 1429 times the weight of others). This discovery established a new inapproximability bound of 0.987.\n",
      "\n",
      "This improvement may seem incremental, but in the mature field of hardness of approximation, such advances often require significant new techniques or combinatorial insights.\n",
      "\n",
      "Gadget found by AlphaEvolve for the reduction to MAX-4-CUT.\n",
      "\n",
      "We also explored the hardness of problems on average , rather than in the worst case. Specifically, we studied the difficulty of certifying bounds on the MAX-2-CUT (as well as maximum independent set ) of sparse random graphs [3d54ef] . Recent work connected this problem to the existence of specific Ramanujan graphs — deterministic graphs that “look” like sparse random graphs. They conjectured that the existence of Ramanujan graphs with unnaturally large cuts implies it is computationally hard to certify the MAX-2-CUT of a random graph.\n",
      "\n",
      "Prior work used computer assistance to find such graphs on up to 10 nodes. Improving their results requires finding more extremal Ramanujan graphs on many more nodes, which are exceedingly difficult to find and verify. AlphaEvolve successfully navigated this vast search space, discovering Ramanujan graphs with even larger cuts on as many as 163 nodes.\n",
      "\n",
      "A 4-regular Ramanujan graph with large 2-cut found by AlphaEvolve.\n",
      "\n",
      "These discoveries significantly improved the lower bounds for average-case hardness. Furthermore, combined with new algorithmic progress (non-AI based), we were able to nearly settle the computational hardness of these questions, matching the upper and lower bounds to within the third decimal place.\n",
      "\n",
      "A critical distinction of this work is that the results come with proofs of correctness.\n",
      "\n",
      "When an LLM is prompted to generate a mathematical proof directly, it often produces a proof sketch or an argument that requires substantial human intervention to verify and complete. Hallucinations or subtle errors can render the output useless. As mentioned earlier, the standard for correctness in math is absolute.\n",
      "\n",
      "In contrast, the approach taken here uses AI to discover a structure within the proof, not the proof itself. The validity of the final theorem relies on two components: the correctness of the lifting framework, and the verification of the discovered structure. While the frameworks are sound, verifying the structures discovered by AlphaEvolve is computationally intensive.\n",
      "\n",
      "Remarkably, AlphaEvolve achieved a 10,000x speedup in the verification process by implementing sophisticated branch-and-bound strategies and system-level optimizations. This massive speedup was the key enabler for the research, allowing the system to explore much larger and more complex gadgets.\n",
      "\n",
      "Crucially, the final gadgets discovered were still verified using the original, brute-force algorithm, ensuring the absolute correctness of the theorems.\n",
      "\n",
      "While these initial research findings are far from conclusive, they suggest that AI is poised to become a helpful collaborator in mathematical discovery. We have observed the models in AlphaEvolve generate intricate mathematical objects that at times exhibit nascent reasoning capabilities. However, as we transition into an era where proofs may increasingly be attributed to AI, the crucial task of verification is set to become a significant bottleneck.\n",
      "\n",
      "We would like to thank Adam Zsolt Wagner, Swarat Chaudhuri, Pasin Manurangsi and Sushant Sachdeva for helping us during various stages of the project.\n",
      "\n",
      "In mathematics, a statement is definitively true or false, with no intermediate state possible. This stands in contrast to several other applications of AI, such as essay-writing or artistic creation, which have subjective standards of correctness and do not need to be correct in an absolute sense.\n",
      "\n",
      "A sparse random graph is generated by randomly adding edges between a pair of nodes, where each node is guaranteed to have exactly d neighbors for some small d .\n"
     ]
    }
   ],
   "source": [
    "idx = 25 #6th article\n",
    "print(df_google_research.loc[idx, 'title'])\n",
    "print(df_google_research.loc[idx, 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "18e5a7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta bought 1 GW of solar this week\n",
      "Meta signed three deals this week to procure nearly 1 gigawatt of solar power as it races to power its lofty AI ambitions.\n",
      "\n",
      "The trio of agreements brings Meta’s total solar purchases to over 3 gigawatts of capacity this year. Solar is cheap and quick to build, and as a result, it has become a\n",
      "\n",
      "go-to power source for tech companies\n",
      "\n",
      "as their data center fleets multiply in size.\n",
      "\n",
      "Meta yesterday announced two agreements in Louisiana that see it buying the environmental attributes of a combined 385 megawatts of electricity. Both projects are expected to be completed two years from now.\n",
      "\n",
      "They follow on the heels of a larger deal announced Monday in which Meta bought 600 megawatts from a massive solar farm near Lubbock, Texas. The project will also start commercial operations in 2027.\n",
      "\n",
      "While the Texas power plant won’t connect directly to Meta data centers, it will feed into the local grid, offsetting use by the facilities.\n",
      "\n",
      "The Louisiana deals, though, involve purchasing of certificates that allow Meta to offset its carbon-intensive sources of power.\n",
      "\n",
      "Such environmental attribute certificates (EACs), sometimes called renewable energy certificates, have been\n",
      "\n",
      "criticized by experts\n",
      "\n",
      "for obscuring the true carbon footprint of tech companies’ operations, which have ballooned as AI has driven up electricity use.\n",
      "\n",
      "Techcrunch event\n",
      "\n",
      "Join the Disrupt 2026 Waitlist\n",
      "\n",
      "Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\n",
      "\n",
      "Join the Disrupt 2026 Waitlist\n",
      "\n",
      "Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\n",
      "\n",
      "San Francisco\n",
      "\n",
      "|\n",
      "\n",
      "October 13-15, 2026\n",
      "\n",
      "W\n",
      "\n",
      "AITLIST\n",
      "\n",
      "NOW\n",
      "\n",
      "EACs were introduced years ago when renewables were costly relative to fossil fuel generators. They let anyone buy the electricity, but also gave companies an option to pay extra to offset their own emissions — and offset the higher costs of renewable power. They helped encourage developers to build more renewable projects.\n",
      "\n",
      "But the cost of new solar and wind has dropped dramatically since then, with renewables undercutting new fossil power and sometimes existing coal and natural gas power plants. EACs don’t provide the same incentive as before, and experts question how much additional renewable power they stimulate.\n",
      "\n",
      "If companies truly want to offset their new energy use from AI, they should be encouraging developers to build new renewable capacity, experts argue.\n"
     ]
    }
   ],
   "source": [
    "idx = 0  # first row\n",
    "\n",
    "print(df_tc_ai.loc[idx, 'title'])\n",
    "print(df_tc_ai.loc[idx, 'content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172e75d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Logical Explanation:</b> \n",
    "\n",
    "* As we can see above that there is an advertisement part (Techcrunch event......NOW). I'll verify if this occurs in other artickes too from TechCrunch!\n",
    "* If yes, I will remove it is redundant advertisement information and does not contribute to AI article news\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "b27f3d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakneck data center growth challenges Microsoft’s sustainability goals\n",
      "Microsoft’s new sustainability report, released late last week, shows how a carbon-heavy economy can weigh on a company that wants to be carbon light.\n",
      "\n",
      "Since 2020, the company’s carbon emissions are up 23.4%, mostly a result of breakneck\n",
      "\n",
      "data center buildout\n",
      "\n",
      "to support its growing cloud and AI operations. Buying enough clean electricity is actually the easy part — it’s the facilities themselves that are laden with carbon-intensive materials and products, including steel, concrete, and computer chips.\n",
      "\n",
      "“We reflect the challenges the world must overcome to develop and use greener concrete, steel, fuels, and chips,” a Microsoft spokesperson told TechCrunch via email. “These are the biggest drivers of our Scope 3 challenges.”\n",
      "\n",
      "Scope 3 emissions are those that are outside a company’s direct control, including raw materials, transportation, and purchased goods and services. Emissions in Scope 3 represent nearly all of Microsoft’s carbon footprint, just over 97% for fiscal year 2024, which the 2025 sustainability report covers.\n",
      "\n",
      "Microsoft’s Scope 3 profile is dominated by capital goods and purchased goods and services, with the two contributing about three-quarters of the company’s total carbon emissions.\n",
      "\n",
      "The construction of data centers has been the main driver behind Microsoft’s stubborn Scope 3 emissions. The steel used in the buildings comes from a supply chain that relies on blast furnaces heated by fossil fuels, and concrete used in the foundation is the product of a chemical reaction that’s both powered by and a producer of carbon dioxide. Some startups are working to decarbonize both\n",
      "\n",
      "steel\n",
      "\n",
      "and\n",
      "\n",
      "cement\n",
      "\n",
      ", and Microsoft is\n",
      "\n",
      "an investor\n",
      "\n",
      "in the space, but it’ll be years before those bets will have a significant impact.\n",
      "\n",
      "Carbon emissions are embodied in the computer chips inside the data center, too. Semiconductor lithography is dependent on chemicals that have extremely high global warming potential. For example, hexafluoroethane, which is used to etch features on chips, is a potent greenhouse gas, with 1 ton generating as much warming as\n",
      "\n",
      "9,200 tons\n",
      "\n",
      "of carbon dioxide.\n",
      "\n",
      "Techcrunch event\n",
      "\n",
      "Join the Disrupt 2026 Waitlist\n",
      "\n",
      "Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\n",
      "\n",
      "Join the Disrupt 2026 Waitlist\n",
      "\n",
      "Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.\n",
      "\n",
      "San Francisco\n",
      "\n",
      "|\n",
      "\n",
      "October 13-15, 2026\n",
      "\n",
      "W\n",
      "\n",
      "AITLIST\n",
      "\n",
      "NOW\n",
      "\n",
      "Even in green electricity, which is easier to find, hurdles have popped up as data centers aren’t always built near abundant clean energy sources. Because of that, Microsoft has had a difficult time finding nearby sources of zero-carbon electricity, forcing it to rely on purchases elsewhere. “Our electricity consumption has grown faster than the grids where we operate have decarbonized,” the spokesperson said.\n",
      "\n",
      "Overall, Microsoft’s 2024 emissions were down slightly compared with 2023, suggesting that the company is getting better at building data centers with lower climate impacts. Still, it has a long way to go to meet its 2030 goal of removing more carbon pollution than it generates. By its own forecast, Microsoft will have to cut its emissions by more than half while also significantly ramping up its carbon-removal efforts.\n",
      "\n",
      "There are signs that Microsoft is making some headway on both fronts. It has been one of the leading\n",
      "\n",
      "investors in\n",
      "\n",
      "and\n",
      "\n",
      "buyers of\n",
      "\n",
      "solar power in recent months, and its zero-carbon electricity portfolio now stands at 34 gigawatts of capacity. Plus, it has recently signed some\n",
      "\n",
      "very\n",
      "\n",
      "large\n",
      "\n",
      "deals that promise to remove millions of metric tons of carbon.\n",
      "\n",
      "However, 2030 is just a few years away, and the company’s push into AI and cloud may be profitable — but it’s made reaching its sustainability goals that much harder.\n"
     ]
    }
   ],
   "source": [
    "idx = 6  # seven row (Zero based indexing)\n",
    "\n",
    "print(df_tc_ai.loc[idx, 'title'])\n",
    "print(df_tc_ai.loc[idx, 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "0df0b335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harvard dropouts to launch ‘always on’ AI smart glasses that listen and record every conversation\n",
      "9:00 AM PDT · August 20, 2025\n",
      "\n",
      "Two former Harvard students are launching a pair of “always-on” AI-powered smart glasses that listen to, record, and transcribe every conversation and then display relevant information to the wearer in real time.\n",
      "\n",
      "“Our goal is to make glasses that make you super intelligent the moment you put them on,” said AnhPhu Nguyen, co-founder of\n",
      "\n",
      "Halo\n",
      "\n",
      ", a startup that’s developing the technology.\n",
      "\n",
      "Or, as his co-founder Caine Ardayfio put it, the glasses “give you infinite memory.”\n",
      "\n",
      "“The AI listens to every conversation you have and uses that knowledge to tell you what to say … kinda like IRL Cluely,” Ardayfio told TechCrunch, referring to\n",
      "\n",
      "the startup that claims to help users “cheat” on everything\n",
      "\n",
      "from job interviews to school exams.\n",
      "\n",
      "“If somebody says a complex word or asks you a question, like, ‘What’s 37 to the third power?’ or something like that, then it’ll pop up on the glasses,” Ardayfio added.\n",
      "\n",
      "Ardayfio and Nguyen have raised $1 million to develop the glasses, led by Pillar VC, with support from Soma Capital, Village Global, and Morningside Venture. The glasses will be priced at $249 and will be available for preorder starting Wednesday. Ardayfio called the glasses “the first real step towards vibe thinking.”\n",
      "\n",
      "The two Ivy League dropouts, who have since moved into their own version of the\n",
      "\n",
      "Hacker Hostel\n",
      "\n",
      "in the San Francisco Bay Area, recently caused a stir after developing a facial-recognition app for Meta’s smart Ray-Ban glasses to prove that the\n",
      "\n",
      "tech could be used to dox people\n",
      "\n",
      ". As a potential early competitor to Meta’s smart glasses, Ardayfio said Meta, given its history of security and privacy scandals, had to rein in its product in ways that Halo can ultimately capitalize on.\n",
      "\n",
      "“Meta doesn’t have a great reputation for caring about user privacy, and for them to release something that’s always there with you — which obviously brings a ton of utility — is just a huge reputational risk for them that they probably won’t take before a startup does it at scale first,” Nguyen added.\n",
      "\n",
      "And while Nguyen has a point, users may not yet have a good reason to trust the technology of a couple of college-aged students purporting to send people out into the world with covert recording equipment.\n",
      "\n",
      "While Meta’s glasses have an indicator light when their cameras and microphones are watching and listening as a mechanism to warn others that they are being recorded, Ardayfio said that the Halo glasses, dubbed Halo X, do not have an external indicator to warn people of their customers’ recording.\n",
      "\n",
      "“For the hardware we’re making, we want it to be discreet, like normal glasses,” said Ardayfio, who added that the glasses record every word, transcribe it, and then delete the audio file.\n",
      "\n",
      "Privacy advocates are warning about the normalization of covert recording devices in public.\n",
      "\n",
      "“Small and discreet recording devices are not new,” Eva Galperin, the director of cybersecurity at the Electronic Frontier Foundation, told TechCrunch.\n",
      "\n",
      "“In some ways, this sounds like a variation on the microphone spy pen,” said Galperin. “But I think that normalizing the use of an always-on recording device, which in many circumstances would require the user to get the consent of everyone within recording distance, eats away at the expectation of privacy we have for our conversations in all kinds of spaces.”\n",
      "\n",
      "There are\n",
      "\n",
      "several states in the U.S.\n",
      "\n",
      "that make it illegal to covertly record conversations without the other persons’ consent. Ardayfio said they are aware of this but that it is up to their customer to obtain consent before using the glasses.\n",
      "\n",
      "“We trust our users to get consent if they are in a two-party consent state,” said Ardayfio, referring to the laws of a dozen U.S. states that require the consent of all recorded parties.\n",
      "\n",
      "“I would also be very concerned about where the recorded data is being kept, how it is being stored, and who has access to it,” Galperin added.\n",
      "\n",
      "Ardayfio said Halo relies on\n",
      "\n",
      "Soniox\n",
      "\n",
      "for audio transcription, which claims to never store recordings. Nguyen claimed when the finished product is released to customers, it will be end-to-end encrypted but provided no evidence of how this would work. He also noted that Halo is aiming to get SOC 2 compliance, which means it has been independently audited and demonstrates adequate protection of customer data. A date for the completed SOC 2 compliance was not provided.\n",
      "\n",
      "Still, the two students are not new to privacy-invasive controversial projects.\n",
      "\n",
      "While still at Harvard last year,\n",
      "\n",
      "Ardayfio and Nguyen developed I-XRAY\n",
      "\n",
      ", a demo project that added facial-recognition capabilities to the Meta Ray-Ban smart glasses, demonstrating how easily the tech could be bolted onto a device not meant to identify people.\n",
      "\n",
      "The duo never released the code behind I-XRAY, but they did test the glasses on random passersby\n",
      "\n",
      "without consent\n",
      "\n",
      ". In a demo video, Ardayfio showed the glasses detecting faces and pulling up personal information of strangers within seconds. The video featured reactions of people who were doxed.\n",
      "\n",
      "In an\n",
      "\n",
      "interview with 404 Media,\n",
      "\n",
      "they acknowledged the risks: “Some dude could just find some girl’s home address on the train and just follow them home,” Nguyen told the tech news website.\n",
      "\n",
      "For now, Halo X glasses only have a display and a microphone, but no camera, although the two are exploring the possibility of adding it to a future model.\n",
      "\n",
      "Users still need to have their smartphones handy to help power the glasses and get “real time info prompts and answers to questions,” per Nguyen. The glasses, which are manufactured by another company that the startup didn’t name, are tethered to an accompanying app on the owner’s phone, where the glasses essentially outsource the computing since they don’t have enough power to do it on the device itself.\n",
      "\n",
      "Under the hood, the smart glasses use Google’s Gemini and Perplexity as its chatbot engine, according to the two co-founders. Gemini is better for math and reasoning, whereas they use Perplexity to scrape the internet, they said.\n",
      "\n",
      "During an interview, TechCrunch asked if their glasses knew when the next season of “The Witcher” would come out. Responding in a way reminiscent of C-3PO, Ardayfio said: “‘The Witcher’ season four will be released on Netflix in 2025, but there’s no exact date yet. Most sources expect it in the second half of 2025.”\n",
      "\n",
      "“I don’t know if that’s correct,” he added.\n",
      "\n",
      "We’re always looking to evolve, and by providing some insight into your perspective and feedback into TechCrunch and our coverage and events, you can help us!\n",
      "\n",
      "Fill out this survey to let us know how we’re doing\n",
      "\n",
      "a\n",
      "\n",
      "nd get the chance to win a prize in return!\n"
     ]
    }
   ],
   "source": [
    "idx = 2  # seven row (Zero based indexing)\n",
    "\n",
    "print(df_tc_ai.loc[idx, 'title'])\n",
    "print(df_tc_ai.loc[idx, 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5446023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex pattern to remove everything from \"Techcrunch Event\" ... \"NOW\"\n",
    "pattern = re.compile(r\"Techcrunch event.*?NOW\", re.DOTALL)\n",
    "\n",
    "def remove_tc_ad(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    cleaned = re.sub(pattern, \"\", text)\n",
    "    cleaned = re.sub(r'\\n\\s*\\n+', '\\n\\n', cleaned).strip()\n",
    "    return cleaned\n",
    "\n",
    "df_tc_ai['content'] = df_tc_ai['content'].apply(remove_tc_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "cc799bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breakneck data center growth challenges Microsoft’s sustainability goals\n",
      "Microsoft’s new sustainability report, released late last week, shows how a carbon-heavy economy can weigh on a company that wants to be carbon light.\n",
      "\n",
      "Since 2020, the company’s carbon emissions are up 23.4%, mostly a result of breakneck\n",
      "\n",
      "data center buildout\n",
      "\n",
      "to support its growing cloud and AI operations. Buying enough clean electricity is actually the easy part — it’s the facilities themselves that are laden with carbon-intensive materials and products, including steel, concrete, and computer chips.\n",
      "\n",
      "“We reflect the challenges the world must overcome to develop and use greener concrete, steel, fuels, and chips,” a Microsoft spokesperson told TechCrunch via email. “These are the biggest drivers of our Scope 3 challenges.”\n",
      "\n",
      "Scope 3 emissions are those that are outside a company’s direct control, including raw materials, transportation, and purchased goods and services. Emissions in Scope 3 represent nearly all of Microsoft’s carbon footprint, just over 97% for fiscal year 2024, which the 2025 sustainability report covers.\n",
      "\n",
      "Microsoft’s Scope 3 profile is dominated by capital goods and purchased goods and services, with the two contributing about three-quarters of the company’s total carbon emissions.\n",
      "\n",
      "The construction of data centers has been the main driver behind Microsoft’s stubborn Scope 3 emissions. The steel used in the buildings comes from a supply chain that relies on blast furnaces heated by fossil fuels, and concrete used in the foundation is the product of a chemical reaction that’s both powered by and a producer of carbon dioxide. Some startups are working to decarbonize both\n",
      "\n",
      "steel\n",
      "\n",
      "and\n",
      "\n",
      "cement\n",
      "\n",
      ", and Microsoft is\n",
      "\n",
      "an investor\n",
      "\n",
      "in the space, but it’ll be years before those bets will have a significant impact.\n",
      "\n",
      "Carbon emissions are embodied in the computer chips inside the data center, too. Semiconductor lithography is dependent on chemicals that have extremely high global warming potential. For example, hexafluoroethane, which is used to etch features on chips, is a potent greenhouse gas, with 1 ton generating as much warming as\n",
      "\n",
      "9,200 tons\n",
      "\n",
      "of carbon dioxide.\n",
      "\n",
      "Even in green electricity, which is easier to find, hurdles have popped up as data centers aren’t always built near abundant clean energy sources. Because of that, Microsoft has had a difficult time finding nearby sources of zero-carbon electricity, forcing it to rely on purchases elsewhere. “Our electricity consumption has grown faster than the grids where we operate have decarbonized,” the spokesperson said.\n",
      "\n",
      "Overall, Microsoft’s 2024 emissions were down slightly compared with 2023, suggesting that the company is getting better at building data centers with lower climate impacts. Still, it has a long way to go to meet its 2030 goal of removing more carbon pollution than it generates. By its own forecast, Microsoft will have to cut its emissions by more than half while also significantly ramping up its carbon-removal efforts.\n",
      "\n",
      "There are signs that Microsoft is making some headway on both fronts. It has been one of the leading\n",
      "\n",
      "investors in\n",
      "\n",
      "and\n",
      "\n",
      "buyers of\n",
      "\n",
      "solar power in recent months, and its zero-carbon electricity portfolio now stands at 34 gigawatts of capacity. Plus, it has recently signed some\n",
      "\n",
      "very\n",
      "\n",
      "large\n",
      "\n",
      "deals that promise to remove millions of metric tons of carbon.\n",
      "\n",
      "However, 2030 is just a few years away, and the company’s push into AI and cloud may be profitable — but it’s made reaching its sustainability goals that much harder.\n"
     ]
    }
   ],
   "source": [
    "#Verifying that it's actually removed!\n",
    "idx = 6  \n",
    "\n",
    "print(df_tc_ai.loc[idx, 'title'])\n",
    "print(df_tc_ai.loc[idx, 'content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec4ccec",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#64e3a1; font-family: arial black; color:#000000; font-size: 300%; text-align: center;\">Combining AI articles from both websites</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3c885",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Logical Explanation:</b> \n",
    "\n",
    "* I will append AI articles from both news websites (Google and TechCrunch) to have a single source of AI news. \n",
    "* This makes it easier to analyze, preprocess, summarization, ROUGE scoring, and sentiment analysis consistently across all articles.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586820a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles: 98\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(98, 3)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining both dataframes \n",
    "combined_df = pd.concat([df_google_research, df_tc_ai], ignore_index=True)\n",
    "print(f\"Total articles: {len(combined_df)}\")\n",
    "\n",
    "# Save the combined dataset\n",
    "combined_df.to_csv('data/combined_AI_articles.csv', index=False)\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "1fa076a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Ashton Kutcher’s Sound Ventures backs Fei-Fei ...</td>\n",
       "      <td>https://techcrunch.com/2024/10/29/ashton-kutch...</td>\n",
       "      <td>Ashton Kutcher’s VC firm,\\n\\nSound Ventures\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Ashton Kutcher explains why he’s betting on AI...</td>\n",
       "      <td>https://techcrunch.com/2024/10/29/ashton-kutch...</td>\n",
       "      <td>Ashton Kutcher, co-founder of Sound Ventures, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>After selling Anchor to Spotify, co-founders r...</td>\n",
       "      <td>https://techcrunch.com/2024/10/24/after-sellin...</td>\n",
       "      <td>The co-founders who\\n\\nsold their last startup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>From Goodreads’ founder, Smashing debuts its A...</td>\n",
       "      <td>https://techcrunch.com/2024/10/24/smashing-an-...</td>\n",
       "      <td>Smashing\\n\\n, a new app\\n\\ncurating the best o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>VCs love using the AI meeting notepad Granola,...</td>\n",
       "      <td>https://techcrunch.com/2024/10/23/vcs-love-usi...</td>\n",
       "      <td>Granola’s notepad app has become a popular too...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "93  Ashton Kutcher’s Sound Ventures backs Fei-Fei ...   \n",
       "94  Ashton Kutcher explains why he’s betting on AI...   \n",
       "95  After selling Anchor to Spotify, co-founders r...   \n",
       "96  From Goodreads’ founder, Smashing debuts its A...   \n",
       "97  VCs love using the AI meeting notepad Granola,...   \n",
       "\n",
       "                                                 link  \\\n",
       "93  https://techcrunch.com/2024/10/29/ashton-kutch...   \n",
       "94  https://techcrunch.com/2024/10/29/ashton-kutch...   \n",
       "95  https://techcrunch.com/2024/10/24/after-sellin...   \n",
       "96  https://techcrunch.com/2024/10/24/smashing-an-...   \n",
       "97  https://techcrunch.com/2024/10/23/vcs-love-usi...   \n",
       "\n",
       "                                              content  \n",
       "93  Ashton Kutcher’s VC firm,\\n\\nSound Ventures\\n\\...  \n",
       "94  Ashton Kutcher, co-founder of Sound Ventures, ...  \n",
       "95  The co-founders who\\n\\nsold their last startup...  \n",
       "96  Smashing\\n\\n, a new app\\n\\ncurating the best o...  \n",
       "97  Granola’s notepad app has become a popular too...  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvSMA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
