title,link,content
Reducing EV range anxiety: How a simple AI model predicts port availability,https://research.google/blog/reducing-ev-range-anxiety-how-a-simple-ai-model-predicts-port-availability/,"Kostas Kollias, Research Scientist, Google Research

We developed a unique model to predict the probability with which an EV charging port will be available at a certain station within a certain amount of minutes from the current time, which helps EV drivers plan their trips efficiently while minimizing waiting time at the charging stations.

The transition to electric vehicles (EVs) is accelerating globally, bringing with it the critical need for a reliable and robust charging infrastructure. While building out more physical charging stations is an important step, an equally important task is maximizing the efficiency of this infrastructure and minimizing ""range anxiety”, a term used to describe an EV driver’s fear of running out of battery before reaching their destination or the nearest available charging station. These concerns led us to design an approach for EV routing that reduces range anxiety by integrating charging stations into the navigational route based on the battery level and destination.

This week we announced a new lightweight, highly efficient prediction model that can answer the core question, “ What is the probability that an EV charging port will be available at a specific station a certain number of minutes from now? ” We found that the most sophisticated model isn't always the best solution. By co-designing the model and the deployment infrastructure, we were able to create a highly effective prediction system based on a simple linear regression approach. This model’s simplicity is its strength, allowing it to rely on easily accessible features while still achieving performance improvements over a strong baseline. Our work demonstrates that combining intuitive real-world logic with machine learning can deliver significant operational and user experience benefits.

Our goal was to maximize predictive power while minimizing the feature set (i.e., the specific, measurable data points the model uses to make a prediction) to ensure speed and low-latency deployment. After testing various architectures, including a decision tree and a simple neural network, a straightforward linear regression model proved to be the most performant and robust for this specific task.

We trained the model using real-time availability data from charging networks to calculate the true number of available charging ports within a certain number of minutes from the current observation time using criteria for model features and weights. We uniformly sampled ports from two distinct regions (CA and Germany). Larger stations were more likely to be included in the training set because they see more traffic than isolated ports and more closely reflect real-world usage.

The model uses the hour of the day as a key piece of information (a ""feature""). It treats each hour (or hour range) separately. For example, ""9 AM"" is one feature, and ""5 PM"" is another.

The ""weights"" are the specific numerical values that the linear regression algorithm learns during training. These numbers dictate how much each specific hour of the day affects the final prediction.

These “hour feature weights"" are the model's learned coefficients that quantify the predictable rate of EV port occupancy change for every hour of the day. Essentially, the model learns to express the difference between the current number of available ports and the future number of available ports as a function of the hour feature weights.

The feature weights learned for each hour of the day are particularly insightful because they directly represent the rate at which port occupancy changes. As illustrated by the chart below, there are clear, predictable trends tied to driver schedules:

Feature weights for each hour for the 30 minute horizon. They correspond to the rate at which port occupancy changes at each 30 minute bucket.

Feature weights for each hour for the 60 minute horizon. They correspond to the rate at which port occupancy changes at each 60 minute bucket.

Note that the model only differentiates from the current state when the change rate is significant (e.g., rush hour) or the station is large (more ports amplify the predicted change), which are intuitively the correct times to issue an updated prediction.

Our evaluation was designed to be rigorous and representative of real-world usage. For both the 30-minute and 60-minute time horizons, we evaluated predictions on 100 randomly selected stations, sampling their occupancy status 48 times daily (every 30 minutes) for a full week.

The model was benchmarked against a remarkably strong baseline: the ""Keep Current State"" approach. This baseline simply assumes that the number of available ports a certain number of minutes ( H ) in the future will be exactly the same as the current number.

While simple, this baseline is very hard to beat, especially over short horizons. For example, our data showed that on the US East Coast, never more than 10% of ports change their availability state within a 30-minute block. Since most of the time the state doesn't change, the simplest prediction — no change — is correct most of the time, making the task of adding predictive value extremely difficult.

We focused on two key metrics to measure the model’s accuracy for predicting the exact number of free ports: mean squared error (MSE) and mean absolute error (MAE). A ratio of MSE/MAE ≥ 1 free port measures the accuracy of the most critical binary task for the user: “Will I find at least one free port (Yes/No)?”

The evaluation confirmed that the linear regression model provides crucial gains over the strong ""Keep Current State"" baseline, primarily by correctly identifying the infrequent, yet vital, moments of high occupancy turnover.

We sampled test instances from among stations with at least 6 ports with horizons of 30 to 60 minutes, a realistic set of cases for charging in urban environments. We evaluated the model for the task of predicting the availability of at least one port in a station. This evaluation focused on the station profile and time of day when the model would differentiate from the baseline, namely large stations at times of significant rates of change.

The table below presents the fraction of time in which we provide a wrong prediction (which is equivalent to the MAE for this problem) for the times of highest change (8am and 8pm).

Comparison of error rates on the availability of at least one free port (30 to 60-Minute Horizon).

In summary, deploying the regression model allows us to reduce the number of bad predictions by approximately 20% in morning peak times and by approximately 40% in evening peak times.

Further examinations revealed that while the shape of the change rate curve (when ports fill vs. when they empty) is similar across regions, the magnitude of the change is distinct enough to warrant separate models. For instance, training the model separately for regions like California and Germany yielded better performance than pooling all data together, suggesting that it’s necessary to account for unique regional EV usage patterns.

We have successfully developed and deployed a lightweight, linear regression model that effectively predicts EV charging port availability. By focusing on simplicity, speed, and co-designing the model with the existing infrastructure, we bypassed the complexity and latency associated with more detailed, but often unscalable, approaches.

The resulting model provides a crucial predictive advantage over a strong ""Keep Current State"" baseline, particularly during high-traffic periods. This capability translates directly into an improved user experience: reduced anxiety, smarter routing decisions, and a better overall experience that supports the continued growth of electric mobility. Future work will focus on extending the prediction horizons to provide even greater value for long-distance travel planning.

We thank our collaborators Achir Ramadhan, Sreenivas Gollapudi, Shubham Gupta, Ilya Eyzerman, and Ivan Kuznetsov.

November 19, 2025

November 18, 2025

November 13, 2025"
Real-time speech-to-speech translation,https://research.google/blog/real-time-speech-to-speech-translation/,"Karolis Misiunas, Research Engineer, Google DeepMind, and Artsiom Ablavatski, Software Engineer, Google Core ML

We introduce an innovative end-to-end speech-to-speech translation (S2ST) model that enables real-time translation in the original speaker's voice with only a 2-second delay — bringing long-imagined technology into reality and making cross-language communication more natural.

Real-time communication is an integral part of both our professional and personal lives. When speaking to people remotely across language barriers, it can be difficult to truly connect by just relying on state-of-the-art translated captions, as they lack personality and real-time responsiveness essential for fluid conversation. The arrival of speech-to-speech translation (S2ST) bridges this gap by directly generating translated audio, leading to more natural communication. Existing speech-to-speech translation systems often incur significant delays (4–5s), tend to accumulate errors, and typically lack personalization.

Today we describe an innovative end-to-end S2ST model that overcomes these limitations, enabling live translation in the original speaker's voice with only 2 second delay. The novel architecture leverages a streaming framework and, with training on time-synchronized data, significantly reduces the delay between the original input and the translated speech. To support a breadth of languages, we introduce a scalable time-synced data acquisition pipeline that allows us to gradually expand the system to include more languages. This technology has demonstrated its effectiveness through successful deployment in real-time sensitive use cases.

Prior real-time speech-to-speech technologies employed a cascaded pipeline of individual processing blocks:

Schematic representation of classic, cascade-style speech-to-speech translation system.

Despite the high quality of the individual cascade components, achieving a seamless, real-time S2ST experience has been challenging due to three primary factors:

To significantly advance S2ST, we created a scalable data acquisition pipeline and developed an end-to-end model that provides direct, real-time language translation with just a two-second delay:

For a given language pair, initial work starts with raw audio acquisition. We utilize a diverse set of audio sources, including data generated by TTS models. This audio undergoes a cleaning and filtering process to ensure it contains a single speaker of the source language and has an appropriate noise level. After initial data collection, an ASR step transcribes the source text. With both source audio and text available, the forced alignment algorithm generates alignment timesteps (audio-to-text mapping). Any audio segments where alignment fails are discarded.

The remaining clips are machine translated from the source into the target language. Subsequently, a series of automated filters validate the translated output, ensuring accuracy and correspondence to the input text. Next, the original transcribed and translated texts are also aligned to generate corresponding timestamp annotations (text-to-translated text mapping).

Using a custom text-to-speech generation engine , the translated text is converted into translated audio, preserving the voice characteristics from the original audio while producing natural-sounding output. The pipeline concludes with one more forced alignment step of the translated text and the generated speech (speech-to-text mapping).

Streaming audio-to-audio translation dataset generation.

Utilizing the three generated alignments from the previous steps, the overlap between them is calculated, yielding alignment masks between the source and target audio. These alignment masks are then used to guide the loss computation during training.

Text alignment of input and translated audio with corresponding overlaps.

Invalid overlaps or translations that fail to meet delay requirements are filtered from the training dataset. The remaining aligned data is used to train the streaming S2ST model in chunks of up to 60 seconds. Various audio augmentation techniques are also applied during training, including sample rate reduction, reverberation, saturation, and denoising.

The end-to-end S2ST model leverages fundamental transformer blocks and consists of two main components:

A feature of these models is their representation of audio as a 2D set of tokens, known as RVQ audio tokens . As shown below, the X-axis represents time, while the Y-axis represents a set of tokens that describe the current audio segment. When summed, all tokens in a specific set can be readily converted into an audio stream using an ML codec. The number of tokens controls the audio quality for every segment, with more tokens yielding higher fidelity. The model predicts tokens sequentially, prioritizing those at the beginning. Typically, 16 tokens are sufficient for high-quality audio representation of a 100 ms chunk.

Schematic representation of audio-to-audio streaming inference for S2ST.

The model outputs a single text token in addition to the audio tokens. This text token acts as an extra prior for audio generation and enables direct metric calculation ( BLEU ) without relying on proxy ASR systems.

During training, a per-token loss is applied to the model to ensure accurate translation. The model's prediction delay, or lookahead, can be adjusted by shifting ground truth tokens to the right, allowing for flexibility based on the target language's complexity. For real-time conversations, a standard 2-second delay is typically used, which is suitable for most languages. While a longer lookahead improves translation quality by providing more context, it negatively impacts the real-time communication experience.

Ablation of lookahead and corresponding quality of translation for Spanish / English language pair (BLEU, higher is better).

In addition to the internal 2 second delay, the model's inference time contributes to the overall system latency. To minimize this and achieve real-time performance, we implemented several optimization techniques, including hybrid low-bit ( int8 and int4 ) quantization and optimized CFG precomputation.

The examples of translation for different language pairs with developed models with corresponding ground truth (taken from publicly available CVSS dataset ) follow:

Direction

Input audio

Translated audio

Ground truth

Spanish to English

In coastal areas there is a larger accumulation of water molecules in the air.

English to Spanish

Su portaaviones insignia, el Portaviones Formidable fue tocado por un kamikaze sin gravísimas consecuencias.

German to English

The electrician used a piece of aluminum foil to splice the fuse.

English to German

Margaret versucht mit allen Mitteln, die bevorstehende Katastrophe zu verhindern.

Italian to English

By clicking on the right, on the bell icon, you can activate Pash notifications so that they are updated in real time.

English to Italian

La voce popolare parla dei proprietari terrieri, dei mafiosi e dei rappresentanti del Partito conservatore e dei loro nomi, che sono noti a tutti.

Portuguese to English

This text is made available under the respective license.

English to Portuguese

Um homem de camisa azul observa algo projetado na frente dele.

French to English

The volunteer firefighters are a full part of our civil security arsenal.

English to French

Ce gouvernement et cette majorité portent donc seuls la responsabilité de cette situation.

The new end-to-end S2ST technology has been launched in two key areas, highlighting the importance of real-time cross-language communication. It is now available in Google Meet on servers, and as a built-in on-device feature for the new Pixel 10 devices. Although the products utilize different strategies for running the S2ST pipeline, they share training data and model architecture. The Pixel Voice Translate on-device feature also employs a cascade approach to maximize language coverage. To mitigate potential feature misuse, prior to each translation session, we inform the end-user that the translation is synthetically generated.

Link to Youtube Video

The new end-to-end S2ST technology enables Google Meet speech translation feature.

The current end-to-end model delivers robust performance for five Latin-based language pairs (English to and from Spanish, German, French, Italian, Portuguese), enabling our initial product launches. We are also observing promising capabilities in other languages, such as Hindi, that we plan to develop further. Future enhancements will focus on improving the dynamism of the model's lookahead. This will enable the S2ST technology to seamlessly adjust to languages with word orders significantly different from English, facilitating more contextual rather than literal word-for-word translation.

We believe that this breakthrough in S2ST technology will revolutionize real-time, cross-language communication, turning a long-envisioned concept into reality.

We are sincerely grateful to everyone who contributed to this project; their critical contributions were instrumental in making it a reality. We are particularly thankful to our colleagues Kevin Kilgour, Pen Li, Félix de Chaumont Quitry, Michael Dooley, Jeremy Thorpe, Mihajlo Velimirović, Alex Tudor, Christian Frank, Daniel Johansson, Hanna Silén, Christian Schuldt, Henrik Lundin, Esbjörn Dominique, Marcus Wirebrand, Daniel Kallander, Pablo Barrera González, Huib Kleinhout, Niklas Blum, Fredric Lindstrom, Esha Uboweja, Karthik Raveendran, Frédéric Rechtenstein, Xing Li, Queenie Zhang, Cheng Yang, Jason Fan, Matsvei Zhdanovich, Jianing Wei, and Matthias Grundmann.

November 21, 2025

November 13, 2025

November 12, 2025"
"Generative UI: A rich, custom, visual interactive user experience for any prompt",https://research.google/blog/generative-ui-a-rich-custom-visual-interactive-user-experience-for-any-prompt/,"Yaniv Leviathan, Google Fellow, Dani Valevski, Senior Staff Software Engineer, and Yossi Matias, Vice President & Head of Google Research

We introduce a novel implementation of generative UI, enabling AI models to create immersive experiences and interactive tools and simulations, all generated completely on the fly for any prompt. This is now rolling out in the Gemini app and Google Search, starting with AI Mode.

Generative UI is a powerful capability in which an AI model generates not only content but an entire user experience. Today we introduce a novel implementation of generative UI, which dynamically creates immersive visual experiences and interactive interfaces — such as web pages, games, tools, and applications — that are automatically designed and fully customized in response to any question, instruction, or prompt. These prompts can be as simple as a single word, or as long as needed for detailed instructions. These new types of interfaces are markedly different from the static, predefined interfaces in which AI models typically render content.

In our new paper, “ Generative UI: LLMs are Effective UI Generators ”, we describe the core principles that enabled our implementation of generative UI and demonstrate the effective viability of this new paradigm. Our evaluations indicate that, when ignoring generation speed, the interfaces from our generative UI implementations are strongly preferred by human raters compared to standard LLM outputs. This work represents a first step toward fully AI-generated user experiences, where users automatically get dynamic interfaces tailored to their needs, rather than having to select from an existing catalog of applications.

Our research on generative UI, also referred to as generative interfaces, comes to life today in the Gemini app through an experiment called dynamic view and in AI Mode in Google Search .

Generative UI is useful for a range of applications. For any user question, need, or prompt, as simple as a single word or as complex as elaborate instructions, the model creates a fully custom interface. Left: Getting tailored fashion advice . Middle: Learning about fractals . Right: Teaching mathematics .

For more examples see the project page .

Generative UI capabilities will be rolled out as two experiments in the Gemini app : dynamic view and visual layout. When using dynamic view, an experience built upon our generative UI implementation, Gemini designs and codes a fully customized interactive response for each prompt, using Gemini’s agentic coding capabilities. It customizes the experience with an understanding that explaining the microbiome to a 5 year old requires different content and a different set of features than explaining it to an adult, just as creating a gallery of social media posts for a business requires a completely different interface to generating a plan for an upcoming trip.

Dynamic view can be used for a wide range of scenarios, from learning about probability to helping in practical tasks like event planning and getting fashion advice . The interfaces allow users to learn, play or explore interactively. Dynamic view, along with visual layout, are rolling out today. To help us learn about these experiments, users may initially see only one of them.

Example of generative UI in dynamic view based on the prompt, “Create a Van Gogh gallery with life context for each piece”.

Generative UI experiences are also integrated into Google Search starting with AI Mode, unlocking dynamic visual experiences with interactive tools and simulations that are generated specifically for a user’s question. Now, thanks to Gemini 3’s unparalleled multimodal understanding and powerful agentic coding capabilities, Gemini 3 in AI Mode can interpret the intent behind any prompt to instantly build bespoke generative user interfaces. By generating interactive tools and simulations on the fly, it creates a dynamic environment optimized for deep comprehension and task completion. Generative UI capabilities in AI Mode are available for Google AI Pro and Ultra subscribers in the U.S. starting today. Select ""Thinking"" from the model drop-down menu in AI Mode to try it out.

Example of AI Mode in Google Search with the prompt, “show me how rna polymerase works. what are the stages of transcription and how is it different in prokaryotic and eukaryotic cells”.

Our generative UI implementation, described in the paper , uses Google’s Gemini 3 Pro model with three important additions:

A high-level system overview of the generative UI implementation.

For some products, it might be preferable to consistently see results in specific styles. Our implementation could be configured for these products so that all results, including generated assets, are created in a consistent style for all users. Without specific styling instructions, the generative UI will select a style automatically, or the user can influence styling in their prompt, as in the case of dynamic view in the Gemini app.

Screenshots of generative UI results with consistent “Wizard Green” styling.

To facilitate consistent evaluations and comparisons of generative UI implementations, we created PAGEN, a dataset of human expert–made websites and will soon be releasing it to the research community.

To evaluate user preferences, we compared our new generative UI experience against various different formats: a website designed for a specific prompt by human-experts, the top Google Search result for the query, and baseline LLM outputs in raw text or the standard markdown formats.

The sites designed by human experts had the highest preference rates. These were followed closely by the results from our generative UI implementation, with a substantial gap from all other output methods. This evaluation did not take into account generation speed. We also show that the performance of generative UI strongly depends on the performance of the underlying model, and that our newest models perform substantially better. See more details in the paper .

We are still in the early days of generative UI, and important opportunities for improvement remain. For example, our current implementation can sometimes take a minute or more to generate results, and there are occasional inaccuracies in the outputs; these are areas of ongoing research. Generative UI is an example of the magic cycle of research , where research breakthroughs lead to product innovation that opens up new opportunities for addressing user needs and in turn fuel further research. We see potential in extending generative UI to access a wider set of services, adapt to additional context and human feedback, and deliver increasingly more helpful visual and interactive interfaces. We are excited about the further opportunities ahead for generative UI.

November 21, 2025

November 7, 2025

October 31, 2025"
Separating natural forests from other tree cover with AI for deforestation-free supply chains,https://research.google/blog/separating-natural-forests-from-other-tree-cover-with-ai-for-deforestation-free-supply-chains/,"Maxim Neumann, Research Engineer, Google DeepMind, and Charlotte Stanton, Senior Program Manager, Google Research on behalf of the broader research team

Natural Forests of the World 2020 is an AI-powered map that distinguishes natural forests from other tree cover. This critical baseline helps governments, companies, and communities meet deforestation-free goals and protect ecosystems.

Forests are vital for our planet as they regulate rainfall, mitigate floods, store and sequester carbon, and help sustain the majority of the planet’s land-based species . Despite their importance, deforestation continues at an alarming rate. A key challenge in conservation efforts is differentiating centuries-old natural ecosystems from newly planted forests or tree crop plantations with satellite data. Most existing maps simply show ""tree cover,"" a basic measure of any woody vegetation, leading to an ""apples-to-oranges"" comparison. This conflates the harvesting of a short-term plantation with the permanent loss of an irreplaceable, biodiversity-rich natural forest.

The need for this distinction is more important than ever due to new global regulations, like the European Union Regulation on Deforestation-free Products (EUDR). This regulation mandates that products like coffee, cocoa, rubber, timber, and palm oil sold in the EU cannot come from land that was deforested or degraded after December 31, 2020, with the goal of protecting natural forests, like primary and naturally regenerating forests. This policy creates a need for a reliable, high-resolution, and globally-consistent map of natural forests as they existed in 2020. The protection of these forests is also a central pillar for COP30 , which recognizes their crucial role in climate stability and human well-being.

Gemini generated image showing natural forest ( left ) bordering a planted forest ( right ). Global satellite-based models struggle to distinguish between them, complicating efforts to protect the more biodiversity-rich natural forest.

In an effort to help meet this need, together with Google DeepMind , we’re releasing Natural Forests of the World 2020 , a new map and dataset, published in Nature Scientific Data . This project stems from a collaboration with the World Resources Institute and the International Institute for Applied Systems Analysis , and provides a critical baseline for deforestation and degradation monitoring. We provide the first globally consistent, 10-meter resolution map that differentiates natural forests from other tree cover and achieves a best-in-class accuracy of 92.2% when validated against a global independent dataset . We hope that this publicly available baseline can help companies conduct due diligence, support governments in monitoring deforestation, and empower conservation groups to target their efforts to protect what matters most.

The global extent of natural forests in 2020 (originally at 10-meter resolution).

Distinguishing a natural forest from a complex agroforestry system or a 50-year-old planted forest is difficult using a single satellite image. To overcome this, we developed an AI model that acts like a forester, observing a patch of land over the course of a year, segmenting a 1280 x 1280 meter patch and estimating the likelihood that each 10 x 10 meter pixel within it is a natural forest. This allows the model to make assessments based on the surrounding context, rather than a single snapshot. This novel multi-modal temporal-spatial vision transformer (MTSViT) model analyzes seasonal Sentinel-2 satellite imagery and topographical data (e.g., elevation and slope), along with the sample’s geographical coordinate. By observing satellite imagery over time, the model identifies distinct spectral, temporal, and texture signatures (i.e., data patterns used to recognize different forest types) that differentiate complex, natural forests from uniform, fast-growing commercial plantations and other land use and land cover.

To build the Natural Forests of the World 2020 map, we sampled over 1.2 million global 1280 x 1280 meter patch locations at 10-meter resolution to create a massive, multi-source training dataset. We used this data to train the MTSViT model to recognize complex patterns of natural forests and other land types. We then applied the trained MTSViT model across all land on Earth, generating a seamless, globally consistent 10-meter probability map. To rigorously validate the map, we created an evaluation dataset by repurposing an independent dataset focused on global forest management for 2015 and updating its labels to focus on natural forests for 2020. See more details in the paper .

End-to-end workflow of the Natural Forests map generation (annotating data generation, processing, model training, map generation, and validation steps).

We hope that the Natural Forests of the World 2020 baseline proves to be a valuable resource for policymakers, auditors, and companies seeking to comply with new deforestation-free regulations such as the EUDR. But forests are not static. To truly support global conservation and sustainability, we need to distinguish between more classes of forest and, crucially, understand how they change over time. This involves differentiating between and locating key forest types: natural forests (carbon-dense and biodiversity-rich forests), planted forests, plantations, and commercial tree crops (such as ecosystem-friendly coffee and cocoa agroforestry systems).

To advance this effort, we’re developing a new multi-year series of global forest type maps, powered by next-generation AI models. These maps will categorize the world's land into six distinct types: Primary Forest, Naturally Regenerating Forest, Planted Forest, Plantation Forest, Tree Crops, and Other Land Cover. We expect to release these comprehensive maps in 2026.

To encourage the broader research community to contribute to this effort, we have also released two large-scale benchmark datasets. These datasets are important for developing and rigorously testing the next generation of AI models designed to analyze the world’s forests. The Planted dataset is a global, multi-sensor long-temporal collection featuring over 2.3 million time-series classification examples. It is specifically designed to help AI models recognize 64 different (species or genera) types of planted forests and tree crops worldwide. The Forest Typology (ForTy) benchmark provides a truly global-scale dataset with 200,000 multi-source and multi-temporal image patches with per-pixel labels for semantic segmentation models. This resource is tailored for the core task of mapping the key classes: natural forest, planted forest, and tree crops.

Turning climate and nature ambitions into action requires transparent, trusted, and high-resolution data. We are committed to making these tools as accessible as possible. We hope these new datasets and tools will help governments, companies, and communities work together to meet their deforestation-free goals and protect the critical ecosystems on which we all depend.

Learn more about our AI and sustainability efforts by checking out Google Earth AI , Google Earth Engine , and AlphaEarth Foundations .

This research was co-developed by Google Deepmind and Google Research in collaboration with WRI and IIASA.

We thank our collaborators at Google, World Resources Institute (WRI) / Global Forest Watch (GFW), and International Institute for Applied Systems Analysis (IIASA): Anton Raichuk, Charlotte Stanton, Dan Morris, Drew Purves, Elizabeth Goldman, Katelyn Tarrio, Keith Anderson, Maxim Neumann, Mélanie Rey, Michelle J. Sims, Myroslava Lesiv, Nicholas Clinton, Petra Poklukar, Radost Stanimirova, Sarah Carter, Steffen Fritz, Yuchang Jiang.

Special thanks to early map reviewers: Andrew Lister (United States Forest Service), Astrid Verheggen (Joint Research Centre), Clement Bourgoin (Joint Research Centre), Erin Glen (WRI), Frederic Achard (Joint Research Centre), Jonas Fridman (Swedish University of Agricultural Sciences), Jukka Meiteninen (VTT), Karen Saunders (World Wildlife Fund Canada), Louis Reymondin (Alliance Bioversity International - CIAT), Martin Herold (GFZ Helmholtz Centre for Geosciences), Olga Nepomshina (GFZ Helmholtz Centre for Geosciences), Peter Potapov (University of Maryland/WRI), Rene Colditz (Joint Research Centre), Thibaud Vantalon (Alliance Bioversity International - CIAT), and Viviana Zalles (WRI).

November 12, 2025

November 5, 2025

November 4, 2025"
A new quantum toolkit for optimization,https://research.google/blog/a-new-quantum-toolkit-for-optimization/,"Stephen Jordan and Noah Shutty, Research Scientists, Google Quantum AI, Google Research

New theoretical work from Google Quantum AI shows that large scale quantum computers could solve certain optimization problems that are intractable for conventional classical computers.

From designing more efficient airline routes to organizing clinical trials, optimization problems are everywhere. Yet for many real-world challenges, even our most powerful supercomputers struggle to find the best solution. This has led to a major, decades-long question in quantum computing: could quantum machines succeed on optimization problems where classical ones fall short? This has proven to be a very difficult mathematical question, which remains largely open. As the capabilities of quantum hardware undergo rapid advancement , such theoretical problems of working out the eventual commercial and scientific use cases of large-scale error-corrected quantum computers become only more urgent.

In a recent Nature paper , researchers from Google Quantum AI and collaborators from Stanford, MIT, and Caltech shed new light on this question. We introduce an efficient quantum algorithm — called Decoded Quantum Interferometry (DQI) — that uses the wavelike nature of quantum mechanics to create interference patterns that converge on near-optimal solutions that are incredibly difficult to find using classical computers.

There is a catch, however. To build the necessary interference patterns, one must solve another hard computational problem called decoding. In a decoding problem one is given a lattice and a point in space, and one needs to find the nearest lattice element to the point. For example, the corners of the squares on a chessboard form a two dimensional lattice. After dropping a grain of sand at a random location on a chessboard, the decoding problem would be to find the nearest corner. Although this problem is easy for a square lattice in two dimensions, it can become very difficult on some lattices in hundreds or thousands of dimensions.

Fortunately, decoding problems have been extremely well studied over the past several decades, mainly due to applications in correcting errors incurred during data storage or transmission. Many sophisticated and powerful algorithms have been devised to solve decoding problems for various specially structured lattices. We have discovered that for certain kinds of optimization problems, the related decoding problems have the right kind of structure to be solved by some of these powerful decoding algorithms. However, it is only through the power of quantum computing that these decoding algorithms can be leveraged to also solve optimization problems. By pairing the quantum interference of DQI with these sophisticated decoding algorithms, a sufficiently large quantum computer could find approximate solutions to these optimization problems — solutions that appear to be beyond the reach of any known classical method.

This mathematical discovery of a quantum algorithm that offers speedup for optimization improves our understanding of the eventual use cases for quantum computers. When quantum computing hardware is advanced enough, researchers can use the DQI algorithm to solve classically challenging optimization problems.

A figurative representation of the conversion of an optimization problem on a rugged cost function landscape into a decoding problem for a periodic lattice.

In this work, our best result is for a problem that we call optimal polynomial intersection (OPI). In the OPI problem, one is given a list of target points and wishes to intersect as many as possible by tuning the coefficients of a polynomial whose degree is lower than the number of points. This is a common task in data science known as polynomial regression . Variants of this problem have arisen in the context of digital error correction as well as cryptography . Consequently, sophisticated algorithms have been developed for solving it in certain special cases, but for other cases, the problem remains hopelessly difficult to solve using known algorithms with conventional classical computers.

Using DQI, a quantum computer could convert this into a problem of decoding Reed-Solomon codes (a widely-used family of codes found in DVDs and QR codes). Very good decoding algorithms have been developed for decoding Reed-Solomon codes, and as a result, quantum computers using DQI can find better approximate optima to the OPI problem than can be found by known algorithms on classical computers. For example, our analysis shows that certain examples of the OPI problem could be solved by quantum computers using only on the order of a few million elementary quantum logic operations which would require over 10 23 (one hundred sextillion) elementary operations to solve on a conventional classical computer using the most efficient known classical algorithm.

An illustration of the OPI problem: One wishes to find a low degree polynomial intersecting as many as possible of the target sets. The polynomials f and g displayed here, despite being very different, each intersect three of the target sets, thus scoring the same value of the objective. This phenomenon of distant solutions achieving equal objective values is one reason why it is so hard to solve using conventional computers.

Taking a step back, we can ask why converting optimization problems into decoding problems should ever be advantageous in the first place? By understanding this more deeply, one could hope to gain intuition to guide the search for additional optimization problems on which quantum computers may provide advantage.

Both the optimization problems that we start with and the decoding problems that we convert them into are something called NP-hard problems . This suggests that it is impossible to efficiently find exact solutions to all instances of these problems, even with the help of quantum computers. By using quantum effects, DQI has converted one hard problem into another hard problem. How does this accomplish anything? The key is that the NP-hardness speaks to the difficulty of the very hardest instances of a given problem. If the problem instances are restricted to have some additional structure, this can make them easier . The promise of DQI is that certain kinds of structure may make the decoding problem much easier, without also making the optimization problem easier to solve using conventional computers.

In the OPI problem, the lattice that arises is algebraically structured; the components of the basis vectors, instead of being arbitrary, are obtained by raising a number to successively higher powers. This algebraic structure is reflected in both the original optimization problem (OPI) and the decoding problem that quantum computers can convert it into (Reed-Solomon decoding). This structure makes the decoding problem much easier, but as far as we can tell does not make the optimization problem easier for conventional computers. In this circumstance, the ability to convert the optimization problem into the decoding problem, using the power of quantum computing, provides advantage.

In the paper, we also consider more generic lattices that lack algebraic structure but whose basis vectors are sparse, i.e., consisting mostly of zeros. The corresponding optimization problem is called max-k-XORSAT and is illustrated below. The sparsity of the lattice is reflected in the fact that each of the constraints involves only a few of the variables (at most k ). In max-k-XORSAT there are more constraints than variables and it is impossible to satisfy all of them. Instead, one wishes to find a solution that satisfies as many constraints as possible. Though it may sound abstract, the max-k-XORSAT problem is commonly used as a testbed for new optimization algorithms and includes a number of other well-known optimization problems as special cases, such as max-cut and QUBO .

An example of a max-k-XORSAT problem with k = 2, which has 4 variables and 5 constraints. By assigning each of A, B, C, D to 0 or 1, how many constraints can you satisfy?

DQI can convert max-k-XORSAT into a decoding problem for codes defined by sparse matrices. Such codes are called low density parity check (LDPC) codes . It was discovered in the 1960s that sparsity makes the decoding problem much easier. However, the sparsity of the original max-k-XORSAT problem also makes it easier to solve on conventional computers using an algorithm called simulated annealing . Thus, it is hard to find max-k-XORSAT problems that have just the right sparsity so that the decoder is helped more than the simulated annealing algorithm that we are comparing against. In the paper we present one example problem where the sparsity is just right so that DQI appears to have a speed advantage over simulated annealing. However, we managed to solve this efficiently on a conventional computer using a specialized algorithm that we tailored to our example. So, at present, unlike for OPI, we do not have an example of a max-k-XORSAT problem that both can be solved by DQI and cannot be solved efficiently by any known algorithm running on conventional computers.

Since sparse optimization problems have widespread practical applications, we continue to search for ways that DQI might achieve quantum advantage on sparse optimization problems. In particular, DQI has motivated new lines of research into both classical and quantum algorithms for decoding LDPC codes.

The DQI algorithm provides a powerful new toolkit for developing quantum optimization algorithms. This approach of translating optimization problems into decoding problems offers a new way to address one of the longest-standing questions in the field. We are excited to see what researchers, both at Google and in the broader community, will build with these tools.

November 21, 2025

November 19, 2025

November 12, 2025"
Differentially private machine learning at scale with JAX-Privacy,https://research.google/blog/differentially-private-machine-learning-at-scale-with-jax-privacy/,"Borja Balle, Staff Research Scientist, Google DeepMind, and Ryan McKenna, Senior Research Scientist, Google Research

We announce the release of JAX-Privacy 1.0, a library for differentially private machine learning on the high-performance computing library, JAX.

From personalized recommendations to scientific advances, AI models are helping to improve lives and transform industries. But the impact and accuracy of these AI models is often determined by the quality of data they use. Large, high-quality datasets are crucial for developing accurate and representative AI models, however, they must be used in ways that preserve individual privacy.

That’s where JAX and JAX-Privacy come in. Introduced in 2020, JAX is a high-performance numerical computing library designed for large-scale machine learning (ML). Its core features — including automatic differentiation , just-in-time compilation , and seamless scaling across multiple accelerators — make it an ideal platform for building and training complex models efficiently. JAX has become a cornerstone for researchers and engineers pushing the boundaries of AI. Its surrounding ecosystem includes a robust set of domain-specific libraries, including Flax , which simplifies the implementation of neural network architectures, and Optax , which implements state-of-the-art optimizers.

Built on JAX, JAX-Privacy is a robust toolkit for building and auditing differentially private models. It enables researchers and developers to quickly and efficiently implement differentially private (DP) algorithms for training deep learning models on large datasets, and provides the core tools needed to integrate private training into modern distributed training workflows. The original version of JAX-Privacy was introduced in 2022 to enable external researchers to reproduce and validate some of our advances on private training . It has since evolved into a hub where research teams across Google integrate their novel research insights into DP training and auditing algorithms.

Today, we are proud to announce the release of JAX-Privacy 1.0 . Integrating our latest research advances and re-designed for modularity, this new version makes it easier than ever for researchers and developers to build DP training pipelines that combine state-of-the-art DP algorithms with the scalability provided by JAX.

For years, researchers have turned to DP as the gold standard for quantifying and bounding privacy leakage. DP guarantees that the output of an algorithm is nearly the same whether or not a single individual (or example) is included in the dataset.

While the theory of DP is well-established, its practical implementation in large-scale ML can be a challenge. The most common approach, differentially private stochastic gradient descent (DP-SGD), requires customized batching procedures, per-example gradient clipping, and the addition of carefully calibrated noise. This process is computationally intensive and can be difficult to implement correctly and efficiently, especially at the scale of modern foundation models.

JAX-Privacy enables researchers and developers to train and fine-tune foundation models on private data using state-of-the-art differentially private algorithms in a scalable and efficient way thanks to its primitive building blocks for gradient clipping and correlated noise generation, both of which work effectively in distributed environments.

Existing frameworks have made strides, but they often fall short in scalability or flexibility. Our work has consistently pushed the boundaries of private ML, from pioneering new DP algorithms to developing sophisticated auditing techniques . We needed a tool that could keep pace with our research — a library that was not only correct and efficient but also designed from the ground up to handle the parallelism and complexity of state-of-the-art models.

JAX's functional paradigm and powerful transformations, like vmap (for automatic vectorization) and shard_map (for single-program multiple-data parallelization), provided a strong foundation. By building on JAX, we could create a library that was parallelism-ready out-of-the-box, supporting the training of large-scale models across multiple accelerators and supercomputers. JAX-Privacy is the culmination of this effort, a time-tested library that has powered internal production integrations and is now being shared with the broader community.

JAX-Privacy simplifies the complexities of DP by providing a suite of carefully engineered components:

JAX-Privacy implements a variety of foundational tools for clipping, noise addition, batch selection, accounting, and auditing that can be combined in various ways to construct end-to-end DP training plans.

One of the most exciting aspects of JAX-Privacy is its practical application. The library is designed to support modern ML frameworks used for pre-training and fine-tuning LLMs. A notable example is our recent use of JAX-Privacy building blocks in the training of VaultGemma , the world's most capable differentially private LLM.

With this open-source release, we want to enable developers to easily fine-tune large models with just a few lines of code via the popular Keras framework. In particular, we include fully-functional examples for fine-tuning models in the Gemma family , a collection of open models built by Google DeepMind based on Gemini. These examples demonstrate how to apply JAX-Privacy to tasks like dialogue summarization and synthetic data generation, showing that this library can deliver state-of-the-art results even when working with the most advanced models.

By simplifying the integration of DP, JAX-Privacy empowers developers to build privacy-preserving applications from the ground up, whether they are fine-tuning a chatbot for a healthcare application or a model for personalized financial advice. It lowers the barrier to entry for privacy-preserving ML and makes powerful, responsible AI more accessible.

We are excited to share JAX-Privacy with the research community. This release is the result of years of dedicated effort and represents a significant contribution to the field of privacy-preserving ML. We hope that by providing these tools, we can enable a new wave of research and innovation that benefits everyone.

We will continue to support and develop the library, incorporating new research advances and responding to the needs of the community. We look forward to seeing what you build using JAX-Privacy. Check out the repository on GitHub or the PIP package to start training privacy-preserving ML models today.

JAX-Privacy includes contributions from: Leonard Berrada, Robert Stanforth, Brendan McMahan, Christopher A. Choquette-Choo, Galen Andrew, Mikhail Pravilov, Sahra Ghalebikesabi, Aneesh Pappu, Michael Reneer, Jamie Hayes, Vadym Doroshenko, Keith Rush, Dj Dvijotham, Zachary Charles, Peter Kairouz, Soham De, Samuel L. Smith, Judy Hanwen Shen.

November 21, 2025

November 19, 2025

November 13, 2025"
Introducing Nested Learning: A new ML paradigm for continual learning,https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/,"Ali Behrouz, Student Researcher, and Vahab Mirrokni, VP and Google Fellow, Google Research

We introduce Nested Learning, a new approach to machine learning that views models as a set of smaller, nested optimization problems, each with its own internal workflow, in order to mitigate or even completely avoid the issue of “catastrophic forgetting”, where learning new tasks sacrifices proficiency on old tasks.

The last decade has seen incredible progress in machine learning (ML), primarily driven by powerful neural network architectures and the algorithms used to train them. However, despite the success of large language models (LLMs), a few fundamental challenges persist, especially around continual learning, the ability for a model to actively acquire new knowledge and skills over time without forgetting old ones.

When it comes to continual learning and self-improvement, the human brain is the gold standard. It adapts through neuroplasticity — the remarkable capacity to change its structure in response to new experiences, memories, and learning. Without this ability, a person is limited to immediate context (like anterograde amnesia ). We see a similar limitation in current LLMs: their knowledge is confined to either the immediate context of their input window or the static information that they learn during pre-training.

The simple approach, continually updating a model's parameters with new data, often leads to “ catastrophic forgetting ” (CF), where learning new tasks sacrifices proficiency on old tasks. Researchers traditionally combat CF through architectural tweaks or better optimization rules. However, for too long, we have treated the model's architecture (the network structure) and the optimization algorithm (the training rule) as two separate things, which prevents us from achieving a truly unified, efficient learning system.

In our paper, “ Nested Learning: The Illusion of Deep Learning Architectures ”, published at NeurIPS 2025 , we introduce Nested Learning, which bridges this gap. Nested Learning treats a single ML model not as one continuous process, but as a system of interconnected, multi-level learning problems that are optimized simultaneously. We argue that the model's architecture and the rules used to train it (i.e., the optimization algorithm) are fundamentally the same concepts; they are just different ""levels"" of optimization, each with its own internal flow of information (""context flow"") and update rate. By recognizing this inherent structure, Nested Learning provides a new, previously invisible dimension for designing more capable AI, allowing us to build learning components with deeper computational depth, which ultimately helps solve issues like catastrophic forgetting.

We test and validate Nested Learning through a proof-of-concept, self-modifying architecture that we call “Hope”, which achieves superior performance in language modeling and demonstrates better long-context memory management than existing state-of-the-art models.

Nested Learning reveals that a complex ML model is actually a set of coherent, interconnected optimization problems nested within each other or running in parallel. Each of these internal problems has its own context flow — its own distinct set of information from which it is trying to learn.

This perspective implies that existing deep learning methods work by essentially compressing their internal context flows. More importantly, Nested Learning reveals a new dimension for designing models, allowing us to build learning components with deeper computational depth.

To illustrate this paradigm, we look at the concept of associative memory — the ability to map and recall one thing based on another (like recalling a name when you see a face).

The uniform and reusable structure as well as multi-time–scale update in the brain are the key components of continual learning in humans. Nested Learning allows for multi-time–scale updates for each component of the brain, while showing that well-known architectures such as transformers and memory modules are in fact linear layers with different frequency updates.

By defining an update frequency rate, i.e., how often each component's weights are adjusted, we can order these interconnected optimization problems into ""levels."" This ordered set forms the heart of the Nested Learning paradigm.

The Nested Learning perspective immediately gives us principled ways to improve existing algorithms and architectures:

Since Nested Learning views optimizers (e.g., momentum-based optimizers) as associative memory modules, it allows us to apply principles from associative memory perspective to them. We observed that many standard optimizers rely on simple dot-product similarity (a measure of how alike two vectors are by calculating the sum of the products of their corresponding components) whose update doesn't account for how different data samples relate to each other. By changing the underlying objective of the optimizer to a more standard loss metric, such as L2 regression loss (a common loss function in regression tasks that quantifies the error by summing the squares of the differences between predicted and true values), we derive new formulations for core concepts like momentum, making them more resilient to imperfect data.

In a standard Transformer, the sequence model acts as a short-term memory, holding the immediate context, while the feedforward neural networks act as long-term memory, storing pre-training knowledge. The Nested Learning paradigm extends this concept into what we call a “continuum memory system” (CMS), where memory is seen as a spectrum of modules, each updating at a different, specific frequency rate. This creates a much richer and more effective memory system for continual learning.

As a proof-of-concept, we used Nested Learning principles to design Hope, a variant of the Titans architecture. Titans architectures are long-term memory modules that prioritize memories based on how surprising they are. Despite their powerful memory management, they only have two levels of parameters update, resulting in a first-order in-context learning. Hope, however, is a self-modifying recurrent architecture that can take advantage of unbounded levels of in-context learning and also is augmented with CMS blocks to scale to larger context windows. It can essentially optimize its own memory through a self-referential process , creating an architecture with infinite, looped learning levels.

We conducted experiments to evaluate the effectiveness of our deep optimizers and the performance of Hope on language modeling, long-context reasoning, continual learning, and knowledge incorporation tasks. The full results are available in our paper .

Our experiments confirm the power of Nested Learning, the design of continuum memory systems, and self-modifying Titans.

On a diverse set of commonly used and public language modeling and common-sense reasoning tasks, the Hope architecture demonstrates lower perplexity and higher accuracy compared to modern recurrent models and standard transformers.

Comparison of performance on language modeling ( perplexity ; left) and common-sense reasoning (accuracy; right) tasks between different architectures: Hope, Titans, Samba and a baseline Transformer.

Hope showcases superior memory management in long-context Needle-In-Haystack (NIAH) downstream tasks, proving that the CMSs offer a more efficient and effective way to handle extended sequences of information.

Performance comparison on long-context tasks with different levels of difficulty between different architectures: Hope, Titans, TTT , and Mamba2 . NIAH-PK, NIAH-H, and NIAH-W are needle-in-a-haystack tasks with pass-key, number, and word, respectively.

The Nested Learning paradigm represents a step forward in our understanding of deep learning. By treating architecture and optimization as a single, coherent system of nested optimization problems, we unlock a new dimension for design, stacking multiple levels. The resulting models, like the Hope architecture, show that a principled approach to unifying these elements can lead to more expressive, capable, and efficient learning algorithms.

We believe the Nested Learning paradigm offers a robust foundation for closing the gap between the limited, forgetting nature of current LLMs and the remarkable continual learning abilities of the human brain. We are excited for the research community to explore this new dimension and help us build the next generation of self-improving AI.

This research was conducted by Ali Behrouz, Meisam Razaviyayn, Peilin Zhong, and Vahab Mirrokni. We thank Praneeth Kacham and Corinna Cortes for reviewing the work and their valuable suggestions. We also thank Yuan Deng and Zeman Li. Finally, we thank Mark Simborg and Kimberly Schwede for their help in crafting this blog post.

November 21, 2025

November 19, 2025

November 18, 2025"
DS-STAR: A state-of-the-art versatile data science agent,https://research.google/blog/ds-star-a-state-of-the-art-versatile-data-science-agent/,"Jinsung Yoon, Research Scientist, and Jaehyun Nam, Student Researcher, Google Cloud

DS-STAR is a state-of-the-art data science agent whose versatility is shown by its ability to automate a range of tasks — from statistical analysis to visualization and data wrangling — across various data types, culminating in a top-ranking performance on the famous DABStep benchmark.

Data science is a field dedicated to transforming raw data into meaningful, actionable insights, playing an essential role in solving real-world challenges. Businesses often depend on data-driven insights to make pivotal strategic decisions. However, the data science process is frequently complex, demanding a high level of expertise in fields like computer science and statistics. This workflow consists of many time-intensive activities, from interpreting various documents to performing complex data processing and statistical analysis.

To streamline this complex workflow, recent research has focused on using off-the-shelf large language models (LLMs) to create autonomous data science agents. The goal of these agents is to convert natural language questions into executable code for a desired task. But despite making significant progress , current data science agents have several limitations that hinder their practical use. A major issue is their heavy reliance on well-structured data, like CSV files in relational databases. This limited focus ignores the valuable information contained in the diverse and heterogeneous data formats , such as JSON, unstructured text, and markdown files, that are common in real-world applications. Another challenge is that many data science problems are open-ended and lack ground-truth labels, making it difficult to verify if an agent's reasoning is correct.

Data science agents answer user queries by generating code that operates on diverse data formats. Following the code's execution, the agent provides a final solution, which may take the form of a trained model, a processed database, a visualization, or a text-formatted answer.

To that end, we present DS-STAR , a new agent designed to solve data science problems. DS-STAR introduces three key innovations: (1) a data file analysis module that automatically extracts context from varied data formats, including unstructured ones; (2) a verification stage where an LLM-based judge assesses the plan’s sufficiency at each step; and (3) a sequential planning process that iteratively refines the initial plan based on feedback. This iterative refinement allows DS-STAR to handle complex analyses that draw verifiable insights from multiple data sources. We demonstrate that DS-STAR achieves state-of-the-art performance on challenging benchmarks like DABStep , KramaBench , and DA-Code . It especially excels with tasks involving diverse, heterogeneous data files.

The DS-STAR framework operates in two main stages. First, it automatically examines all files in a directory and creates a textual summary of their structure and contents. This summary becomes a vital source of context for tackling the task at hand.

DS-STAR creates a Python script to analyze diverse data files by extracting key information.

Second, DS-STAR engages in a primary loop of planning, implementing, and verifying. The Planner agent first creates a high-level plan, which the Coder agent then transforms into a code script. Subsequently, the Verifier agent evaluates the code's effectiveness in solving the problem. The Verifier agent is an LLM-based judge prompted to determine if the current plan is adequate. If the judge finds the plan insufficient, DS-STAR refines it by altering or adding steps (determined by the Router agent) and then repeats the cycle. Importantly, DS-STAR uses a method that mimics how an expert analyst uses tools like Google colab to build a plan sequentially, reviewing intermediate results before proceeding. This iterative cycle continues until a plan is deemed satisfactory or the maximum number of rounds (10) is reached, at which point the final code is delivered as the solution.

DS-STAR's workflow is an iterative loop. It starts by executing a simple plan and uses a Verifier agent to check if it's sufficient. If the plan is inadequate, a Router agent guides the refinement by adding a step or correcting any errors before the cycle repeats. The process continues until the Verifier approves the plan or the maximum number of rounds is reached.

To evaluate DS-STAR’s effectiveness, we compared its performance to existing state-of-the-art methods ( AutoGen , DA-Agent ) using a set of well-regarded data science benchmarks, DABStep , KramaBench , and DA-Code . These benchmarks evaluate performance on complex tasks like data wrangling, machine learning, and visualization that use multiple data sources and formats.

The results show that DS-STAR substantially outperforms AutoGen and DA-Agent in all test scenarios. Compared to the best alternative, DS-STAR raised the accuracy from 41.0% to 45.2% on DABStep, 39.8% to 44.7% on KramaBench, and 37.0% to 38.5% on DA-Code. Notably, DS-STAR also secured the top rank on the public leaderboard for the DABStep benchmark (as of 9/18/2025). On both easy tasks (where the answer is in a single file) and hard tasks (requiring multiple files), DS-STAR consistently surpasses competing baselines, demonstrating its superior ability to work with multiple, heterogeneous data sources.

This chart shows the normalized accuracy (%) on both easy (single-file) and hard (multi-file) tasks from the DABStep, KramaBench, and DA-Code benchmarks. DS-STAR consistently outperforms competing baselines, showing a particularly strong advantage in hard tasks that require processing multiple, heterogeneous data files.

Next, we conducted ablation studies to verify the effectiveness of DS-STAR’s individual components and analyze the impact of the number of refinement rounds, specifically by measuring the iterations required to generate a sufficient plan.

Data File Analyzer : This agent is essential for high performance. Without the descriptions it generates (Variant 1), DS-STAR's accuracy on difficult tasks within the DABStep benchmark sharply dropped to 26.98%, underscoring the importance of rich data context for effective planning and implementation.

Router : The Router agent’s ability to determine if a new step is needed or to fix an incorrect step is vital. When we removed it (Variant 2), DS-STAR only added new steps sequentially, leading to worse performance on both easy and hard tasks. This demonstrated that it is more effective to correct mistakes in a plan than to keep adding potentially flawed steps.

Generalizability Across LLMs : We also tested DS-STAR's adaptability by using GPT-5 as the base model. This yielded promising results on the DABStep benchmark, indicating the framework's generalizability. Interestingly, DS-STAR with GPT-5 performed better on easy tasks, while the Gemini-2.5-Pro version performed better on hard tasks.

Ablation study results for DS-STAR on the DABStep benchmark, evaluating individual agent effectiveness and LLM compatibility.

An analysis of the refinement process : The figure below shows that difficult tasks naturally require more iterations. On the DABStep benchmark, hard tasks needed an average of 5.6 rounds to solve, whereas easy tasks required only 3.0 rounds. Furthermore, over half of the easy tasks were completed in just a single round.

An analysis of refinement rounds on the DABStep benchmark shows that difficult tasks require more iterations. Hard tasks average 5.6 rounds versus 3.0 for easy tasks, with over 50% of easy tasks being solved in the first round alone.

In this work, we introduced DS-STAR, a new agent that can autonomously solve data science problems. The framework is defined by two core innovations: the automatic analysis of diverse file formats and an iterative, sequential planning process that uses a novel LLM-based verification system. DS-STAR establishes a new state-of-the-art on the DABStep, KramaBench, and DA-Code benchmarks, outperforming the best alternative. By automating complex data science tasks, DS-STAR has the potential to make data science more accessible for individuals and organizations, helping to drive innovation across many different fields.

We would like to thank Jiefeng Chen, Jinwoo Shin, Raj Sinha, Mihir Parmar, George Lee, Vishy Tirumalashetty, Tomas Pfister and Burak Gokturk for their valuable contributions to this work.

November 7, 2025

October 29, 2025

October 20, 2025"
Forecasting the future of forests with AI: From counting losses to predicting risk,https://research.google/blog/forecasting-the-future-of-forests-with-ai-from-counting-losses-to-predicting-risk/,"Drew Purves, Research Scientist, Google DeepMind, and Charlotte Stanton, Senior Program Manager, Google Research, on behalf of the ForestCast team

We introduce the first deep learning–powered benchmark for proactive deforestation risk forecasting.

Nature underpins our climate, our economies, and our very lives. And within nature, forests stand as one of the most powerful pillars — storing carbon, regulating rainfall, mitigating floods, and harboring the majority of the planet’s terrestrial biodiversity.

Yet, despite their critical importance, the world continues to lose forests at an alarming rate. Last year alone, we lost the equivalent of 18 soccer fields of tropical forest every minute, totaling 6.7 million hectares — a record high and double the amount lost the year before . Today, habitat conversion is the greatest threat to biodiversity on land.

For years, satellite data has been our essential tool for measuring this loss. More recently, in collaboration with the World Resources Institute , we helped map the underlying drivers of that loss — from agriculture and logging to mining and fire — for the years 2000–2024. These maps, which are at an unprecedented 1km 2 resolution, provide a basis for a wide range of forest protection measures. However those insights, critical as they are, only look backward. Now, it's time to look ahead.

We're excited to announce the release of “ ForestCast: Forecasting Deforestation Risk at Scale with Deep Learning ”, along with the first publicly available benchmark dataset dedicated to training deep learning models to predict deforestation risk. This shift from merely monitoring what's already gone to forecasting what's at risk in the future changes the game. Previous approaches to risk have depended on assembling patchily-available input maps, such as roads and population density, which can quickly go out of date. By contrast, we have developed an efficient approach based on pure satellite data that can be applied consistently, in any region, and can be readily updated in the future when more data becomes available. We found that this approach could match or exceed the accuracy of previous approaches. To ensure the community can reproduce and build on our work, we are releasing all of the input, training, and evaluation data as a public benchmark dataset.

Deforestation is fundamentally a human process driven by a complex web of economic, political, and environmental factors. It's fueled by commodity-driven expansion for products like cattle, palm oil, and soy, but also by wildfires, logging, the expansion of settlements and infrastructure, and the extraction of hard minerals and energy. Predicting the location and timing of future loss is therefore incredibly hard.

The current state-of-the-art approach tries to solve this by assembling specialized geospatial information on as many of those factors as possible: maps of roads, economic indicators, policy enforcement data, etc. This approach has provided accurate predictions for some regions at some times. However, it is not generally scalable because those input maps are often patchy, inconsistent, and need to be assembled separately for each region. This approach is also not future-proof, because the input maps tend to quickly go out of date, and there is no guarantee when, if ever, they may be refreshed.

To overcome these challenges, we adopt a “pure satellite” model, where the only inputs are derived from satellites. We tested raw satellite inputs from the Landsat and Sentinel 2 satellites. We also included a satellite-derived input we refer to as “change history”, which identifies each pixel that has already been deforested and provides a year for when that deforestation occurred. We trained and evaluated the model using satellite-derived labels of deforestation.

The pure satellite approach provides consistency, in that we can apply the exact same method anywhere on Earth, allowing for meaningful comparisons between different regions. It also makes our model future proof — these satellite data streams will continue for years to come, so we can repeat the method to give updated predictions of risk and examine how risk is changing through time.

To achieve accuracy and scalability, we developed a custom model based on vision transformers . The model receives a whole tile of satellite pixels as input, which is crucial to capture the spatial context of the landscape and recent deforestation (as captured in the change history). It then outputs a whole tile’s worth of predictions in one pass, which makes the model scalable to large regions.

We found that our model was able to reproduce, or exceed, the accuracy of methods based on specialized inputs (such as roads), accurately predicting tile-to-tile variation in the amount of deforestation, and, within tiles, accurately predicting which pixels were the most likely to become deforested next.

Our deep learning vision model analyzes satellite time series and historical forest loss to forecast deforestation risk.

Surprisingly, we found that by far the most important satellite input was the simplest, the change history. So much so that a model receiving only this input could provide predictions with accuracy metrics indistinguishable from models using the full, raw satellite data. In retrospect we can see that the change history is a small, but highly information dense, model input — including information on tile-to-tile variation in recent deforestation rates, and how these are trending through time, and also capturing moving deforestation fronts within tiles.

To promote transparency and repeatability, we are releasing the training and evaluation data used in this work, as a benchmark . This allows the wider machine learning community to verify our results; to potentially extract deeper understanding of why the model makes certain predictions; and ultimately, to build and compare improved deforestation risk models.

Moreover, our benchmark and paper provide a clear template for scaling this approach globally — to model tropical deforestation across Latin America and Africa, and eventually, to temperate and boreal latitudes where forest loss is often driven by different dynamics, such as cattle ranching and fire.

Land-use change, especially tropical deforestation and forest conversion, is responsible for roughly 10% of global anthropogenic greenhouse-gas emissions and threatens the vast majority of the planet's terrestrial life . Forecasts of deforestation risk could be a vital tool for targeting resources where they can have the greatest impact in curbing those emissions and protecting nature.

This ability to anticipate risk allows governments, companies, and communities to act early, when there’s still time to prevent loss, rather than reacting to damage that’s already done. For example:

Thus, a forecast like this isn't a prediction of an unavoidable future. Instead, it's a tool designed to change a future outcome. The goal is to share this information with those who can act, helping them channel resources to the most vulnerable areas before it's too late, and empowering them to ensure these high-risk forests remain standing. By combining open data and advanced AI, we're forging a powerful new tool to safeguard nature.

Learn more about our AI and sustainability efforts by checking out Google Earth AI , Google Earth Engine , and AlphaEarth Foundations .

This research was co-developed by Google Deepmind and Google Research.

Google Deepmind: Matt Overlan, Arianna Manzini, Drew Purves, Julia Haas, Maxim Neumann, Mélanie Rey.

Google Research: Charlotte Stanton, Michelangelo Conserva.

We’d also like to thank our additional collaborators Kira Prabhu, Youngin Shin and Kuan Lu, as well as Peter Battaglia and Kat Chou for their support.

November 13, 2025

October 31, 2025

October 23, 2025"
"Exploring a space-based, scalable AI infrastructure system design",https://research.google/blog/exploring-a-space-based-scalable-ai-infrastructure-system-design/,"Travis Beals, Senior Director, Paradigms of Intelligence

Project Suncatcher is a moonshot exploring a new frontier: equipping solar-powered satellite constellations with TPUs and free-space optical links to one day scale machine learning compute in space.

Artificial intelligence (AI) is a foundational technology that could reshape our world, driving new scientific discoveries and helping us tackle humanity's greatest challenges. Now, we're asking where we can go to unlock its fullest potential.

The Sun is the ultimate energy source in our solar system, emitting more power than 100 trillion times humanity’s total electricity production. In the right orbit, a solar panel can be up to 8 times more productive than on earth, and produce power nearly continuously, reducing the need for batteries. In the future, space may be the best place to scale AI compute. Working backwards from there, our new research moonshot, Project Suncatcher, envisions compact constellations of solar-powered satellites, carrying Google TPUs and connected by free-space optical links . This approach would have tremendous potential for scale, and also minimizes impact on terrestrial resources.

We’re excited about this growing area of exploration, and our early research, shared today in “ Towards a future space-based, highly scalable AI infrastructure system design, ” a preprint paper, which describes our progress toward tackling the foundational challenges of this ambitious endeavor — including high-bandwidth communication between satellites, orbital dynamics, and radiation effects on computing. By focusing on a modular design of smaller, interconnected satellites, we are laying the groundwork for a highly scalable, future space-based AI infrastructure.

Project Suncatcher is part of Google’s long tradition of taking on moonshots that tackle tough scientific and engineering problems. Like all moonshots, there will be unknowns, but it’s in this spirit that we embarked on building a large-scale quantum computer a decade ago — before it was considered a realistic engineering goal — and envisioned an autonomous vehicle over 15 years ago , which eventually became Waymo and now serves millions of passenger trips around the globe.

The proposed system consists of a constellation of networked satellites, likely operating in a dawn–dusk sun-synchronous low earth orbit , where they would be exposed to near-constant sunlight. This orbital choice maximizes solar energy collection and reduces the need for heavy onboard batteries. For this system to be viable, several technical hurdles must be overcome:

Large-scale ML workloads require distributing tasks across numerous accelerators with high-bandwidth, low-latency connections. Delivering performance comparable to terrestrial data centers requires links between satellites that support tens of terabits per second. Our analysis indicates that this should be possible with multi-channel dense wavelength-division multiplexing (DWDM) transceivers and spatial multiplexing .

However, achieving this kind of bandwidth requires received power levels thousands of times higher than typical in conventional, long-range deployments. Since received power scales inversely with the square of the distance, we can overcome this challenge by flying the satellites in a very close formation (kilometers or less), thus closing the link budget (i.e., the accounting of the end-to-end signal power losses in the communications system). Our team has already begun validating this approach with a bench-scale demonstrator that successfully achieved 800 Gbps each-way transmission (1.6 Tbps total) using a single transceiver pair.

High-bandwidth inter-satellite links require our satellites to fly in a much more compact formation than any current system. We developed numerical and analytic physics models to analyze the orbital dynamics of such a constellation. We used an approximation starting from the Hill-Clohessy-Wiltshire equations (which describe the orbital motion of a satellite relative to a circular reference orbit in a Keplerian approximation ) and a JAX -based differentiable model for the numerical refinement that accounts for further perturbations.

At the altitude of our planned constellation, the non-sphericity of Earth's gravitational field, and potentially atmospheric drag, are the dominant non-Keplerian effects impacting satellite orbital dynamics. In the figure below, we show trajectories (over one full orbit) for an illustrative 81-satellite constellation configuration in the orbital plane, at a mean cluster altitude of 650 km. The cluster radius is R=1 km, with the distance between next-nearest-neighbor satellites oscillating between ~100–200m, under the influence of Earth’s gravity.

The models show that, with satellites positioned just hundreds of meters apart, we will likely only require modest station-keeping maneuvers to maintain stable constellations within our desired sun-synchronous orbit.

Evolution of a free-fall (“no thrust”) constellation under Earth’s gravitational attraction, modeled to the level of detail required to obtain sun-synchronous orbits, in a non-rotating coordinate system, relative to a central reference satellite S0. Arrow points towards Earth’s center. Magenta: nearest neighbors of satellite S0. Orange: Example ""peripheral"" satellite S1. Orange dashed: S1’s positions relative to the cluster center (in the non-rotating coordinate frame).

For ML accelerators to be effective in space, they must withstand the environment of low-Earth orbit. We tested Trillium , Google’s v6e Cloud TPU, in a 67MeV proton beam to test for impact from total ionizing dose (TID) and single event effects (SEEs).

The results were promising. While the High Bandwidth Memory (HBM) subsystems were the most sensitive component, they only began showing irregularities after a cumulative dose of 2 krad(Si) — nearly three times the expected (shielded) five year mission dose of 750 rad(Si). No hard failures were attributable to TID up to the maximum tested dose of 15 krad(Si) on a single chip, indicating that Trillium TPUs are surprisingly radiation-hard for space applications.

Historically, high launch costs have been a primary barrier to large-scale space-based systems. However, our analysis of historical and projected launch pricing data suggests that with a sustained learning rate, prices may fall to less than $200/kg by the mid-2030s . At that price point, the cost of launching and operating a space-based data center could become roughly comparable to the reported energy costs of an equivalent terrestrial data center on a per-kilowatt/year basis [608112] . See the preprint paper for more details.

Our initial analysis shows that the core concepts of space-based ML compute are not precluded by fundamental physics or insurmountable economic barriers. However, significant engineering challenges remain, such as thermal management, high-bandwidth ground communications, and on-orbit system reliability.

To begin addressing these challenges, our next milestone is a learning mission in partnership with Planet , slated to launch two prototype satellites by early 2027. This experiment will test how our models and TPU hardware operate in space and validate the use of optical inter-satellite links for distributed ML tasks.

Eventually, gigawatt-scale constellations may benefit from a more radical satellite design; this may combine new compute architectures more naturally suited to the space environment with a mechanical design in which solar power collection, compute, and thermal management are tightly integrated. Just as the development of complex system-on-chip technology was motivated by and enabled by modern smartphones, scale and integration will advance what’s possible in space.

“Towards a future space-based, highly scalable AI infrastructure system design” was authored by Blaise Agüera y Arcas, Travis Beals, Maria Biggs, Jessica V. Bloom, Thomas Fischbacher, Konstantin Gromov, Urs Köster, Rishiraj Pravahan and James Manyika.

We thank Amaan Pirani for critical contributions to cost modeling and overall feasibility analysis, Marcin Kowalczyk for independent numerical validation calculations, Paul Epp and Stephen Palese for input on the ISL concept, Thomas Zurbuchen for his contributions to the systems and architecture concepts, and Kenny Vassigh and Jerry Chiu for technical input on system and thermal design. We also thank Muon Space for general discussions and for technical and economic feasibility analysis of the concept.

Based on publicly reported energy costs for the data center industry.

November 13, 2025

November 12, 2025

October 20, 2025"
Accelerating the magic cycle of research breakthroughs and real-world applications,https://research.google/blog/accelerating-the-magic-cycle-of-research-breakthroughs-and-real-world-applications/,"Yossi Matias, Vice President & Head of Google Research

From earth science to genomics to quantum, we share the latest scientific breakthroughs from Google Research and how today’s powerful AI tools and platforms are accelerating innovation.

Last week at our flagship Research@ event in Mountain View, we shared some of Google Research’s latest announcements, from understanding earth to advancements in genomics to advancements in quantum computing. Working collaboratively with colleagues across the company, our teams drive breakthrough research and accelerate real-world solutions for products, businesses, science and society. As research comes to reality, we uncover new research opportunities, driving innovation further and faster. I call this powerful, cyclical relationship between research and real-world impact the magic cycle of research .

This cycle is accelerating significantly these days, propelled by more powerful models, new agentic tools that help accelerate scientific discovery, and open platforms and tools. We see this momentum across domains .

Link to Youtube Video

At Research@MTV last week, we highlighted three of our latest breakthroughs: Google Earth AI, DeepSomatic, and Quantum Echoes.

Earth AI is a powerful collection of geospatial AI models and reasoning designed to address critical global challenges; it gives users an unprecedented level of understanding about what is happening across the planet.

For years we’ve been developing state-of-the-art geo-spatial AI models including floods , wildfires , cyclones , air quality , pollen , weather nowcasting and long range forecasting , agriculture , population dynamics , AlphaEarth Foundations and mobility . These models, developed by teams across Google, are already helping millions of people worldwide and we keep making progress. We have just expanded access to our new Remote Sensing Foundations and new global Population Dynamics Foundations. And we can now share that our riverine flood models — expanded over the years to cover 700 million people in 100 countries — now provide forecasts covering over 2B people in 150 countries for significant riverine flood events.

Earth AI is a Google-wide program building on our long-standing efforts. Our latest research updates to Earth AI integrate and synthesize these vast amounts of real-world imagery, population and environmental data. Using LLMs and their reasoning capabilities, the Earth AI geospatial reasoning agent can understand nuanced concepts and discover correlations across multiple datasets and models. This agent allows users to ask complex questions and receive answers in plain language, making Earth AI capabilities accessible even to non-experts. Users can quickly generate insights from business logic use cases and supply chain management to crisis resilience and international policy.

In our evaluations, Geospatial Reasoning Agent improved responses over baseline models that did not have access to Earth AI models and tools. We share the results in our research blog and our technical report .

Google Earth with Gemini capabilities will soon be powered by our Earth AI imagery models, enabling users to search for objects in satellite imagery. Plus, our powerful models are now available to trusted testers on Google Cloud. And we continue to hear from our partners about diverse important use cases, including testimonials from Give Directly , McGill and Partners , Cooper/Smith , WPP , WHO AFRO , Planet Labs and Airbus .

DeepSomatic , published in Nature Biotechnology , is our newest of many AI tools designed to help the scientific community and health practitioners.

DeepSomatic builds on 10 years of genomics research at Google. Since 2015, we’ve been building models like DeepConsensus and DeepVariant to help us better understand the genome. With these models, we’ve helped map human and non-human genomes and used this information to inform our understanding of disease.

Some cancers have complex genetic signatures that may make them targets for tailored treatments based on their specific mutations. So, we asked ourselves if we could sequence the genomes of these cancerous cells more precisely. The result, DeepSomatic, is our new open-source AI-powered tool to help scientists and doctors make sense of genetic variants in cancer cells.

The model works by first turning genetic sequencing data into a set of images and then using a convolutional neural network to differentiate between the reference genome, the non-cancer germline variants in that individual, and the cancer-caused somatic variants in the tumor.

Identifying cancer variants could potentially lead to brand-new therapies, and it could help clinicians decide between treatments such as chemotherapy and immunotherapy. Our partners at Children’s Mercy are using it to pinpoint how and why a particular form of cancer affects a patient in order to create personalized cures.

DeepSomatic follows other breakthroughs which share the same goal of using AI to help fight cancer. We also just released a 27 billion parameter foundation model for single-cell analysis, C2S-Scale , in collaboration with Google DeepMind. This builds upon our work from earlier this year , in collaboration with Yale, and recently generated a novel hypothesis about cancer cellular behavior. With more clinical tests, this may reveal a promising new pathway for developing therapies to fight cancer.

To accelerate the next exponential wave of scientific discovery, we’re looking to our strategic, long-term investment in quantum computing.

Our foundation rests on decades of research, leading to our hardware milestone on the Willow chip in late 2024. This work is supported by Michel Devoret, our Chief Scientist of Quantum Hardware, who together with with former Quantum AI hardware lead John Martinis, and John Clarke of the University of California, Berkeley, became 2025 Physics Nobel Laureates for their research in the 1980s that laid the groundwork for today's superconducting qubits.

Now we’ve announced a new verifiable quantum advantage , published in the cover of Nature . Our “ Quantum Echoes ” algorithm runs on our Willow chip 13,000 times faster than the best classical algorithm on one of the world’s fastest supercomputers. It offers a new way to explain interactions between atoms in a real world molecule observed using nuclear magnetic resonance spectroscopy . This is the world’s first algorithm to demonstrate verifiable quantum advantage and points towards practical applications of quantum computing that are beyond the capabilities of classical computers.

Quantum computing has the potential to meaningfully advance drug design and help make fusion energy a reality. And given our latest breakthrough, we’re optimistic that we’ll start to see real-world applications within five years.

Link to Youtube Video

Fireside Chat about Quantum AI with James Manyika and Hartmut Neven.

We also shared some of the work across various domains where teams are driving breakthrough research and accelerating real-world solutions. The breadth and depth of the opportunities is ever increasing. Here are a few recent examples.

AI co-scientist is a multi-agent AI system built as a virtual scientific collaborator to help scientists generate novel hypotheses and research proposals, and to accelerate scientific and biomedical discoveries. Our new AI-powered empirical software system, a Gemini-backed coding agent, helps scientists write expert-level empirical software. It accelerates the historically slow task of creating custom software to evaluate and iteratively improve scientific hypotheses. This opens the door to a future where scientists can easily, rapidly, and systematically investigate hundreds or thousands of potential solutions to the problems that motivate their research.

AMIE , a conversational medical AI agent, demonstrates clinical reasoning and communication on par with primary care physicians in both multimodal and multi-visit settings . As we explore how AMIE may translate to real-world environments, we are testing it under physician oversight , including in a partnership with Beth Israel Deaconess Medical Center to evaluate AMIE with real-world patients.

MedGemma , part of our Health AI Developer Foundations ( HAI-DEF ) collection, is Google's most capable open model for multimodal medical comprehension. Since launch MedGemma and HAI-DEF have >1M downloads and >40K unique users.

We continue advancing our research on factuality and grounding for LLMs, including studying how LLMs convey uncertainty , assessing whether LLMs encode more factual knowledge in their parameters than they express in their outputs, and more. We expand to multimodal content - for example, Time-Aligned Captions and our contrastive sequential video diffusion method focus on making scenes in videos visually consistent, helping improve the quality of our image and video models.

Improving the efficiency of LLMs remains a high priority goal across the industry. Building on our speculative decoding work which enabled substantial efficiency gains without any compromise on quality, we keep seeing many new approaches , such as our recent speculative cascades . We keep advancing other techniques for efficiency and for energy innovation techniques.

Algorithmic research contributes to new Ads model connecting advertisers to customers, continued research on our large-scale optimisations, enhancements to Google Maps routing and improved voice search in India. Privacy research includes recent advances such as confidential federated analytics , differentially private synthetic data and provably private insights into AI use . We are making progress on TimesFM which has hundreds of millions of queries per month in BigQuery alone, and recently introduced a novel approach using in-context fine-tuning .

We keep exploring new ways to improve learning and education, building on our earlier work on LearnLM , such as Learn Your Way to improve learning efficacy. And we keep exploring AI innovations such as the use of diffusion models for real-time game engines , which inspire new horizons for simulating immersive world environments.

Link to Youtube Video

At Research@ Mountain View, Yossi Matias joins Alex Kantrowitz on the Big Technology Podcast to discuss our research efforts in areas like cancer treatment and Quantum.

The magic cycle of research is quickly gaining momentum. This is propelled by more powerful models, by agentic tools like the AI co-scientist and AI-based expert-level empirical software that help accelerate scientific discovery, and open platforms and tools like MedGemma , HAI-DEF and DeepSomatic . Innovation today is happening at unprecedented speed.

The latest advancements point to a world where AI is not just a tool, but an essential partner and collaborator. This partnership is already taking shape in tangible ways, empowering researchers, engineers, healthcare workers, and educators. With humans at the steering wheel, we can leverage AI to bring new ideas to life and take on the challenges that matter most.

This fusion of human ingenuity with the powerful capabilities of AI will fuel further innovation and accelerate its impact for people at a global scale, defining a new era of scientific discovery for the benefit of everyone, everywhere.

November 18, 2025

November 13, 2025

November 13, 2025"
Toward provably private insights into AI use,https://research.google/blog/toward-provably-private-insights-into-ai-use/,"Artem Lagzdin, Software Engineer, and Daniel Ramage, Research Director, Google Research

We detail how confidential federated analytics technology is leveraged to understand on-device generative AI features, ensuring strong transparency in user data handling and analysis.

Generative AI (GenAI) enables personalized experiences and powers the creation of unstructured data, including summaries, transcriptions, and more. Insights into real-world AI use [ 1 , 2 ] can help GenAI developers enhance their tools by understanding common applications and identifying failure modes. And especially when those tools are applied to on-device data, our goal is to offer increasingly robust privacy guarantees during the insight generation process. This post introduces provably private insights (PPI), a new north star for generating dynamic insights into how people use LLMs and GenAI tools while guaranteeing that individual data is not inspectable and that aggregate insights are anonymous.

Today we announce a first-of-its kind PPI system that leverages the power of large language models (LLMs), differential privacy (DP), and trusted execution environments (TEEs) to analyze unstructured GenAI data. This system proves that server-side processing is limited to privacy-preserving computations and can be fully externally inspected. With our system, GenAI tool developers can analyze interactions using a “data expert” LLM, tasked with answering questions like “what topic is being discussed?” or “is the user frustrated?” The LLM’s answers are aggregated with DP to provide a comprehensive view of GenAI feature usage across the user population without exposing unaggregated data. The “data expert” LLM itself resides within the TEE. PPI is enabled by confidential federated analytics (CFA), a technique first deployed in Gboard , where open source analysis software runs in TEEs, offering complete transparency into the mechanisms and privacy properties of server-side data processing. Our deployment of PPI in the Recorder application for Pixel leverages Google’s latest open-source Gemma models as the “data expert” to offer insights into Recorder usage.

To encourage the external community to verify our claims, we’ve open-sourced LLM-powered privacy preserving insights as part of confidential federated analytics in Google Parfait , along with the rest of our TEE-hosted confidential federated analytics stack.

Google’s CFA leverages confidential computing to protect unaggregated user data during processing, and only releases outputs with a formal (user-level) DP guarantee. CFA provides strong data isolation and anonymization guarantees regardless of what query an analyst runs.

In this technique, user devices first decide what data should be uploaded for analysis. Devices encrypt and upload this data, along with the processing steps that the server is authorized to use for decryption. Uploaded data is protected with encryption keys managed by a TEE-hosted key management service, which releases decryption keys only to device-approved processing steps. Devices can verify that the key management service is the expected open source code (included in a public, tamper-resistant transparency log, Rekor ), and that the code is running in a properly configured TEE that is inaccessible to Google. The key management service in turn verifies that the approved, public processing steps are running in TEEs. No other analyses can be performed on the data and no human can access data from individual devices.

Private insights are derived by passing the data through a well-defined set of processing steps. First, unstructured raw data is analyzed by an LLM tasked with extracting the answer to a specific question, such as the category or topic of the input (“structured summarization”). Processing begins by using an open-source Gemma 3 model to classify transcripts into categories of interest. These classes are then summed to compute a histogram of topics with differentially private noise guaranteeing that the output histogram cannot be strongly influenced by any one user. The LLM’s prompt can be changed frequently, because the DP guarantee applies to the aggregation algorithm regardless of the LLM prompt. Even if the developer asked a question designed to single out one user, the differential private statistics would not reveal it.

All privacy-relevant parts of our system are open source and reproducibly buildable — from the private aggregation algorithm to the TEE stack — and the LLM itself is also open source. The signatures of the workflows used to analyze the data are also public. When combined with TEEs' ability to attest to the state of the system running the software, every part of the data processing pipeline can be verifiably linked to published code. This provides external parties the ability to verify our privacy claims. This commitment to end-to-end verifiability is how the system makes progress toward being provable — we anchor on this capability, allowing third parties to inspect the open-source code and confirm that it is exactly the code we claim to run, thereby proving to clients that this is the only code their data will be processed with, subject to known weaknesses in current-generation TEEs.

In short, provably private insights can be generated by an LLM-powered structured summarization workflow in confidential federated analytics. The combination of structured summarization with differentially private histogram generation enables deeper understanding into how the GenAI tools are used in the real world, all while guaranteeing privacy. Technical details of the system can be found in the whitepaper .

Google’s Recorder app on Pixel offers powerful AI features, such as transcription, summarization, and speaker labeling. A key challenge for the application developers is to understand how users interact with these features. For instance, are users creating ""Notes to self,"" ""Reminders,"" or recording ""Business meetings""? Traditional count-based analytics are insufficient to analyze such data without the help of structured summarization or another form of classification. In a traditional setting, a system would log these transcripts to a central server for classification, and then run (differentially) private count queries on the results. PPI operates in a similar way but without the risk of data being used for any other purpose.

In the Recorder application, a subset of transcripts (from users who have enabled “Improve for everyone” in settings) are encrypted with a public key managed by a central TEE-hosted keystore protected via Google’s Project Oak attestation stack running on AMD Secure Encrypted Virtualization-Secure Nested Paging (SEV-SNP) CPUs. The keystore ensures that the uploaded data can be decrypted only by pre-approved processing steps, themselves attested to running the expected processing steps in TEEs. A Gemma 3 4B model running within the AMD SEV-SNP TEE classifies the transcripts into topics, which are then aggregated with differential privacy. External parties can verify that raw transcripts never leave the secure environment of the TEE, and only private sums of the summarized output categories are released to Google.

An example differentially private distribution of Recorder transcripts across various topics, as categorized by Gemma. Inner rectangle size is proportional to relative topic frequency.

PPI can also help evaluate the performance of on-device GenAI features, such as the accuracy of summaries generated by Recorder. Instead of relying solely on synthetic data, which may not accurately represent real-world use, CFA can run an LLM auto-rater as a part of the structured summarization component. This auto-rater LLM also resides within the TEE and can assess the results of the on-device model, ensuring a more accurate and privacy-preserving evaluation. This allows developers to fine-tune the on-device model based on real user interactions without compromising individual privacy.

The configuration we’re running in Recorder is available in our GitHub repository which can be connected to the specific code paths and privacy guarantees by following these instructions . The Recorder configuration guarantees that whatever LLM query is run, it is passed through the auto-tuned DP histogram aggregator with strict privacy guarantees (user-level ε = 1 used in the figure above).

This work demonstrates that provably private insights are possible: real-world GenAI tool use is analyzed with LLMs and then aggregated into differentially private statistics, all with full transparency into the server-side processing steps. Every step of the insight generation process has been designed to offer state-of-the-art data isolation and anonymization, and external verifiers can check the source code of the methods and the proof that we run them.

Moreover, we’ve shared LLM-powered structured summarization as a first application. We expect others, including differentially private clustering and synthetic data generation to follow, all with the same level of verifiability and confidentiality. And with future work to enable confidential use of higher-throughput accelerators such as Google TPUs , richer analyses will become possible, including detailed transcript analysis and auto-rating. Insight generation is now possible without exposing sensitive user data outside of the confidential computation boundary, and with strong user-level DP guarantees for generated insights. We are excited that the technology for provably private insights is maturing just as GenAI tools are beginning to apply to on-device and sensitive-data experiences.

We thank the teams at Google that helped with algorithm design, infrastructure implementation, and production maintenance of this system, in particular teams led by Marco Gruteser, Peter Kairouz, and Timon Van Overveldt, with product manager Prem Eruvbetine, including: Albert Cheu, Brett McLarnon, Chunxiang (Jake) Zheng, Edo Roth, Emily Glanz, Grace Ni, James Bell-Clark, Kassem Fawaz, Katharine Daly, Krzysztof Ostrowski, Maya Spivak, Mira Holford, Nova Fallen, Rakshita Tandon, Ren Yi, Stanislav Chiknavaryan, Stefan Dierauf, Steve He, and Zoe Gong. We also thank close partners who supported this system through technologies and the Recorder integration, including: Allen Su, Austin Hsu, Console Chen, Daniel Minare Ho, Dennis Cheng, Jan Wassenberg, Kristi Bradford, Ling Li, Mina Askari, Miranda Huang, Tam Le, Yao-Nan Chen, and Zhimin Yao. This work was supported by Corinna Cortes, Jay Yagnik, Ramanan Rajeswaran, Seang Chau, and Yossi Matias. We additionally thank Peter Kairouz, Marco Gruteser, Mark Simborg, and Kimberly Schwede for feedback and contributions to the writing of this post.

November 18, 2025

November 12, 2025

November 7, 2025"
StreetReaderAI: Towards making street view accessible via context-aware multimodal AI,https://research.google/blog/streetreaderai-towards-making-street-view-accessible-via-context-aware-multimodal-ai/,"Jon E. Froehlich, Visiting Faculty Researcher, and Shaun Kane, Research Scientist, Google Research

We introduce StreetReaderAI, a new accessible street view prototype using context-aware, real-time AI and accessible navigation controls.

Interactive streetscape tools, available today in every major mapping service, have revolutionized how people virtually navigate and explore the world — from previewing routes and inspecting destinations to remotely visiting world-class tourist locations. But to date, screen readers have not been able to interpret street view imagery, and alt text is unavailable. We now have an opportunity to redefine this immersive streetscape experience to be inclusive for all with multimodal AI and image understanding. This could eventually allow a service like Google Street View, which has over 220 billion images spanning 110+ countries and territories, to be more accessible to people in the blind and low-vision community, offering an immersive visual experience and opening up new possibilities for exploration.

In “ StreetReaderAI: Making Street View Accessible Using Context-Aware Multimodal AI ”, presented at UIST’25 , we introduce StreetReaderAI, a proof-of-concept accessible street view prototype that uses context-aware, real-time AI and accessible navigation controls. StreetReaderAI was designed iteratively by a team of blind and sighted accessibility researchers, drawing on previous work in accessible first-person gaming and navigation tools, such as Shades of Doom , BlindSquare , and SoundScape . Key capabilities include:

StreetReaderAI provides a context-aware description of the street view scene by inputting geographic information sources and the user’s current field-of-view into Gemini. For the full audio-video experience, including sound, please refer to this YouTube video .

StreetReaderAI uses Gemini Live to provide a real-time, interactive conversation about the scene and local geographic features. For the full audio-video experience, including sound, please refer to this YouTube video .

StreetReaderAI offers an immersive, first-person exploration experience, much like a video game where audio is the primary interface.

StreetReaderAI provides seamless navigation through both keyboard and voice interaction. Users can explore their surroundings using the left and right arrow keys to shift their view. As the user pans, StreetReaderAI shares audio feedback, voicing the current heading as a cardinal or intercardinal direction ( e.g., “ Now facing: North ” or “ Northeast ”). It also expresses whether the user can move forward and if they are currently facing a nearby landmark or place.

To move, the user can take “virtual steps” using the up arrow or move backward with the down arrow. As a user moves through the virtual streetscape, StreetReaderAI describes how far the user traveled and key geographic information, such as nearby places. Users can also use “jump” or “teleport” features to quickly move to new locations.

The core of StreetReaderAI is its two underlying AI subsystems backed by Gemini: AI Describer and AI Chat. Both subsystems take in a static prompt and optional user profile as well as dynamic information about the user’s current location, such as nearby places, road information, and the current field-of-view image (i.e., what’s being shown in Street View).

AI Describer functions as a context-aware scene description tool that combines dynamic geographic information about the user’s virtual location along with an analysis of the current Street View image to generate a real-time audio description.

It has two modes: a “default ” prompt emphasizing navigation and safety for blind pedestrians, and a “tour guide ” prompt that provides additional tourism information (e.g., historic and architectural context). We also use Gemini to predict likely follow-up questions specific to the current scene and local geography that may be of interest to blind or low-vision travelers.

A diagram of how AI Describer combines multimodal data to support context-aware scene descriptions.

AI Chat builds on AI Describer but allows users to ask questions about their current view, past views, and nearby geography. The chat agent uses Google's Multimodal Live API , which supports real-time interaction, function calling, and temporarily retains memory of all interactions within a single session. We track and send each pan or movement interaction along with the user's current view and geographic context (e.g., nearby places, current heading).

What makes AI Chat so powerful is its ability to hold a temporary “memory” of the user's session — the context window is set to a maximum of 1,048,576 input tokens, which is roughly equivalent to over 4k input images. Because AI Chat receives the user's view and location with every virtual step, it collects information about the user’s location and context. A user can virtually walk past a bus stop, turn a corner, and then ask, “ Wait, where was that bus stop? ” The agent can recall its previous context, analyze the current geographic input, and answer, “ The bus stop is behind you, approximately 12 meters away. ”

To evaluate StreetReaderAI, we conducted an in-person lab study with eleven blind screen reader users. During the sessions, participants learned about StreetReaderAI and used it to explore multiple locations and evaluate potential walking routes to destinations.

A blind participant using StreetReaderAI to explore potential travel to a bus stop and inquire about bus stop features, such as the existence of benches and a shelter. For the full audio-video experience, including sound, please refer to this YouTube video .

Overall, participants reacted positively to StreetReaderAI, rating the overall usefulness 6.4 (median=7; SD=0.9) on a Likert scale from 1–7 (where 1 was ‘not at all useful’ and 7 was ‘very useful’), emphasizing the interplay between virtual navigation and AI, the seamlessness of the interactive AI Chat interface, and the value of information provided. Qualitative feedback from participants consistently highlighted StreetReaderAI's significant accessibility advancement for navigation, noting that existing street view tools lack this level of accessibility. The interactive AI chat feature was also described as making conversations about streets and places both engaging and helpful.

During the study, participants visited over 350 panoramas and made over 1,000 AI requests. Interestingly, AI Chat was used six times more often than AI Describer, indicating a clear preference for personalized, conversational inquiries. While participants found value in StreetReaderAI and adeptly combined virtual world navigation with AI interactions, there is room for improvement: participants sometimes struggled with properly orienting themselves, distinguishing the veracity of AI responses, and determining the limits of AI knowledge.

In one study task, participants were given the instruction, “Find out about an unfamiliar playground to plan a trip with your two young nieces.” This video clip illustrates the diversity of questions asked and the responsiveness of StreetReaderAI. For the full audio-video experience, including sound, please refer to this YouTube video .

As the first study of an accessible street view system, our research also provides the first-ever analysis of the types of questions blind people ask about streetscape imagery. We analyzed all 917 AI Chat interactions and annotated each with up to three tags drawn from an emergent list of 23 question type categories. The four most common question types included:

Because StreetReaderAI relies so significantly on AI, a critical challenge is response accuracy. Of the 816 questions that participants asked AI Chat:

Of the 32 incorrect responses:

More work is necessary to explore how StreetReaderAI performs in other contexts and beyond lab settings.

StreetReaderAI is a promising first step toward making streetscape tools accessible to all. Our study highlights what information blind users desire from and ask about streetscape imagery and the potential for multimodal AI to answer their questions.

There are several other opportunities to expand on this work:

Though a “proof-of-concept” research prototype, StreetReaderAI helps demonstrate the potential of making immersive streetscape environments accessible.

This research was conducted by Jon E. Froehlich, Alexander J. Fiannaca, Nimer Jaber, Victor Tsaran, Shaun K. Kane, and Philip Nelson. We thank Project Astra and the Google Geo teams for their feedback as well as our participants. Diagram icons are from Noun Project, including: “ prompt icon ” by Firdaus Faiz, “ command functions ” by Kawalan Icon, “ dynamic geo-context ” by Didik Darmanto, and “ MLLM icon ” by Funtasticon.

November 18, 2025

November 7, 2025

November 6, 2025"
How we are building the personal health coach,https://research.google/blog/how-we-are-building-the-personal-health-coach/,"Shwetak Patel, Distinguished Scientist & Health Technologies Lead, and Florence Thng, Health Intelligence PM Director, Google

The personal health coach is built with Gemini models to deliver personalized and adaptive coaching, grounded in science and informed by expert oversight.

Historically, health and fitness journeys have been fragmented, generic and inaccessible, whether within existing apps or through general health and fitness journeys outside of apps. For instance, a primary care provider might suggest seeing a specialist or losing weight for better diabetes management, but often without providing connections to a nutritionist or fitness coach. This leaves users with the burden of connecting these dots themselves.

Our vision is to address this by offering a proactive, personalized and adaptive AI-powered personal health coach. This coach will seamlessly enable:

Starting tomorrow and over the next week, we are rolling out an optional public preview of the health coach for eligible US-based Fitbit Premium Android users and expanding to iOS users soon. Users who opt in to participate will be presented the consent to provide access to their Fitbit data to receive personalized insights.

This innovation is powered by advances in Gemini models plus our new AI-first personal health coach experience on the Fitbit app, and our continuous progress in cutting-edge research across Fitbit, Google Research and Google DeepMind. Building from the ground up takes time and rigor, a commitment especially important for health and wellness. We are approaching this thoughtfully and iteratively, grounded in science, incorporating feedback from the scientific community and users. We will continue to be open about our work through publications and updates, and here we provide a quick peek into what went into building the personal health coach.

“Do I get better sleep after exercising?” sounds like a simple question, but answering it like a proactive, personalized and adaptive coach required several technical innovations.

First, we need the coach to understand and do numerical reasoning on physiological time series data such as sleep and activity, using capabilities similar to those showcased by PH-LLM . For questions like this, the coach verifies recent data availability, chooses the right metrics, contrasts relevant days, contextualizes results against personal baselines and population-level statistics, incorporates prior interactions with the coach, and finally uses the analysis to provide tailored answers and insights.

Second, we utilize a multi-agent framework that coordinates expert sub-agents to provide clear, consistent and holistic support, such as (1) a conversational agent for multi-turn conversations, intent understanding, agent orchestration, context gathering and response generation; (2) a data science agent that iteratively uses tools to fetch, analyze, and summarize relevant data (e.g., sleep and workout data), leveraging code-generation capabilities as needed ; and (3) a domain expert, such as a fitness expert that analyzes user data to generate personalized fitness plans and adapt them as progress and context change.

Schematic of how the personal health coach works. The user can ask questions conversationally. Behind the scenes, a conversation agent handles the conversation, gathers context, and orchestrates other agents. The other agents include a data science agent focusing on retrieving and analyzing relevant data, and a domain expert agent that understands specific fields such as fitness.

Third, while foundational models are incredibly capable, careful steer is required for it to be useful in the health and wellness context. We developed evaluations based on consumer health and wellness needs to inform the system instructions and improve upon Gemini’s core capabilities in assisting our users.

Working together , these innovations result in more personalized and beneficial guidance.

We know that technical excellence alone isn’t enough, and reliability and safety are paramount.

First, we grounded the coach in scientific and well established coaching and fitness frameworks .

We also used human-centered design to integrate expert and user feedback including:

Finally, we continuously validate the personal health coach using dimensions of safety, helpfulness, accuracy, relevance, and personalization, collectively known as the SHARP evaluation framework . This multi-level assessment involves over 1 million human annotations and more than 100k hours of human evaluation by generalists and experts across various fields such as sports, sleep, family medicine, cardiology, endocrinology, exercise and behavioral science. This process is further extended and scaled with autoraters to ensure that wellness recommendations are scientifically accurate. The framework also incorporates the coach’s real-world performance, enabling ongoing improvements in the most critical areas.

Join the public preview and share your feedback in the app or through our community forum . You will help shape the coach, so it can do more for and with you.

Public Preview eligibility criteria are subject to change without prior notice. Features will be added incrementally and on a rolling basis, so your experience may vary as new functionalities are introduced. Not a medical device. This product is intended for general wellness and fitness purposes only. Always consult a qualified healthcare professional for any health concerns.

November 18, 2025

November 7, 2025

October 31, 2025"
Google Earth AI: Unlocking geospatial insights with foundation models and cross-modal reasoning,https://research.google/blog/google-earth-ai-unlocking-geospatial-insights-with-foundation-models-and-cross-modal-reasoning/,"Niv Efron, Senior Director of Engineering, and Luke Barrington, Director of Product Management, Google Research

Google Earth AI is our family of geospatial AI models and reasoning agents that provides users with actionable insights, grounded in real-world understanding. Today, we’re sharing our latest Earth AI innovations and expanding access to these new capabilities on Google Earth and Google Cloud.

For years, Google has developed AI models that enhance our understanding of the planet. These models help keep Google products fresh, for example, ensuring Maps is accurate by analyzing satellite images and giving Search users the most up-to-date alerts about weather and natural disasters .

As individual models grow more powerful, we’ve learned that many real-world questions require the combination of insights across domains. Answering complex queries like, ""Where is a hurricane likely to make landfall? Which communities are most vulnerable and how should they prepare?"" requires reasoning about imagery, population and the environment.

Earlier this year, we introduced Google Earth AI to solve this core challenge. By pairing our family of powerful foundation models with a geospatial reasoning agent, which uses our latest Gemini models, it’s becoming possible to perform complex, real-world reasoning at planetary scale. The models provide detailed understanding of our planet, grounded in real-world data. The agent, in turn, acts as an intelligent orchestrator. It deconstructs a complex question into a multi-step plan; executes the plan by calling on these foundation models, querying vast datastores, and using geospatial tools; and finally fuses the results at each step into a holistic answer.

Today, we're introducing new Earth AI innovations :

To learn more, we invite you to read our full technical paper, "" Google Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning "". You can also get involved by expressing interest as we expand access to these new capabilities for developers and enterprises.

Earth AI unites state-of-the-art models with geospatial reasoning agents to address critical global challenges.

Our new Remote Sensing Foundations models simplify and accelerate satellite imagery analysis using three core capabilities: vision-language models, open-vocabulary object detection, and adaptable vision backbones. Users can ask natural language queries, like ""find all flooded roads"" in an image captured after a storm, and get rapid, accurate answers. Our models are trained on a large corpus of high-resolution overhead imagery, paired with text descriptions. They achieve state-of-the-art results on multiple public Earth observation benchmarks. For instance, we achieve >16% average improvement on text-based image search tasks, while our zero-shot model for novel object detection more than doubles the baseline accuracy.

Model evaluation shows significant Average Precision (AP50) improvement of our Remote-Sensing optimized RS-OWL-ViT-v2 model (“Ours”) over the OWL-ViT-v2 open vocabulary detection model in a zero-shot setting and illustrates the advantage of the combined FLAME + RS-OWL-ViT-v2 approach (""Ours"") over SIoU for few-shot detection on novel classes.

This area of research, which includes Mobility AI and Population Dynamics Foundations , aims to understand the complex interplay between people and places. Our latest research in Population Dynamics Foundations introduces two key innovations: globally-consistent embeddings across 17 countries and monthly updated embeddings that capture the changing dynamics of human activity, which are critical for time-sensitive predictions. Population Dynamics Foundations has shown remarkable effectiveness in independent studies; for example, researchers at the University of Oxford found that incorporating these embeddings into a forecasting model for Dengue fever in Brazil improved long-range R² (a metric that measures how well a model explains the actual disease rates) from 0.456 to 0.656 for 12-month predictions.

Evaluation of our Population Dynamics Foundations across 17 countries; R2 score (range is 0–1, higher is better) by country for predicting population density, tree cover, night time lights, and elevation. The global trend matches the strong performance we originally demonstrated in the US only.

Similarity per dimension of the Population Dynamics Foundations embeddings, visualized by US zip code. The patterns across dimensions capture the diverse characteristics of the US population.

Our previously-published research demonstrates state-of-the-art forecasts for medium-range weather , monsoon onsets , air quality and riverine floods . We've recently expanded these Environment models to make precipitation nowcasts for the entire planet , and we’re now covering 2 billion people with forecasts for the most significant riverine floods.

While each foundation model provides powerful insights, our findings confirm that combining models yields even more predictive power. This synergistic approach produces a more comprehensive and accurate understanding of real-world phenomena and dramatically improves predictions across critical applications.

For example, FEMA’s National Risk Index shows which communities are most at risk to natural hazards like floods and storms, based on a variety of factors including economic and social vulnerability as well as physical and environmental risk. By fusing embeddings that capture socio-economic features from our Population Dynamics Foundations and landscape features from AlphaEarth Foundations , we improved prediction of FEMA’s National Risk Index by an average of 11% in R² across 20 different hazards, versus using either data source alone, with the most significant gains in predicting risk from tornadoes (+25% R²) and riverine flooding (+17% R²).

The example above illustrates that tackling real-world problems requires insights from multiple models with diverse capabilities. Orchestrating these Earth AI insights is simplified by our new Gemini-powered Geospatial Reasoning agent. The agent deconstructs complex, natural language queries and plans a dynamic, multi-step path to an answer. To execute each step, the agent can call on “expert” sub-agents that are equipped with Earth AI models described above, as well as the vast, real-world data found in Data Commons , Earth Engine , and geospatial-specific tools. This modular network of agents allows for extensibility and customization.

To see how it works, consider a user who wishes to identify specific populations that are vulnerable to the risk of an oncoming storm. The agent executes a transparent series of reasoning steps:

To assist a user in understanding vulnerability to an oncoming storm, our Gemini-powered Geospatial Reasoning agent uses our Environment model to identify the likely path of hurricane force winds, intersects this with country boundaries and population density from Big Query and Data Commons, and reasons across all of this data to pick the most critical locations. It also trains a model on the fly to generate higher resolution vulnerability data using Population Dynamics Foundations. And identifies critical infrastructure in satellite imagery using Remote Sensing Foundations.

To evaluate the agent, we developed two new methods for evaluation: a Q&A benchmark for fact-finding and analysis with verifiable ground truth answers based on publicly available data and Crisis Response case studies for complex, predictive scenarios (e.g., solving the entire challenge above).

On the Q&A benchmark, our Geospatial Reasoning Agent achieved an overall accuracy of 0.82, significantly outperforming the baseline Gemini 2.5 Pro (0.50) and Gemini 2.5 Flash (0.39) agents (scores derived from ROUGE-L F1 and percentage error, higher is better). This highlights the importance of giving agents access to specialized geospatial models and tools for these types of queries.

Visualizing the performance of agents on the Q&A benchmark. The Geospatial Reasoning agent outperformed the baseline Gemini 2.5 Pro agent by 37% in the Descriptive and Retrieval category, and 124% in the more complex Analytical and Relational category, for an overall 64% higher score (scores derived from ROUGE-L F1 and percentage error).

In the more complex Crisis Response scenarios, our paper demonstrates the benefit of orchestrating a diverse set of Environment, Remote Sensing and Population Dynamics insights via case studies. Leveraging specialized sub-agents for geospatial and demographic analysis, we’re able to solve real-world analysis tasks.

Earth AI represents a fundamental leap in planetary understanding. Our findings show that a multimodal, reasoning-based approach, built upon a foundation of state-of-the-art geospatial AI models, can unlock insights that are intractable with siloed analysis alone.

We are just beginning to explore the full potential of Earth AI and are committed to expanding access in order to help the global community address the planet’s most pressing challenges. For example:

In addition to supporting UN Global Pulse, GiveDirectly, and other organizations using Earth AI, Google.org is providing funds to partners like Khushi Baby , Cooper/Smith , Direct Relief and Froncort.ai who are utilizing Population Dynamics Foundations to model infectious diseases and improve public health action globally. New enterprise users of Earth AI include Public Storage , CARTO and Visiona Space Technology (part of Embraer ).

We want to hear how Earth AI might be helpful to you. We encourage organizations to express interest in getting early access to Remote Sensing Foundations (available as Imagery models in Vertex AI ), Population Dynamics Foundations, and Geospatial Reasoning.

November 18, 2025

November 13, 2025

November 7, 2025"
A verifiable quantum advantage,https://research.google/blog/a-verifiable-quantum-advantage/,"Xiao Mi and Kostyantyn Kechedzhi, Research Scientists, Google Quantum AI

In our latest Nature publication, we introduce a new quantum computational task measuring Out-of-Time-Order Correlators (OTOCs). This work demonstrates a verifiable quantum advantage and paves the way for solving real-world problems like Hamiltonian learning in Nuclear Magnetic Resonance (NMR).

Nature is brimming with chaos, a phenomenon characterized by the high sensitivity of a system toward small perturbations. In the macroscopic world, notable examples of chaotic systems include weather patterns, wherein a small change in initial conditions leads to vastly different outcomes over time (often dubbed “the butterfly effect ”), and population dynamics, where small shifts in local populations may eventually affect the entire ecosystem. Chaos is similarly abundant in quantum systems, with examples including the dynamics of magnetization of atomic nuclei when subjected to a time-varying magnetic field, and the flow of electrons in high-temperature superconductors .

Simulating quantum-chaotic systems is challenging for classical computation due to exponentially scaling computational cost, making quantum computers ideal for achieving quantum advantage. In 2019, we demonstrated the first beyond-classical quantum computation by sampling bitstrings from a highly chaotic quantum state of qubits . However, this random circuit sampling approach has limited practical utility since the same bitstring never appears twice in a large quantum system, restricting its ability to reveal useful information.

In “ Observation of constructive interference at the edge of quantum ergodicity ”, featured on the cover of Nature , we introduce and experimentally demonstrate a quantum algorithm which we call Quantum Echoes. The heart of the algorithm is measuring the expectation value of a quantum observable , called the out-of-time-order correlator (OTOC). OTOC and its higher order generalizations are a new family of observables that describe how quantum dynamics become chaotic. Unlike bitstrings, quantum expectation values, e.g., current, velocity, magnetization and density, are verifiable computational outcomes that remain the same when run on different quantum computers. The wide relevance of expectation values combined with their verifiability indicates a direct path toward using OTOCs to solve real-world problems using quantum computers, which are not possible to solve on classical computers. Remarkably, we show that running the Quantum Echoes algorithm on the Willow quantum chip is already in the beyond-classical regime for a set of benchmarking quantum circuits.

In practice, OTOC represents the state of a single qubit at the end of a series of quantum operations. In our experiments running Quantum Echoes on Willow , a total of 103 qubits underwent both “forward” ( U ) and “backward” ( U † ) evolutions in the form of random quantum circuits. A forward evolution applied to a state where all qubits are independent from each other brings the system to a highly chaotic state with quantum correlations across all qubits. A perturbation, a one-qubit operation B , is applied to a qubit in between the two time evolutions. This circuit is followed by another probe, a one-qubit operation M . Repeating this process once or twice leads to an OTOC of first or second order. In absence of B the forward and backward evolution returns the system to the initial state, where all qubits are independent. Inclusion of the perturbation B sets off a butterfly effect: after such perturbed forward and backward evolution, the whole system ends in a chaotic state with quantum correlations across all qubits that is very different from the initial state.

A crucial insight we obtained from our experiments is that higher-order OTOCs exhibit complex quantum interference effects analogous to a traditional interferometer . This is known as many-body interference, meaning the quantum states of many particles interfere with each other, much like waves of water might interfere, leading to complex overall effects. Here the perturbations, B and M , act as imperfect mirrors that modify the system’s trajectories. Higher order OTOCs become more sensitive to the perturbation due to increasing number of “round trip” evolutions, where the trajectories bounce off of B and M . When a resonance condition is satisfied, which corresponds to evolution U † being the exact inverse of U , the interference is constructive and it amplifies the subset of quantum correlations from the totality of those present in the chaotic state. More specifically, this interferometry reveals how the evolution U generates correlations between the two qubits where operations B and M were applied. It can be used as a sensitive instrument to characterize the evolution of U .

Left : Quantum circuit measuring OTOCs of different orders, k. Qubits are initiated in the ground state, with one qubit in the state denoted by |𝜓M〉. A complex many-body evolution (U) is implemented by the quantum processor, consisting of one- & two-qubit operations applied to neighboring qubits on a two-dimensional grid. The evolution is reversed (U † ) after perturbing one qubit with gate B, followed by a probe operation M on the initially prepared qubit |𝜓M〉. This repeats k times before measuring another qubit M. Right : Conceptual representation of OTOCs of different order as interferometers.

The interference nature of the OTOC leads to two consequences crucial for attaining quantum advantage. First, the forward and backward evolutions partially reverse the effects of chaos and amplify the quantum signal measured at the end. We observed the signature of this amplification in OTOC signals. More specifically, OTOC signal magnitude, characterized by the width of the distribution of OTOC values over the ensemble of random circuits, scales as a negative power of time, whereas quantum signals measured without back evolutions decay exponentially. The slow power law decay of OTOCs suggests that measuring these quantities on a quantum computer is significantly more efficient than classical simulations, where costs increase exponentially over time.

Left : Time-dependent values of signals measured without time inversion ( gray ) and with time inversion ( magenta , blue , green ). The vertical axis shows standard deviation over random circuits for correlation function, C (1) , and the first/second order OTOCs, C (2) and C (4) . Right : A set of 2nd order OTOC values measured on a Willow device that are estimated to require 3.2 years to simulate each data point on the Frontier supercomputer . The horizontal axis labels instances of random circuits. A total of 65 qubits out of the 105 available qubits are used in this experiment.

The second consequence of many-body interference is classical complexity. A central task for quantum computing is to identify the computational cost gap between quantum and classical computers on specific computational tasks. We approached this in two ways: (1) through a combination of theoretical analysis and experiments, we revealed the fundamental obstacles to known classical algorithms in achieving the same outcome as our OTOC calculations on Willow, and (2) we tested the performance of nine relevant classical simulation algorithms by direct implementation and cost estimation.

In the first approach we identified that quantum interference is an obstacle for classical computation. A distinct characteristic of quantum mechanics is that predicting an outcome of an experiment requires analyzing probability amplitudes rather than probabilities as in classical mechanics. A well known example is the entanglement of light that manifests in quantum correlations between photons, elementary particles of light, that persist over long distances ( 2022 Physics Nobel Laureates ) or macroscopic quantum tunneling phenomena in superconducting circuits ( 2025 Physics Nobel Laureates ).

The interference in our second order OTOC data (i.e., an OTOC that runs through the backward and forward circuit loop twice) reveals a similar distinction between probabilities and probability amplitudes. Crucially, probabilities are non-negative numbers, whereas probability amplitudes can be of an arbitrary sign and are described by complex numbers. Taken together, these features mean they contain a much more complex collection of information. Instead of a pair of photons or a single superconducting junction, our experiment is described by probability amplitudes across an exponentially large space of 65 qubits. An exact description of such a quantum mechanical system requires storing and processing 2 65 complex numbers in memory, which is beyond the capacity of supercomputers. Moreover, quantum chaos in our circuits ensures that every amplitude is equally important, and therefore algorithms using a compressed description of the system require memory and processing time beyond the capacity of supercomputers.

Our further theoretical and experimental analysis revealed that carefully accounting for the signs of the probability amplitudes is necessary to predict our experimental data by a numerical calculation. This presents a significant barrier for a class of efficient classical algorithms, quantum Monte Carlo , that have been successful at describing quantum phenomena in a large quantum mechanical space (e.g., superfluidity of liquid Helium-4 ). These algorithms rely on description in terms of probabilities, yet our analysis demonstrates that such approaches would result in an uncontrollable error in the computation output.

Our direct implementation of algorithms relying on both compressed representation and efficient quantum Monte Carlo confirmed the impossibility of predicting second-order OTOC data. Our experiments on Willow took approximately 2 hours, a task estimated to require 13,000 times longer on a classical supercomputer. This conclusion was reached after an estimated 10 person years spent in classical red teaming of our quantum result, implementing a total of nine classical simulation algorithms as a result.

Having established the beyond-classical complexity of OTOCs, we began exploring how they could be applied to solving real-world problems of practical interest. To this end, we proposed Hamiltonian learning , a scheme where the quantum computer simulates OTOC signals from a physical system in nature, such as molecules, whose system parameters are not fully known. Then, we compare the quantum computer OTOC signals against real-world data about the physical system and observe when they best agree. By looking for this agreement, we aim to obtain a more precise estimate of system parameters than what is possible through other techniques.

To make this scheme practical, we have to find systems in nature that can perform our Quantum Echoes algorithm, and simulate these systems on our quantum hardware. As a step toward this goal, in "" Quantum computation of molecular geometry via many-body nuclear spin echoes ”, we show that we tested this concept using nuclear magnetic resonance (NMR) spectroscopy. In NMR, one uses the precession of nuclear spins in a large magnetic field to learn the structure of molecules and materials, like the proteins in your body or the battery components in your phone. Nuclear spins obey the laws of quantum mechanics, and under certain conditions (namely in solids or solid-like materials) they demonstrate the same quantum-chaotic behavior described above. This makes them a perfect candidate for the OTOC protocol.

In this pre-print, which will be submitted for peer review, we measured OTOCs on two organic molecules dissolved in liquid crystal at the Pines Magnetic Resonance Center at UC Berkeley. This experiment was then simulated on our Willow chip, resulting in improved models of the molecular structure. Due to the inherent complexity of simulating real-world systems and performance limits of our current chip, this initial demonstration is not yet beyond classical. However, our results demonstrate sensitivity to molecular details and we're confident that this path will lead to some of the first useful applications of quantum computation.

A schematic for refining knowledge of a physical quantum system through the quantum computer, known as Hamiltonian learning.

We have performed the first quantum computing experiment measuring a quantum observable that is both verifiable through another quantum computer or a natural quantum system, and beyond the simulation capacity of known classical algorithms. This experiment was made possible by our recent hardware advancement , and paves the way toward the first real-world application of quantum computers in probing the microscopic structures of physical systems such as molecules.

This work involved many members of the Quantum AI team, along with Google DeepMind and external collaborators at UC Berkeley , Dartmouth College , QSimulate , and NVIDIA .

November 13, 2025

October 31, 2025

June 23, 2025"
A picture's worth a thousand (private) words: Hierarchical generation of coherent synthetic photo albums,https://research.google/blog/a-pictures-worth-a-thousand-private-words-hierarchical-generation-of-coherent-synthetic-photo-albums/,"Weiwei Kong, Software Engineer, and Umar Syed, Research Scientist, Google Research

We introduce a method for generating differentially private synthetic photo albums that uses an intermediate text representation and produces the albums in a hierarchical fashion.

Differential privacy (DP) provides a powerful, mathematically rigorous assurance that sensitive individual information in a dataset remains protected, even when a dataset is used for analysis. Since DP’s inception nearly two decades ago , researchers have developed differentially private versions of myriad data analysis and machine learning methods, ranging from calculating simple statistics to fine-tuning complex AI models . However, the requirement for organizations to privatize every analytical technique can be complex, burdensome, and error-prone.

Generative AI models like Gemini offer a simpler, more efficient solution. Instead of separately modifying every analysis method, they create a single private synthetic version of the original dataset. This synthetic data is an amalgamation of common data patterns, containing no unique details from any individual user. By using a differentially private training algorithm, such as DP-SGD , to fine-tune the generative model on the original dataset, we ensure the synthetic dataset is both private and highly representative of the real data. Any standard, non-private analytical technique or modeling can then be performed on this safe (and highly representative) substitute dataset, simplifying workflows. DP fine-tuning is a versatile tool that is particularly valuable for generating high-volume, controlled datasets in situations where access to high-quality, representative data is unavailable.

Most published work on private synthetic data generation has focused on simple outputs like short text passages or individual images, but modern applications using multi-modal data (images, video, etc.) rely on modeling complex, real-world systems and behaviors, which simple, unstructured text data cannot adequately capture.

We introduce a new method for privately generating synthetic photo albums as a way to address this need for synthetic versions of rich, structured image-based datasets. This task presents unique challenges beyond generating individual images, specifically the need to maintain thematic coherence and character consistency across multiple photos within a sequential album. Our method is based on translating complex image data to text and back. Our results show that this process, with rigorous DP guarantees enabled, successfully preserves the high-level semantic information and thematic coherence in datasets necessary for effective analysis and modeling applications.

Our method differs from most other approaches to generating private synthetic image data in two major respects: (1) we use an intermediate text representation and (2) we generate the data hierarchically.

Here’s how it works:

Illustration of our method for generating synthetic photo albums.

Generating text as an intermediate step towards generating images has a number of advantages. First, text generation is the main strength of a large language model. Second, text summarization is inherently privacy enhancing, since describing an image by text is a lossy operation, so synthetic photos are unlikely to be exact copies of the originals, even when differential privacy is not enabled. Finally, generating images is far more costly than generating text, so by first generating text, we can filter albums based on their content before expending resources to produce the images in which we are most interested.

Our hierarchical generation strategy ensures that the photos in each album are internally consistent, since each photo caption in an album is generated with the same album summary as context. Also, generating the structured representations in two steps (first the album summaries, and then the photo captions) preserves significant computational resources relative to generating each representation in one shot. Since training cost scales quadratically with context length (due to self-attention ), training two models with shorter contexts is far less costly than training a single model with a long context.

It may seem that describing images with words is too lossy an operation to preserve any interesting characteristics of the original images, but a simple demonstration (without differential privacy, to allow for side-by-side comparison) illustrates the power of this approach. In the figure below, we prompted Gemini to describe an image using several hundred words, and then fed the response text back to Gemini, prompting it to generate an image matching the description. While this circular series of transformations does not satisfy differential privacy, it does illustrate the utility of text as an intermediary for synthetic image generation. As the saying goes, a picture is worth a thousand words — and it seems that it is not worth much more than that!

Left: Original image. Right: Synthetic image.

We asked Gemini to describe the original image in text, and then prompted Gemini to generate the synthetic image based on the text description.

Concurrent work by Wang et al . showed how one can leverage text-based intermediaries to generate differentially private single images using Private Evolution .

We tested our method on the YFCC100M dataset, a repository containing nearly 100 million images that have been released under the Creative Commons license. We formed “albums” from these images by grouping together photos taken by the same user within the same hour. We constructed training sets for the large language models described above, taking care that no user contributes more than one example to any training set (contribution bounding is necessary to ensure the validity of the differential privacy guarantee).

After applying our method to generate synthetic photo albums, we evaluated how well they resemble the original albums. First, we computed the MAUVE score , a neural embedding–based measure of semantic similarity, between the original and synthetic structured representations.

The figure below shows the MAUVE scores between real and synthetic album summaries, as well as real and synthetic photo captions, both before and after fine-tuning.

Left: MAUVE scores between real and synthetic album summaries. Right: MAUVE scores between real and synthetic photo captions. Higher MAUVE scores indicate greater similarity. Higher values of the privacy parameter ε imply weaker privacy constraints.

Next, we calculated the most common topics in the album summaries, shown in the table below, and found that they were very similar between real and synthetic data.

Left: Most common topics in real album summaries. Right: Most common topics in synthetic album summaries.

Finally, direct visual examination of the synthetic photos albums shows that each album is typically centered on a common theme, just like real photo albums, as demonstrated by the examples in the figure below.

Two synthetically-generated photo albums. Each album maintains a specific theme ( top: apple picking trip; bottom: couple visits a meadow).

The challenges of modern AI require data that is not only private, but also structurally and contextually rich, a need that simple, unstructured data can’t meet. By applying our hierarchical, text-as-intermediate method to the demanding task of generating coherent synthetic photo albums, we’ve successfully shown a pathway for extending the benefits of synthetic data beyond simple text or isolated images.

This methodology opens exciting new avenues for privacy-preserving AI innovation. It helps resolve the persistent tension between the need for large, high-quality data and the imperative to protect user privacy, paving the way for safer and more generalized AI development across critical industries.

This work is the result of a collaboration between many people at Google Research, including (in alphabetical order by last name): Kareem Amin, Eva Bertels, Alex Bie, Rudrajit Das, Alessandro Epasto, Adel Javanmard, Weiwei Kong, Dennis Kraft, Alex Kurakin, Natalia Ponomareva, Monica Ribero, Jane Shapiro, Yurii Sushko, Umar Syed, and Sergei Vassilvitskii.

November 18, 2025

November 12, 2025

November 7, 2025"
Teaching Gemini to spot exploding stars with just a few examples,https://research.google/blog/teaching-gemini-to-spot-exploding-stars-with-just-a-few-examples/,"Turan Bulmus, Head of GenAI Blackbelts and Solution Architects, Google Cloud, and Dr. Fiorenzo Stoppa, Royal Society Newton International Fellow, University of Oxford

In a publication in Nature Astronomy, we show how Google's Gemini model can be transformed into an expert astronomy assistant that classifies cosmic events with high accuracy and explains its reasoning in plain language, achieving 93% accuracy across three datasets by learning from just 15 annotated examples per survey.

Modern astronomy is a treasure hunt on a cosmic scale. Every night, telescopes around the globe scan the skies, searching for fleeting events like exploding stars ( supernovae ) that give us crucial insights into the workings of the universe. These surveys generate millions of alerts about potential discoveries, but there’s a catch: the vast majority are not real cosmic events but ""bogus"" signals from satellite trails, cosmic ray hits, or other instrumental artefacts.

For years, astronomers have used specialized machine learning models, like convolutional neural networks (CNNs), to sift through this data. While effective, these models often act as “black boxes,” providing a simple ""real"" or ""bogus"" label with no explanation. This forces scientists to either blindly trust the output or spend countless hours manually verifying candidates — a bottleneck that will soon become insurmountable with next-generation telescopes like the Vera C. Rubin Observatory , expected to generate 10 million alerts per night .

This challenge led us to ask a fundamental question: could a general-purpose, multimodal model, designed to understand text and images together, not only match the accuracy of these specialized models but also explain what it sees? In our paper, “ Textual interpretation of transient image classifications from large language models ”, published in Nature Astronomy , we demonstrate that the answer is a resounding yes. We show how Google’s Gemini model can be transformed into an expert astronomy assistant that can classify cosmic events with high accuracy and, crucially, explain its reasoning in plain language. We accomplished this by employing few-shot learning with Gemini, providing it with just 15 annotated examples per survey and concise instructions to accurately classify and explain cosmic events.

Instead of training a specialized model on millions of labeled images, we used a technique called few-shot learning on a general-purpose model. We gave Gemini just 15 annotated examples for each of three major astronomical surveys: Pan-STARRS , MeerLICHT , and ATLAS . Each example consisted of three small images: a new image of the transient alert, a reference image of the same patch of sky from a previous observation, and a difference image that highlights the change between the two. Alongside these images, we provided a concise set of instructions, a short expert-written note explaining the classification, and an interest score (e.g., “high interest” for a likely supernova, “low interest” for a variable star, or “no interest” for a bogus signal) along with an explanation of that score.

The model had to learn to classify transients from a diverse set of telescopes, each with different resolutions, pixel scales, and camera characteristics. As shown below, the same celestial object can appear quite different across these surveys, but Gemini was able to generalize from the few examples provided.

Gemini operates across surveys with diverse pixel scales and resolutions. The same transient is observed in three different surveys, with rows corresponding to Pan-STARRS ( top ), MeerLICHT ( middle ) and ATLAS ( bottom ). Each row includes, from left to right , a new image, a reference image and a difference image. The image stamps are all the same size in pixels (100 × 100) but differ in angular sky coverage due to survey-specific pixel scales: Pan-STARRS (0.25"" per pixel), MeerLICHT (0.56"" per pixel) and ATLAS (1.8"" per pixel).

Guided only by this minimal input, we asked Gemini to classify thousands of new alerts. The model achieved an average accuracy of 93% across the three datasets, which is on par with specialized CNNs that require massive, curated training datasets.

But unlike a traditional classifier, we prompted Gemini not just to output a label but also to generate for every candidate:

This turns the model from a black box into a transparent, interactive partner. Scientists can read the explanation to understand the model’s reasoning, building trust and allowing for more nuanced decision-making.

Gemini provides human-readable transient classifications and follow-up priorities. Each example consists of a new, reference and difference image for a candidate transient, followed by the Gemini classification, textual description and follow-up interest score. The examples shown in the figure are from the MeerLICHT dataset.

A critical step in building a reliable system is ensuring the quality of its output. We assembled a panel of 12 professional astronomers who reviewed 200 of Gemini’s classifications and explanations. Using a single, anchored 0–5 coherence rubric (0 = hallucination, 5 = perfectly coherent) tied to how well the text matched the new/reference/difference images, plus a simple Yes/Maybe/No check that the follow-up interest score agreed with the explanation, they rated the model’s descriptions as highly coherent and useful, confirming alignment with expert reasoning.

But perhaps our most important finding was that Gemini can effectively assess its own uncertainty. We prompted the model to assign a “coherence score” to its own explanations. We discovered that low-coherence scores were a powerful indicator of an incorrect classification. In other words, the model is good at telling us when it’s likely to be wrong. The details:

Left: Average coherence scores from 12 astronomers for 200 MeerLICHT transients, sorted by mean score ( blue ). Most examples received high values (4–5), indicating close alignment with user expectations. Inset: The consistency between the interest score assigned by the model & its own explanation, with nearly all cases marked as self-consistent (i.e., “Yes”). Right : Average user coherence scores, split by the correctness of the classification made by Gemini. Correctly classified examples (TPs & TNs, green ) tend to have higher coherence scores than incorrect ones (FPs & FNs, red ).

This capability is a game-changer for building reliable ""human-in-the-loop"" workflows. By automatically flagging its most uncertain cases, the system can focus astronomers' attention where it is most needed. This creates a powerful feedback loop. By reviewing the flagged cases and adding a few of these challenging examples back into the prompt, we can rapidly improve the model’s performance. Using this iterative process, we improved the model's accuracy on the MeerLICHT dataset from ~93.4% to ~96.7%, demonstrating how the system can learn and improve in partnership with human experts.

We believe this approach marks a step toward a new era of scientific discovery — one accelerated by models that can both reason over complex scientific datasets and explain their outputs in natural language., but by models that can reason, explain their output, and collaborate with researchers.

Because this method requires only a small set of examples and plain-language instructions, it can potentially be rapidly adapted for new scientific instruments, surveys, and research goals across many different fields. We envision this technology as a foundation for ""agentic assistants"" in science. Such systems could integrate multiple data sources, check their own confidence, request follow-up observations, and escalate only the most promising discoveries to human scientists.

This work shows a path toward systems that learn with us, explain their reasoning, and empower researchers in any field to focus on what matters most: asking the next great question.

This research was a collaborative effort. We extend our sincere thanks to our co-authors Steven Bloemen, Stephen J. Smartt, Paul J. Groot, Paul Vreeswijk, and Ken W. Smith.

November 18, 2025

November 7, 2025

November 4, 2025"
Solving virtual machine puzzles: How AI is optimizing cloud computing,https://research.google/blog/solving-virtual-machine-puzzles-how-ai-is-optimizing-cloud-computing/,"Pratik Worah, Research Scientist, Google Research, and Martin Maas, Research Scientist, Google DeepMind

We present LAVA, a new scheduling algorithm that continuously re-predicts and adapts to the actual lifetimes of virtual machines to optimize resource efficiency in large cloud data centers.

Imagine a puzzle game similar to Tetris with pieces rapidly falling onto a stack. Some fit perfectly. Others don’t. The goal is to pack the blocks as tightly and efficiently as possible. This game is a loose analogy to the challenge faced by cloud data centers several times every second as they try to allocate processing jobs (called virtual machines or VMs) as efficiently as possible. But in this case, the “pieces” (or VMs) appear and disappear, some with a lifespan of only minutes, and others, days. In spite of the initially unknown VM lifespans, we still want to fill as much of the physical servers as possible with these VMs for the sake of efficiency. If only we knew the approximate lifespan of a job, we could clearly allocate much better.

At the scale of large data centers, efficient resource use is especially critical for both economic and environmental reasons. Poor VM allocation can lead to ""resource stranding"", where a server's remaining resources are too small or unbalanced to host new VMs, effectively wasting capacity. Poor VM allocation also reduces the number of ""empty hosts"", which are essential for tasks like system updates and provisioning large, resource-intensive VMs.

This classic bin packing problem is made more complex by this incomplete information about VM behavior. AI can help with this problem by using learned models to predict VM lifetimes. However, this often relies on a single prediction at the VM's creation. The challenge with this approach is that a single misprediction can tie up an entire host for an extended period, degrading efficiency.

In “ LAVA: Lifetime-Aware VM Allocation with Learned Distributions and Adaptation to Mispredictions ”, we introduce a trio of algorithms — non-invasive lifetime aware scoring (NILAS), lifetime-aware VM allocation (LAVA), and lifetime-aware rescheduling (LARS) — which are designed to solve the bin packing problem of efficiently fitting VMs onto physical servers. This system uses a process we call “continuous reprediction”, which means it doesn’t rely on the initial, one-time guess of a VM’s lifespan made at its creation. Instead, the model constantly and automatically updates its prediction for a VM's expected remaining lifetime as the VM continues to run.

One of the key insights driving this research is the recognition that VM lifetimes are often unpredictable and follow a long-tailed distribution . For example, while the vast majority of VMs (88%) live for less than an hour, these short-lived VMs consume only a tiny fraction (2%) of the total resources. This means that the placement of a small number of long-lived VMs has a disproportionately large impact on overall resource efficiency.

Distribution of VM lifetimes of scheduled VMs ( left ) vs. their resource consumption ( right ). Interestingly, the shortest jobs (0–10 min, dark blue), which account for 53% by number, take a negligible fraction of resources. In contrast, the longest running jobs (>30 days, orange), which take considerable resources (18%), amount to a negligible fraction by number.

Instead of trying to predict a single, average lifetime, which can be misleading for VMs with bi-modal or highly varied lifespans, we designed an ML model that predicts a probability distribution for a VM's lifetime. This approach, inspired by survival analysis , allows the model to capture the inherent uncertainty of a VM's behavior.

More importantly, our system uses this distribution to continuously update its predictions. We ask, “Given a VM has been running for five days, what is its expected remaining lifetime?” As a VM continues to run, the system gains more information, and its lifetime prediction becomes more accurate. Our algorithms are specifically co-designed to leverage these repredictions, actively responding to mispredictions and improving the accuracy over time.

Lifetime distribution of VM lifetimes. When the VM is scheduled, the expected (average) lifetime is 0.2 days. After it has run for 1 day, the expected remaining lifetime is 4 days. After 7 days, the expected remaining lifetime is 10 days.

With this new, more robust prediction model, we developed three novel algorithms to improve VM allocation.

NILAS is a non-invasive algorithm that incorporates lifetime predictions into an existing scoring function. It ranks potential hosts for a new VM by considering the repredicted exit times of all existing VMs on that host. By prioritizing hosts where all VMs are expected to exit at a similar time, NILAS aims to create more empty machines. Our use of repredictions is less sensitive to prediction accuracy and allows NILAS to correct for errors. The NILAS algorithm has been deployed on our large-scale cluster manager , Borg, where it significantly improves VM allocation.

LAVA is a more fundamental departure from existing scheduling mechanisms. While NILAS aims to pack VMs with similar lifetimes, LAVA does the opposite: it puts shorter-lived VMs on hosts with one or more long-lived VMs. The goal is to fill in resource gaps with short-lived VMs that are at least an order of magnitude shorter than the host’s anticipated lifespan, so that they exit quickly without extending the host’s overall lifespan. LAVA also actively adapts to mispredictions by increasing a host’s anticipated lifespan if a VM outlives its expected deadline. Simulations show that this strategy minimizes fragmentation and ensures that hosts are eventually freed up.

LARS uses our lifetime predictions to minimize VM disruptions during defragmentation and maintenance. When a host needs to be defragmented, LARS sorts the VMs on that host by their predicted remaining lifetime and migrates the longest-lived VMs first. Shorter-lived VMs exit naturally before migration. Simulations with LARS indicate it has the potential to reduce the total number of migrations required by around 4.5%.

Developing powerful models and algorithms is only one part of the solution. Getting them to work reliably at large scale required us to rethink our approach to model deployment.

A common practice is to serve machine learning models on dedicated inference servers. However, this would have created a circular dependency , as these servers would themselves run on our cluster scheduling system. A failure in the model serving layer could then cause a cascading failure in the scheduler itself, which is unacceptable for a mission-critical system.

Our solution was to compile the model directly into the Borg scheduler binary . This approach eliminated the circular dependency and ensured that the model was tested and rolled out with the same rigorous process as any other code change to the scheduler. This also yielded an additional benefit: the model's median latency is just 9 microseconds (µs), which is 780 times faster than a comparable approach that uses separate model servers. This low latency is crucial for running repredictions frequently and for using the model in performance-sensitive tasks, like maintenance and defragmentation.

We also found that for our largest zones, the number of required predictions could become a bottleneck. We addressed this by introducing a host lifetime score cache, which only updates predictions when a VM is added or removed from a host, or when a host's expected lifetime expires. This caching mechanism ensures high performance and allows us to deploy our system fleet-wide.

Our NILAS algorithm has been running in Google's production data centers since early 2024. The results are clear and significant.

Simulations running LAVA suggest it will provide a further ~0.4 pp improvement over NILAS. Similarly, simulations with LARS indicate that it has the potential to reduce the number of VM live migrations needed for maintenance by 4.5%.

We believe this work is a foundational step towards a future where data center management is increasingly optimized by machine learning systems. The techniques we developed, particularly the use of repredictions and the co-design of models and systems, are generalizable to other tasks. We have demonstrated that it is possible to integrate advanced machine learning techniques into the lowest layers of a system’s infrastructure stack without sacrificing reliability or latency, while still delivering significant efficiency gains.

LAVA is a large collaborative project that spanned multiple teams across Google, including Google Cloud, Google DeepMind, Google Research, and SystemsResearch@Google. Key contributors include Jianheng Ling, Pratik Worah, Yawen Wang, Yunchuan Kong, Anshul Kapoor, Chunlei Wang, Clifford Stein, Diwakar Gupta, Jason Behmer, Logan A. Bush, Prakash Ramanan, Rajesh Kumar, Thomas Chestna, Yajing Liu, Ying Liu, Ye Zhao, Kathryn S. McKinley, Meeyoung Park, and Martin Maas.

November 21, 2025

November 19, 2025

November 13, 2025"
Using AI to identify genetic variants in tumors with DeepSomatic,https://research.google/blog/using-ai-to-identify-genetic-variants-in-tumors-with-deepsomatic/,"Kishwar Shafin, Technical Lead, and Andrew Carroll, Product Lead, Google Research

DeepSomatic is an AI-powered tool that identifies cancer-related mutations in a tumor’s genetic sequence to help pinpoint what’s driving the cancer.

Cancer is fundamentally a genetic disease in which the genetic controls on cell division go awry. Many types of cancer exist, and each poses unique challenges as it can have distinct genetic underpinnings. A powerful way to study cancer, and a critical step toward creating a treatment plan, is to identify the genetic mutations in tumor cells. Indeed, clinicians will now often sequence the genomes of biopsied tumor cells to inform treatment plans that specifically disrupt how that cancer grows.

With partners at the University of California, Santa Cruz Genomics Institute and other federal and academic researchers, our new paper, “ DeepSomatic: Accurate somatic small variant discovery for multiple sequencing technologies ” in Nature Biotechnology presents a tool that leverages machine learning to identify genetic variants in tumor cells more accurately than current methods. DeepSomatic is a flexible model that uses convolutional neural networks to identify tumor variants. It works on data from all major sequencing platforms, for different types of sample processing, and can extend its learning to cancer types not included in training.

We have made both the tool and the high-quality training dataset we created openly available to the research community. This work is part of broader Google efforts to develop AI methods to understand cancer and help scientists treat cancer, including analyzing mammogram images for breast cancer screening , CT scans for lung cancer screening , as well as a partnership aimed at using AI to advance research on gynecological cancers . Our hope is to speed cancer research and further the goal of precision medicine.

Genome sequencing is used in research and medical clinics to identify genetic variations between an individual and the human reference genome . Distinguishing between real variants and simple errors made during the sequencing process is challenging. That’s why almost a decade ago Google Research introduced DeepVariant to identify inherited variants, also called germline variants , that came from parents and are found in all of the body’s cells.

The genetics of cancer is more complex. Cancer is often driven by variants acquired after birth. Environmental exposure that damages DNA, such as UV light or chemical carcinogens, as well as random errors that occur during DNA replication, can cause cells in the body, known as somatic cells, to acquire new variants. Sometimes, these acquired variants change the normal behavior of cells, and can cause them to replicate when they shouldn’t. This process drives the initial development of cancer, as well as its later progression to more fast-growing and invasive stages.

Identifying variants specific to some of a person’s somatic cells is much harder than identifying inherited variants. Tumor cells can contain a diverse set of acquired variants at different frequencies, and the error rate of sequencing can be higher than the rate a somatic variant is present in a sample.

We developed DeepSomatic to address these challenges and accurately identify somatic variants. In most clinical and research settings, cancer is studied by sequencing the tumor cells acquired through biopsy, as well as normal cells that are unaffected by the tumor growth and contain more typical inherited genetic variations. DeepSomatic is trained to identify variations observed in tumor cells that are not inherited variants. These types of variations can provide critical insights about which variations are driving the tumor growth. DeepSomatic is also able to identify somatic variation in tumor-only mode where a non-tumor sequence is not available, for example in a blood cancer like leukemia where it is hard to get only normal cells from a blood draw. The ability to extend to different types of use-cases that follow common ways clinicians and researchers study cancer makes DeepSomatic applicable to many research and clinical settings.

Like our earlier tool, DeepVariant , the DeepSomatic model works by first turning genetic sequencing data into a set of images . The images represent the sequencing data, alignment along the chromosome, the quality of the output, and other variables. DeepSomatic then uses its convolutional neural network on data from tumor cells and non-cancerous cells to differentiate between the reference genome, the non-cancer germline variants in that individual, and the cancer-caused somatic variants in the tumor, while discarding variations caused by small errors acquired during the sequencing process. The result is a list of cancer-related variants, or mutations.

DeepSomatic detects cancer variants in genomic data. First, sequencing data from the tumor cells and non-cancerous cells are turned into an image. DeepSomatic passes these images through its convolutional neural network to differentiate between the reference genome, the non-cancer germline variants in that individual, and the cancer-caused somatic variants in the tumor, while discarding variations caused by small sequencing errors. The result is a list of cancer-caused variants, or mutations.

Training accurate models that can identify genetic variation for different cancer types requires comprehensive, high-quality data and truth sets. For this work we created a new training and evaluation dataset for detecting variants in tumor cells. With our partners at UC Santa Cruz and the National Cancer Institute , we sequenced tumor cells and accompanying normal cells from four breast cancer samples and two lung cancer samples from research cell lines.

Benchmark dataset used to train DeepSomatic. Each bar shows the number of mutations found in four breast cancer samples and two lung cancer samples, with color representing different types of mutations. Lung cancer displays a notable type of mutation caused by environmental toxins, including SBS4 shown in green. But even the same type of cancer shows big differences in its mutational signature. These individual differences can predict how well it will respond to a treatment.

To create an accurate training dataset, we did whole-genome sequencing of these six samples using three leading platforms: Illumina’s short-read sequencing , PacBio’s long-read sequencing , and Oxford Nanopore Technology’s long-read sequencing . Output from all three platforms was combined to remove platform-specific errors and create a single, accurate reference dataset we call the Cancer Standards Long-read Evaluation dataset (CASTLE) for genetic diversity in tumor and normal cells.

We trained DeepSomatic on three of the breast cancer genomes and the two lung cancer genomes in the CASTLE reference dataset. We then tested DeepSomatic’s performance in several ways, including on the single breast cancer genome that was not included in its training data, and on chromosome 1 from each sample, which we also excluded from the training.

Results show that DeepSomatic models developed for each of the three major sequencing platforms performed better than other methods, identifying more tumor variants with higher accuracy. The tools used for comparison on short-read sequencing data were SomaticSniper , MuTect2 and Strelka2 (with SomaticSniper specifically for single nucleotide variants, or SNVs). For long-read sequencing data we compared against ClairS , a deep learning model trained on synthetic data.

In our tests DeepSomatic identified 329,011 somatic variants across the six reference cell lines and a seventh preserved sample. DeepSomatic does particularly well at identifying cancer variations that involve insertions and deletions (“Indels”) of genetic code. For these types of variants, DeepSomatic substantially increased the F1-score , a balanced measure of how well the model finds true variants in a sample (recall) while not making false positives (precision). On Illumina sequencing data the next-best method scored 80% at identifying Indels, while DeepSomatic scored 90%. On Pacific Biosciences sequencing data, the next-best method scored less than 50% at identifying Indels, and DeepSomatic scored more than 80%.

DeepSomatic results ( purple ) for a breast cancer sample widely used in research, compared to other tools. Several software tools identify cancer variants in Illumina’s data, while only a single alternative ( pink ) exists for the long-read sequencing data generated by PacBio and Oxford Nanopore Technologies. The F1-score measures how many variants are discovered and with what accuracy. DeepSomatic performs slightly better for single-letter variations in genetic code, known as single nucleotide variations, and shows major improvements for variations that involve Indels.

The seventh sample was one of the previously used research cell lines of a breast cancer tumor that was preserved using formalin-fixed-paraffin-embedded (FFPE). This common preservation method introduces additional patterns of DNA damage that can complicate genetic analysis. This sample was also sequenced using whole exome sequencing (WES), a more affordable method that focuses only on the roughly 1% of the genome that codes for proteins. When DeepSomatic was trained on these types of sample data and then tested on chromosome 1, which was reserved from training, it again outperformed other tools, suggesting it can be used to identify variants in lower-quality or historic tumor samples, potentially rescuing samples that have been harder to sequence, and working on clinical data where only the exome was sequenced.

DeepSomatic has notably higher accuracy on samples prepared with more complicated pre-processing steps involving: fixed formalin paraffin embedded (FFPE), a method used to preserve tissue samples ( left ), and whole exome sequencing (WES), a method to sequence only the parts of the genome that code for proteins ( right ). The middle section shows a sample that was preserved with FFPE and also sequenced using whole exome sequencing.

To test DeepSomatic’s performance on other types of cancers, we analyzed a single sample of glioblastoma , an aggressive form of brain cancer that arises from a small number of variants. DeepSomatic was able to pinpoint those variants, showing that it can generalize its learning to apply it to a different cancer type.

We also worked with partners at Children’s Mercy in Kansas City to analyze eight previously sequenced samples of pediatric leukemia , a cancer of the white blood cells that is the most common childhood cancer. Leukemia exists in the bloodstream, so a “normal” non-cancer blood sample is not possible. Despite that challenge, DeepSomatic identified the previously known variants as well as 10 new ones, showing that it can work with a tumor-only sample.

Our hope is that research labs and clinicians can begin to use this tool. Detecting known cancer variants could help choose between existing treatments, such as chemotherapy, immunotherapy or other methods. Identifying new cancer variants could potentially lead to brand-new therapies. We hope people can take these tools and learn more about each cancer tumor, find what’s driving it, and ultimately deliver the most effective treatments to patients.

We thank all research participants whose participation in research programs and donation of cell lines made this work and other biomedical research possible. We thank our collaborators at UC Santa Cruz Genomics Institute, the National Cancer Institute, the Frederick National Laboratory for Cancer Research, Children’s Mercy Hospital, and NYU. We thank Hannah Hickey for writing contributions. We thank Avinatan Hassidim, Katherine Chou, Lizzie Dorfman, and Yossi Matias for research leadership support. We thank Resham Parikh and Isha Mishra for communications support.

November 13, 2025

November 5, 2025

November 4, 2025"
Coral NPU: A full-stack platform for Edge AI,https://research.google/blog/coral-npu-a-full-stack-platform-for-edge-ai/,"Billy Rutledge, Engineering Director, Google Research

Introducing Coral NPU, a full-stack, open-source platform designed to address the core performance, fragmentation, and privacy challenges that limit powerful, always-on AI with low-power edge devices and wearables.

Generative AI has fundamentally reshaped our expectations of technology. We've seen the power of large-scale cloud-based models to create, reason and assist in incredible ways. However, the next great technological leap isn't just about making cloud models bigger; it's about embedding their intelligence directly into our immediate, personal environment. For AI to be truly assistive — proactively helping us navigate our day, translating conversations in real-time, or understanding our physical context — it must run on the devices we wear and carry. This presents a core challenge: embedding ambient AI onto battery-constrained edge devices, freeing them from the cloud to enable truly private, all-day assistive experiences.

To move from the cloud to personal devices, we must solve three critical problems:

Today we introduce Coral NPU , a full-stack platform that builds on our original work from Coral to provide hardware designers and ML developers with the tools needed to build the next generation of private, efficient edge AI devices. Co-designed in partnership with Google Research and Google DeepMind, Coral NPU is an AI-first hardware architecture built to enable the next generation of ultra-low-power, always-on edge AI. It offers a unified developer experience, making it easier to deploy applications like ambient sensing. It's specifically designed to enable all-day AI on wearable devices while minimizing battery usage and being configurable for higher performance use cases. We’ve released our documentation and tools so that developers and designers can start building today.

Developers building for low-power edge devices face a fundamental trade-off, choosing between general purpose CPUs and specialized accelerators. General-purpose CPUs offer crucial flexibility and broad software support but lack the domain-specific architecture for demanding ML workloads, making them less performant and power-inefficient. Conversely, specialized accelerators provide high ML efficiency but are inflexible, difficult to program, and ill-suited for general tasks.

This hardware problem is magnified by a highly fragmented software ecosystem. With starkly different programming models for CPUs and ML blocks, developers are often forced to use proprietary compilers and complex command buffers. This creates a steep learning curve and makes it difficult to combine the unique strengths of different compute units. Consequently, the industry lacks a mature, low-power architecture that can easily and effectively support multiple ML development frameworks.

The Coral NPU architecture directly addresses this by reversing traditional chip design. It prioritizes the ML matrix engine over scalar compute, optimizing architecture for AI from silicon up and creating a platform purpose-built for more efficient, on-device inference.

As a complete, reference neural processing unit (NPU) architecture, Coral NPU provides the building blocks for the next generation of energy-efficient, ML-optimized systems on chip (SoCs). The architecture is based on a set of RISC-V ISA compliant architectural IP blocks and is designed for minimal power consumption, making it ideal for always-on ambient sensing. The base design delivers performance in the 512 giga operations per second (GOPS) range while consuming just a few milliwatts, thus enabling powerful on-device AI for edge devices, hearables, AR glasses, and smartwatches.

A unified view of the Coral NPU ecosystem, showcasing end-to-end stack for SoC designers and ML developers.

The open and extensible architecture based on RISC-V gives SoC designers flexibility to modify the base design, or use it as a pre-configured NPU. The Coral NPU architecture includes the following components:

Visualizing the architectural shift from traditional design to the Coral NPU.

The Coral NPU architecture is a simple, C-programmable target that can seamlessly integrate with modern compilers like IREE and TFLM . This enables easy support for ML frameworks like TensorFlow , JAX , and PyTorch .

Coral NPU incorporates a comprehensive software toolchain, including specialized solutions like the TFLM compiler for TensorFlow, alongside a general-purpose MLIR compiler, C compiler, custom kernels, and a simulator. This provides developers with flexible pathways. For example, a model from a framework like JAX is first imported into the MLIR format using the StableHLO dialect. This intermediate file is then fed into the IREE compiler, which applies a hardware-specific plug-in to recognize the Coral NPU's architecture. From there, the compiler performs progressive lowering — a critical optimization step where the code is systematically translated through a series of dialects, moving closer to the machine's native language. After optimization, the toolchain generates a final, compact binary file ready for efficient execution on the edge device. This suite of industry-standard developer tools helps simplify the programming of ML models and can allow for a consistent experience across various hardware targets.

The Coral NPU compiler toolchain, illustrating the complete flow from ML model creation through optimization and compilation to on-device deployment.

Coral NPU’s co-design process focuses on two key areas. First, the architecture efficiently accelerates the leading encoder-based architectures used in today's on-device vision and audio applications. Second, we are collaborating closely with the Gemma team to optimize Coral NPU for small transformer models, helping to ensure the accelerator architecture supports the next generation of generative AI at the edge.

This dual focus means Coral NPU is on track to be the first open, standards-based, low-power NPU designed to bring LLMs to wearables. For developers, this provides a single, validated path to deploy both current and future models with maximum performance at minimal power.

Coral NPU is designed to enable ultra-low-power, always-on edge AI applications, particularly focused on ambient sensing systems. Its primary goal is to enable all day AI-experiences on wearables, mobile phones and Internet of Things (IoT) devices minimizing battery usage.

Potential use cases include:

A core principle of Coral NPU is building user trust through hardware-enforced security. Our architecture is being designed to support emerging technologies like CHERI , which provides fine-grained memory-level safety and scalable software compartmentalization. With this approach, we hope to enable sensitive AI models and personal data to be isolated in a hardware-enforced sandbox, mitigating memory-based attacks.

Open hardware projects rely on strong partnerships to succeed. To that end, we’re collaborating with Synaptics , our first strategic silicon partner and a leader in embedded compute, wireless connectivity, and multimodal sensing for the IoT. Today, at their Tech Day, Synaptics announced their new Astra™ SL2610 line of AI-Native IoT Processors . This product line features their Torq™ NPU subsystem, the industry’s first production implementation of the Coral NPU architecture. The NPU’s design is transformer-capable and supports dynamic operators, enabling developers to build future-ready Edge AI systems for consumer and industrial IoT.

This partnership supports our commitment to a unified developer experience. The Synaptics Torq™ Edge AI platform is built on an open-source compiler and runtime based on IREE and MLIR. This collaboration is a significant step toward building a shared, open standard for intelligent, context-aware devices.

With Coral NPU, we are building a foundational layer for the future of personal AI. Our goal is to foster a vibrant ecosystem by providing a common, open-source, and secure platform for the industry to build upon. This empowers developers and silicon vendors to move beyond today's fragmented landscape and collaborate on a shared standard for edge computing, enabling faster innovation. Learn more about Coral NPU and start building today.

We would like to thank the core contributors and leadership team for this work, particularly Billy Rutledge, Ben Laurie, Derek Chow, Michael Hoang, Naveen Dodda, Murali Vijayaraghavan, Gregory Kielian, Matthew Wilson, Bill Luan, Divya Pandya, Preeti Singh, Akib Uddin, Stefan Hall, Alex Van Damme, David Gao, Lun Dong, Julian Mullings-Black, Roman Lewkow, Shaked Flur, Yenkai Wang, Reid Tatge, Tim Harvey, Tor Jeremiassen, Isha Mishra, Kai Yick, Cindy Liu, Bangfei Pan, Ian Field, Srikanth Muroor, Jay Yagnik, Avinatan Hassidim, and Yossi Matias.

November 21, 2025

November 18, 2025

November 12, 2025"
XR Blocks: Accelerating AI + XR innovation,https://research.google/blog/xr-blocks-accelerating-ai-xr-innovation/,"Ruofei Du, Interactive Perception & Graphics Lead, and Benjamin Hersh, Product Manager, Google XR

XR Blocks is an open-source framework to help you develop immersive experiences for the web, featuring XR realism, XR interaction, and AI + XR applications with live demos in xrblocks.github.io .

The combination of artificial intelligence (AI) and extended reality (XR) has the potential to unlock a new paradigm of immersive intelligent computing. However, a significant gap exists between the ecosystems of these two fields today. AI research and development is accelerated by mature frameworks like JAX , PyTorch , TensorFlow , and benchmarks like ImageNet and LMArena . Meanwhile, prototyping novel AI-driven XR interactions remains a high-friction process, often requiring practitioners to manually integrate disparate, low-level systems for perception, rendering, and interaction.

To bridge this gap, we introduce XR Blocks (presented at ACM UIST 2025 ), a cross-platform framework designed to accelerate human-centered AI + XR innovation. This is a significant step from our prior research in Visual Blocks for ML , which targets non-XR use cases and streamlines prototyping machine learning pipelines with visual programming. XR Blocks provides a modular architecture with plug-and-play components for core abstraction in AI + XR: user , world , interface , AI , and agents . Crucially, it is designed with the mission of accelerating rapid prototyping of perceptive AI + XR apps. Built upon accessible technologies ( WebXR , threejs , LiteRT , Gemini ), our toolkit lowers the barrier to entry for XR creators. We demonstrate its utility through a set of open-source templates , live demos , and source code on GitHub , with the goal of empowering the community to quickly move from concept to interactive prototype. You can find an overview of these capabilities in our directional paper and teaser video .

Link to Youtube Video

Introductory video of XR Blocks.

Our architectural and API design choices are guided by three principles:

Drawing inspiration from Visual Blocks for ML and InstructPipe , we designed the XR Blocks framework to provide a high-level, human-centered abstraction layer that separates the what of an interaction (denoted as Script , described more below) from the how of its low-level implementation.

XR Blocks accelerates the prototyping of real-time AI + XR applications across desktop simulators and Android XR devices. Examples: (a) XR Realism: Prototype depth-aware, physics-based interactions in simulation and deploy the same code to real-world XR devices. (b) XR Interactions: Seamlessly integrate custom gesture models to desktop simulator and on-device XR deployment. (c) AI + XR Integration: Build intelligent, context-aware assistants, like the Sensible Agent prototype that provides proactive suggestions with unobtrusive interactions.

We propose a new Reality Model composed of high-level abstractions to guide the implementation of the XR Blocks framework. Unlike the World Model designed for end-to-end unsupervised training, our Reality Model consists of replaceable modules for XR interaction. At the heart of our design is Script , the narrative and logical center of an application. Script operates on six first-class primitives (described and visualized below):

The conceptual Reality Model of the XR Blocks framework. At the center, Script contains the application’s logic and operates on a unified model of first-class primitives including the user, the physical world, AI agents, and the application context.

This Reality Model is realized by XR Blocks’s modular Core engine, which provides high-level APIs that enable developers to harness the following subsystems without needing to master the implementation:

The modular architecture of the XR Blocks’s core engine, which consists of essential subsystems to realize the framework’s high-level abstractions, spanning perception ( depth , input ), AI integration ( ai , agent ), and user experience ( ui , ux ).

By separating the abstract Reality Model from the concrete Core engine, XR Blocks enables a powerful new creative workflow. The goal is to allow creators to move from high level, human-centric ideas to interactive prototypes much more quickly. We envision a future where any declarative prompt, “When the user pinches at an object, an agent should generate a poem of it” , could be directly translated to high-level instructions in XR Blocks:

Hence, the creator’s prompt is no longer pseudocode but a direct summary of the implementation logic. We envision this framework to more seamlessly translate such user intent into a system-level execution flow, composing capabilities from the input , sound , ai , world , ui , and agent modules to generate an emergent, intelligent behavior with user interaction.

The Interaction Grammar of XR Blocks, which abstracts user input by distinguishing between two types of interaction. Explicit events are direct, low-level inputs (e.g., a touch or click), while implicit intents are higher-level interpretations (e.g., a gesture or voice command), allowing creators to build interaction against user intent.

We provide a suite of interactive applications to demonstrate the expressive power and flexibility of the XR Blocks framework. These examples showcase how our framework enables the rapid prototyping of sophisticated experiences that were previously too complex and costly to build, facilitating the creation of realistic, interactive, and intelligent mixed-reality worlds:

Applications of XR Blocks. (1) XR Realism: Depth-aware and physics-based ball pit ( demo ) and splash games ( demo ); geometry-aware shadows ( demo ), 3D Gaussian splatting with occlusion, and lighting estimation. (2) XR Interaction: Immersive emoji ( demo ) and rock paper scissors game ( demo ) empowered by custom ML models, dynamic swipe recognition, touch and grab with the physical world. (3) AI + XR: Integration with conversational AI ( demo ), XR objects ( demo ), glasses simulation in XR, and poem generation with a real-world camera.

The true power of the framework is realized when this Reality Model is deeply integrated with generative AI to create dynamic, personalized environments. We demonstrate this by building systems like Augmented Object Intelligence ( XR-Objects ), which imbues everyday physical objects with interactive digital affordances, such as dynamic virtual buttons. XR Blocks also serves as the foundation for Sensible Agent (published on ACM UIST 2025), a system for proactive and unobtrusive AR assistance. Our architecture provides the agent's core perception and interaction logic, providing an example of our primary goal: by providing robust, high-level tools, XR Blocks empowers Human-Computer Interaction researchers to bypass low-level implementation and focus directly on higher-order challenges like the cognitive principles of human-agent collaboration.

Demonstrations of XR Blocks SDK. (1) Using XR Blocks with conversational AI to automatically generate and test user prompts. (2) Running physical collision with depth sensing on Android XR. (3) Running LiteRT on a device with a custom gesture model to trigger XR animation.

Creating intelligent XR experiences is currently too fragmented, placing a major barrier between a creator's vision and its realization. We presented XR Blocks , an architecture and toolkit that dissolves this complexity by providing a high-level abstraction layer that separates what (the intent) from the how (the low-level implementation), dramatically accelerating the prototyping of context-aware applications. This is a foundational step toward a future where the boundaries between programming, design, and conversation disappear, enabling us to script realities as fluidly as we script stories. XR Blocks is far from perfect, and this work serves as an initial visionary document to invite more creators to join our journey, based on our belief that with the right set of tools, everyone can unleash their inner creativity with AI .

This work is a joint collaboration across multiple teams at Google. The following researchers and engineers contributed to this work: David Li and Ruofei Du (equal primary contributions), Nels Numan, Xun Qian, Yanhe Chen, and Zhongyi Zhou, (equal secondary contributions, sorted alphabetically), as well as Evgenii Alekseev, Geonsun Lee, Alex Cooper, Min Xia, Scott Chung, Jeremy Nelson, Xiuxiu Yuan, Jolica Dias, Tim Bettridge, Benjamin Hersh, Michelle Huynh, Konrad Piascik, Ricardo Cabello, and David Kim. We would like to thank Mahdi Tayarani, Max Dzitsiuk, Patrick Hackett, Seeyam Qiu, Brian Collins, Steve Toh, Eric Gonzalez, Nicolás Peña Moreno, Yi-Fei Li, Ziyi Liu, Jing Jin for their feedback and discussion on our early-stage proposal and WebXR experiments. We thank Max Spear, Adarsh Kowdle, and Guru Somadder for the directional contribution and thoughtful reviews.

November 18, 2025

November 7, 2025

October 31, 2025"
​​Speech-to-Retrieval (S2R): A new approach to voice search,https://research.google/blog/speech-to-retrieval-s2r-a-new-approach-to-voice-search/,"Ehsan Variani and Michael Riley, Research Scientists, Google Research

Voice Search is now powered by our new Speech-to-Retrieval engine, which gets answers straight from your spoken query without having to convert it to text first, resulting in a faster, more reliable search for everyone.

Voice-based web search has been around a long time and continues to be used by many people, with the underlying technology evolving rapidly to allow for expanded use cases. Google’s initial voice search solution used automatic speech recognition (ASR) to turn the voice input into a text query, and then searched for documents matching that text query. However, a challenge with this cascade modeling approach is that any slight errors in the speech recognition phase can significantly alter the meaning of the query, producing the wrong results.

For example, imagine someone does a voice-based web search for the famous painting, “ The Scream ”, by Edvard Munch. The search engine uses the typical approach of cascade modeling, first converting the voice query to text via ASR before passing the text to the search system. Ideally, the ASR transcribes the query perfectly. The search system then receives the correct text — “the Scream painting” — and provides relevant results, like the painting’s history, its meaning, and where it’s displayed. However, what if the ASR system mistakes the “m” of “scream” for an “n”? It misinterprets the query as “screen painting” and returns irrelevant results about screen painting techniques instead of details about Munch's masterpiece.

ASR accuracy is key for voice search. See what happens when a system correctly transcribes a query versus when it transcribes it incorrectly.

To prevent such errors in web search systems, what if the system could map directly from speech to the desired retrieval intent, bypassing the textual transcription entirely?

Enter Speech-to-Retrieval (S2R). At its core, S2R is a technology that directly interprets and retrieves information from a spoken query without the intermediate, and potentially flawed, step of having to create a perfect text transcript. It represents a fundamental architectural and philosophical shift in how machines process human speech. Where today's common voice search technologies are focused on the question, ""What words were said?"", S2R is designed to answer a more powerful question: ""What information is being sought?"" This post explores the substantial quality gap in current voice search experiences and demonstrates how the S2R model is poised to fill it. In addition, we are open-sourcing the Simple Voice Questions (SVQ) dataset, a collection of short audio questions recorded in 17 different languages and 26 locales, which we used to evaluate the performance potential of S2R. The SVQ dataset is part of the new Massive Sound Embedding Benchmark benchmark.

When a traditional ASR system converts audio into a single text string, it may lose contextual cues that could help disambiguate the meaning (i.e., information loss). If the system misinterprets the audio early on, that error is passed along to the search engine, which typically lacks the ability to correct it (i.e., error propagation). As a result, the final search result may not reflect the user's intent.

To investigate this relationship, we conducted an experiment designed to simulate an ideal ASR performance. We began by collecting a representative set of test queries reflecting typical voice search traffic. Crucially, these queries were then manually transcribed by human annotators, effectively creating a ""perfect ASR"" scenario where the transcription is the absolute truth.

We then established two distinct search systems for comparison (see chart below):

The retrieved documents from both systems (cascade ASR and cascade groundtruth) were then presented to human evaluators, or ""raters"", alongside the original true query. The evaluators were tasked with comparing the search results from both systems, providing a subjective assessment of their respective quality.

We use word error rate (WER) to measure the ASR quality and to measure the search performance, we use mean reciprocal rank (MRR) — a statistical metric for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness and calculated as the average of the reciprocals of the rank of the first correct answer across all queries. The difference in MRR and WER between the real-world system and the groundtruth system reveals the potential performance gains across some of the most commonly used voice search languages in the SVQ dataset (shown below).

The word error rate (WER) of the ASR model across voice search languages in the SVQ dataset.

MRR of current real-world (“Cascade ASR”; blue) models vs ground truth (i.e., perfect; “Cascade Groundtruth”; green).

The results of this comparison lead to two critical observations. First, and as can be seen by comparing both charts above, we found that a lower WER does not reliably lead to a higher MRR across different languages. The relationship is complex, suggesting that the impact of transcription errors on downstream tasks is not fully captured by the WER metric. The specific nature of an error — not just its existence — appears to be a critical, language-dependent factor. Second, and more importantly, there’s a significant MRR difference between the two systems across all tested languages. This reveals a substantial performance gap between current cascade designs and what is theoretically possible with perfect speech recognition. This gap represents the clear potential for S2R models to fundamentally improve voice search quality.

At the heart of our S2R model is a dual-encoder architecture. This design features two specialized neural networks that learn from vast amounts of data to understand the relationship between speech and information. An audio encoder processes the raw audio of a query, converting it into a rich vector representation that captures its semantic meaning. In parallel, a document encoder learns a similar vector representation for documents.

Difference in similarity loss between audio and document embedding.

The key to this model is how it is trained. Using a large dataset of paired audio queries and relevant documents, the system learns to adjust the parameters of both encoders simultaneously.

The training objective ensures that the vector for an audio query is geometrically close to the vectors of its corresponding documents in the representation space. This architecture allows the model to learn something closer to the essential intent required for retrieval directly from the audio, bypassing the fragile intermediate step of transcribing every word, which is the principal weakness of the cascade design.

When a user speaks a query, the audio is streamed to the pre-trained audio encoder, which generates a query vector. This vector is then used to efficiently identify a highly relevant set of candidate results from our index through a complex search ranking process.

How S2R processes a spoken query.

The animation above illustrates how S2R understands and answers a spoken query. It starts with a user's voice request for “The Scream painting”. An audio encoder translates the sound into a rich audio embedding , a vector that represents the deep meaning of the query. This embedding is then used to scan a massive index of documents, surfacing initial candidates with high similarity scores, like the Wikipedia page for “The Scream” (0.8) and the Munch Museum website (0.7).

But finding relevant documents is just the beginning. The crucial final step is orchestrated by the search ranking system. This powerful intelligence goes far beyond the initial scores, weaving them together with hundreds of other signals to deeply understand relevance and quality. It weighs all this information in a fraction of a second to choreograph the final ranking, ensuring the most helpful and trustworthy information is presented to the user.

We evaluated the S2R system described above on the SVQ dataset:

MRR of current real-world (“Cascade ASR”; blue) models vs ground truth (i.e., perfect; “Cascade Groundtruth”; green) and the S2R model's performance (""S2R"" orange bar).

The S2R model's performance (orange bar) shows two key results:

While promising, the remaining gap indicates that further research is required.

The move to S2R-powered voice search isn’t a theoretical exercise; it’s a live reality. In a close collaboration between Google Research and Search, these advanced models are now serving users in multiple languages, delivering a significant leap in accuracy beyond conventional cascade systems.

To help propel the entire field forward, we are also open-sourcing the SVQ dataset as part of the Massive Sound Embedding Benchmark (MSEB). We believe shared resources and transparent evaluation accelerates progress. In that spirit, we invite the global research community to use this data, test new approaches on public benchmarks, and join the effort to build the next generation of truly intelligent voice interfaces.

The authors sincerely thank all who contributed to this project, whose critical input made it possible. We are especially grateful to our colleagues Hawi Abraham, Cyril Allauzen, Tom Bagby, Karthik Kumar Bandi, Stefan Buettcher, Dave Dopson, Lucy Hadden, Georg Heigold, Sanjit Jhala, Shankar Kumar, Ji Ma, Eyal Mizrachi, Pandu Nayak, Pew Putthividhya, David Rybach, Jungshik Shin, Venkat Subramanian, Sundeep Tirumalareddy and Trystan Upstill. We also wish to acknowledge those who helped prepare this post: Mark Simborg for his extensive editing, Kimberly Schwede for the wonderful illustrations, and Mickey Wurts for his valuable assistance.

November 21, 2025

November 18, 2025

November 7, 2025"
A collaborative approach to image generation,https://research.google/blog/a-collaborative-approach-to-image-generation/,"Guy Tennenholtz, Senior Research Scientist, and Craig Boutilier, Principal Scientist, Google Research

We introduce PASTA, a reinforcement learning agent that refines text-to-image output over multiple turns of interaction with a user by learning their unique preferences. This process is made possible by a novel user simulation technique.

You have a perfect image in your mind. You enter a prompt, hit generate, and the result is close to what you were thinking, but not quite right. You try refining the prompt, adding more detail, but you can't seem to bridge the gap between your idea and the final image.

This is a common experience. While text-to-image (T2I) models are incredibly powerful, they often struggle to capture the nuance and specificity of an individual's unique creative intent given just a single prompt. What if we could turn image generation into a collaborative conversation?

In this post, we describe our research “ Preference Adaptive and Sequential Text-to-image Agent ” (PASTA), a reinforcement learning (RL) agent that collaborates with users to progressively refine T2I results. This approach eliminates the need for users to rely on trial-and-error prompt refinement to reach a desirable image. Through human evaluations, we created a novel dataset of sequential preferences, which we then used to compare PASTA with a baseline state-of-the-art model. The results demonstrated that PASTA, trained with our mix of real and simulated data, consistently produced images that users rated as more satisfying. We’ve also released our foundational dataset with a collection of over 7,000 human rater interactions with PASTA.

To effectively train an AI agent to adapt to a user's individual preferences, a large, diverse set of interaction data is needed. However, gathering this data from real users is challenging due to several factors, including user privacy. To address this, we trained PASTA using a two-stage strategy that combines real human feedback with large-scale user simulation.

First, we collected a high-quality foundational dataset with over 7,000 raters' sequential interactions. These interactions included prompt expansions generated by a Gemini Flash large multimodal model and corresponding images generated by a Stable Diffusion XL (SDXL) T2I model. This initial seed of authentic preference data was then used to train a user simulator, designed to generate additional data that replicate real human choices and preferences.

At the heart of our method is a user model, comprising two key components: 1) a utility model that predicts the degree to which a user will like any set of images, and 2) a choice model that predicts which set of images they will select when presented with several sets. We constructed the user model using pre-trained CLIP encoders and added user-specific components. We trained the model using an expectation-maximization algorithm that allows us to simultaneously learn the specifics of user preferences while also discovering latent “user types,” that is, clusters of users with similar tastes (e.g., tendencies to prefer images with animals, scenic views, or abstract art).

The trained user simulator can provide feedback and express preferences on generated images, and make selections from sets of proposed images. This allows us to generate over 30,000 simulated interaction trajectories.. Our approach does more than just create more data; it gives us a controlled environment in which to explore a vast range of user behaviors so we can train the PASTA agent to effectively collaborate with users.

Our user simulator learns to identify distinct user types from preference data. Each row shows the top-rated images for an emergent user profile, revealing clear preferences for categories like ""Animals"" or ""Food.""

With this robust, data-driven foundation, the PASTA agent is trained to effectively engage with arbitrary users to generate images that match their preferences. The agent itself is a value-based reinforcement learning model that learns to select the best ""slate"" of prompt expansions (i.e., elaborations of the current prompt used to generate subsequent images) to show the user at each turn. Its goal is to maximize the user's cumulative satisfaction over the entire interaction.

Once PASTA is trained and deployed, a user initiates the engagement with an initial prompt. PASTA first uses a candidate generator (a large multimodal model) to create a diverse set of potential prompt expansions. Then, a candidate selector (our trained RL agent) selects the optimal slate of four such expansions, which are used to generate corresponding images to present to the user. The user selects the image that is closest to their vision, which provides feedback that guides PASTA's next set of suggestions. This collaborative back-and-forth allows the model to learn the user's preferences on the fly, steering the creative process toward their ideal goal with each step.

Starting with a simple prompt for ""A white cat"", PASTA engages the user in a visually grounded dialogue. The user's selections (highlighted in blue) help the agent quickly learn their preference for a more fantastical and colorful style.

To evaluate our approach, we trained PASTA as a value-based reinforcement learning agent using implicit Q-learning (IQL). We specifically wanted to see how the use of different training data impacted performance. We created three versions of the agent: 1) trained only on the real volunteer-rater data, 2) trained only on the simulated data, and 3) trained on a combination of real and simulated datasets.

We then ran a series of human evaluations comparing these agents to a baseline model (i.e., base Gemini Flash and SDXL models with no additional training) across four metrics: accuracy over the Pick-a-Pic dataset, Spearman’s rank correlation , choice model accuracy, and cross turn accuracy. Pick-a-Pic accuracy and Spearman's rank correlation assess the model's ability to predict user preferences and rankings on existing, large-scale, single-turn datasets. Choice model accuracy and cross-turn accuracy measure the model's ability to predict which image a user will choose at a given turn and whether the selected images are an improvement over the previous turn, respectively.

The results demonstrated that training PASTA on synthetic data alone didn't beat the baseline and while the agent trained on real human data showed significant improvement, it also didn’t outperform the baseline. However, the agent trained on the combination of both real and simulated data offered the best performance, confirming that our user simulation successfully captures key dynamics of human interaction while providing the scale needed for robust RL training.

The graphs above present the accuracy performance of a trained user model (y axis) as a function of the number of user types considered (x axis). The top row displays the model’s accuracy on the Pick-a-Pic test set ( left ) and its Spearman’s rank correlation on the HPS test set ( right ). The bottom row shows the model’s choice accuracy ( left ) and cross-turn preference accuracy ( right ), both evaluated on our human-rated test set .

When we asked raters to directly compare the final images from our best-performing agent against the baseline, 85% preferred PASTA's generated images. The difference is especially striking with abstract prompts. Starting with a simple idea like ""an image of love"", PASTA adapted to different user types to create a wide variety of results, from tender portraits to abstract, geometric art.

With the same starting prompt, ""An image of happiness"", PASTA produces dramatically different results for two distinct user types (User Type A and User Type B), showcasing its ability to adapt to an individual's unique creative style. For example, the result for Type A corresponds to a prompt like “Abstract happy faces, Art Deco inspired geometric shapes, muted jewel-toned background.”

PASTA shows that the future of generative AI can be more interactive, preference adaptive, and collaborative. The methods we developed, particularly the use of robust user simulators, can be applied to many other generative tasks to create AI that better aligns and adapts to human users.

To help spur further research, we have open-sourced our sequential rater dataset and our simulated user data. We can't wait to see what the community builds with it.

The author list is: Ofir Nabati, Guy Tennenholtz, ChihWei Hsu, Moonkyung Ryu, Deepak Ramachandran, Yinlam Chow, Xiang Li, and Craig Boutilier. Special thanks to Mark Simborg for his help crafting this blog post and Kimberly Schwede for creating the figures in this post.

November 18, 2025

November 7, 2025

November 6, 2025"
Introducing interactive on-device segmentation in Snapseed,https://research.google/blog/introducing-interactive-on-device-segmentation-in-snapseed/,"Ben Hahn and Florian Kübler, Software Engineers, Google Cloud

A novel mobile technology that facilitates real-time image segmentation, thereby improving the user experience for photo editing within Snapseed.

The key to elevating a good photo often lies in selective image adjustments: brightening a subject in the foreground, enhancing the sky, or making the color of a jacket pop. Yet, isolating specific elements with existing tools that offer subject, background, sky, or color-based selections has remained a frustrating and complex endeavor. This challenge has been particularly acute on mobile devices, where imprecise touch input and limited processing have made detailed selections and edits very difficult.

Now, we have made object-based image adjustments quick and easy. The new Object Brush in Snapseed on iOS, accessible in the ""Adjust"" tool, now lets you edit objects intuitively. It allows you to simply draw a stroke on the object you want to edit and then adjust how you want it to look, separate from the rest of the image. Give it a try as we roll this new capability out in the coming week!

Selective editing using Snapseed's Object Brush.

At its core, Object Brush is powered by our Interactive Segmenter, a powerful AI model that runs entirely on device. With a simple gesture — just a tap or tracing a quick line — you can choose an object or person in the frame. The model will then immediately detect and select the complete object or person, in less than 20ms. The model generates a mask for the object, which accurately matches its boundaries, whether it's a person, a pet, or the clouds in the sky. This real-time feedback lets you refine your selection on the fly, easily adding or subtracting areas until it's just right. This entire process is powered by MediaPipe and LiteRT’s GPU acceleration for a fast and seamless experience.

This powerful fusion of a simple, intuitive user interface with an effective and efficient machine learning model makes advanced photo editing more accessible, enjoyable, and more precise than ever before, all running seamlessly on your own device.

Use foreground prompts (green) to select parts of an image and background prompts (red) to refine the selection.

The Interactive Segmenter model is designed to be a universally capable segmentation model, not limited to any specific class of objects or scenes. To avoid having to annotate large amounts of data to cover all areas, we chose to follow the Big Transfer approach and use a general pre-trained image encoder for pseudo-annotation to complement small amounts of manually annotated images.

We started with a pre-trained and highly-generalizable model, fine-tuned for interactive segmentation. We took samples for 350+ different object categories and asked annotators to precisely annotate object masks with pixel-perfect quality. Through this process, we obtained ~30,000 high-quality image masks for these categories. While insufficient for direct training of a small mobile model, large pre-trained models can successfully be fine-tuned on this data to predict high accuracy masks. Using this dataset we trained an interactive segmentation model, which we call “Interactive Segmenter: Teacher”.

Interactive Segmenter: Teacher produces high-quality segmentation masks; however, its speed and size hinder its use in on-device scenarios. To overcome this challenge, we developed “Interactive Segmenter: Edge”, a specialized model tailored for on-device use cases by leveraging the knowledge distilled from the original Interactive Segmenter: Teacher model.

Since the on-device model is significantly smaller, it has limited generalization capabilities, and the 30,000 annotated images we used for fine-tuning aren't sufficient to train a new model. At the same time the small model size implies we won’t see significant gains from pre-training on different domains or tasks.

For knowledge transfer from Interactive Segmenter: Teacher to Interactive Segmenter: Edge, we need millions of images and realistic prompts for a diverse range of object categories. So, we leveraged a large, weakly annotated dataset, which contains over 2 million images with masks across hundreds of different categories.

Interactive Segmenter: Edge yields a similar quality as Interactive Segmenter: Teacher for a given, fixed input prompt, as measured by the intersection over union (IOU) metric.

The segmentation masks in the distillation dataset are not pixel-perfect, because they were generated through automated or semi-automated procedures , and are not ideal for training high-quality segmenters. Nevertheless, they are suitable for creating realistic prompts for interactive segmentation. In this process, the ground truth mask is produced on-the-fly by Interactive Segmenter: Teacher, which acts as a teacher model in a process known as knowledge distillation . Importantly, both the teacher as well as the student model use the same prompts during training, ensuring consistency across models.

We attempt to simulate a user selecting objects in an image. We draw random scribbles within the (eroded) ground truth mask to get foreground prompts (i.e., what the user wants to select, shown in red in the image below) and random scribbles outside the ground truth mask to get background prompts (i.e., what the user explicitly does not want to select, shown in blue). We simulate tapping by drawing random points as well as random scribbles. Furthermore, to support lasso selection we also expose the model during training to box prompts around an object.

By utilizing a teacher model we can train on data with low-quality ground truth annotations, reducing labeling costs without sacrificing model quality.

A central challenge was reconciling the conflicting demands of segmentation quality versus real-time, interactive latency. To reach the right balance, we decouple image and prompt understanding into distinct sub-models. First, a powerful, heavyweight image encoder is run once per image to extract a rich set of semantic features. This image encoder can be run as soon as the user’s intent to use interactive segmentation becomes apparent, thus effectively hiding the latency from the user. Second, a lightweight interactive encoder-decoder operates on these pre-computed features. This network takes the user's touch prompts and generates the final segmentation mask, executing well under our 20ms budget. This separation into two models allows Interactive Segmenter to harness the image understanding of a large model while delivering the instantaneous responsiveness of a small one.

Interactive Segmenter neural network architecture.

Model inference latency when running Interactive Segmenter: Edge on-device.

The final student models (encoder + super decoder) are quantized to 8 bits and both run on LiteRT's GPU acceleration with decoder inference latencies of 7.4ms on an iPhone 16 Pro, enabling seamless and intuitive image editing.

To preserve the best image editing quality on high-resolution images, we need high-resolution segmentation masks. To achieve this, we train our segmentation model to predict a mask in 768x768 resolution and further upsample it to image resolution (capped at 4k to have it fit within a single GPU buffer). We use an efficient GPU implementation of the edge-preserving joint-bilateral upsampling method . To improve latency, we only apply upsampling once a user completes a gesture by lifting their finger.

Original Interactive Segmenter mask ( left ) and upsampled mask ( right ).

With the new Interactive Segmenter in Snapseed image editing has become easier and more powerful than ever. Simple taps and strokes are translated into accurate selections, allowing users to translate their editing ideas into reality. Download Snapseed for iOS here and let your photos shine. Object Brush will be rolled out to more tools in Snapseed in the coming months. The underlying model powers a wide range of image editing and manipulation tasks and serves as a foundational technology for intuitive selective editing. It has also been shipped in the new Chromebook Plus 14 to power AI image editing in the Gallery app. Next, we plan to integrate it across more image and creative editing products at Google.

Special thanks to all members who worked on the tech with us: Valentin Bazarevsky, Daniel Fenner, Lutz Justen, Ronald Wotzlaw, Tai-Yu Daniel Pan, Jason Chang, Matthew Harries, Giles Ochs, Jonathan Horsman, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Karthik Raveendran, Matsvei Zhdanovich, Mogan Shieh, Chris Parsons, Jianing Wei, and Matthias Grundmann.

November 21, 2025

November 18, 2025

October 29, 2025"
AI as a research partner: Advancing theoretical computer science with AlphaEvolve,https://research.google/blog/ai-as-a-research-partner-advancing-theoretical-computer-science-with-alphaevolve/,"Ansh Nagda, Student Researcher, and Abhradeep Thakurta, Staff Research Scientist, Google DeepMind, and Prabhakar Raghavan, Chief Technologist, Google

We invoke AlphaEvolve, an LLM-based coding agent, to find and verify combinatorial structures that improve results on the hardness of approximately solving certain optimization problems.

Recently, large language models (LLMs) have demonstrated surprising capabilities in competitive mathematics and competitive programming , demonstrating world-leading performance across both of these fields. However, their successes in mathematical discovery — proving novel theorems or uncovering new combinatorial structures — have been relatively few (with some notable exceptions [ 1 , 2 , 3 ]). Since mathematics and theoretical computer science demand absolute correctness [94fb54] , any AI-based method that makes mathematical discovery must either have a proof of correctness that can be confirmed computationally (without any human involvement), or have a domain-expert human in the loop to certify correctness.

In our recent paper, “ Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory ”, we demonstrate how an LLM-powered coding agent can help discover new mathematical structures that push the boundaries of our understanding of complexity theory (a sub-field of theoretical computer science). Our work utilizes AlphaEvolve , a system developed at Google DeepMind that uses LLMs to iteratively evolve code. By employing a feedback loop, AlphaEvolve began with populations of code snippets, evaluated the structures produced by the code snippets, and used an LLM to morph the most successful snippets toward better solutions. This approach led to new results in two distinct areas of complexity theory: 1) improving the state-of-the-art for the limit on our ability to approximate the outcome (i.e., the ""inapproximability"") of the maximum cut problem for 4 slices (which we define as the MAX-4-CUT problem ), and 2) tightening the bounds on the average-case hardness of certifying properties of random graphs .

AI-assisted mathematical research can operate in the following modes:

Our work falls in the second category, where we obtain better proof elements using AlphaEvolve that can be automatically verified by a computer program.

A fundamental challenge in using AI for theoretical computer science research lies in the universal nature of the problems studied. An AI system might find a solution to a specific instance of a problem — say, the optimal route for a traveling salesman visiting 50 specific cities. However, computer scientists often seek theorems that hold true universally for all problem instances and sizes (denoted as ∀n).

How can we use AlphaEvolve to prove a universal statement? The answer lies in a technique known as ""lifting"" (see image below). If a proof is viewed as a long string, then one can take a chunk of the proof (corresponding to a certain finite structure), and evolve it to support a stronger universal statement, while keeping the interface to the rest of the proof intact. The advantage of this approach is that to certify overall correctness, one needs to only certify the correctness of the finite structure that has been evolved.

Lifting: Morphing finite structures using AI, while keeping the interface to the broader proof intact.

In complexity theory, researchers often use established proof frameworks that rely on the existence of specific, highly optimized finite structures. If a better structure can be found, the entire proof framework ""lifts"" this improvement to a better universal result.

A key example of this is a "" gadget reduction ."" To prove that a target problem is computationally hard (intractable), researchers try to map a known intractable source problem to it, hence demonstrating that the target problem is at least as hard as the source problem. A gadget is a recipe for locally transforming a small piece of the source problem into a piece of the target problem. These gadgets are finite structures, and finding the optimal gadget is a painstaking process often done by hand.

By tasking AlphaEvolve with finding better gadgets, we were able to discover structures far more complex than those previously known. These finite discoveries, when plugged into the existing mathematical frameworks, immediately yield new universal theorems in complexity theory.

We applied this methodology to the MAX-k-CUT problem. Given a graph (a network of nodes and edges), the goal is to partition the nodes into k distinct sets such that the number of edges crossing between different sets is maximized. This is a classic intractable ( NP-hard ) problem, meaning we do not expect to find efficient algorithms that solve it exactly. Therefore, we focused on approximation algorithms — those that efficiently find solutions guaranteed to be close to the optimum.

The crucial question is: what is the limit of approximation?

For MAX-4-CUT (partitioning into four sets), the previous best-known result proved that it is NP-hard to approximate the solution within a factor of 0.9883 . AlphaEvolve was deployed to search for a new gadget reduction to MAX-4-CUT.

The system discovered an intricate gadget involving 19 variables (nodes) with a complex weighting scheme (some connections having up to 1429 times the weight of others). This discovery established a new inapproximability bound of 0.987.

This improvement may seem incremental, but in the mature field of hardness of approximation, such advances often require significant new techniques or combinatorial insights.

Gadget found by AlphaEvolve for the reduction to MAX-4-CUT.

We also explored the hardness of problems on average , rather than in the worst case. Specifically, we studied the difficulty of certifying bounds on the MAX-2-CUT (as well as maximum independent set ) of sparse random graphs [3d54ef] . Recent work connected this problem to the existence of specific Ramanujan graphs — deterministic graphs that “look” like sparse random graphs. They conjectured that the existence of Ramanujan graphs with unnaturally large cuts implies it is computationally hard to certify the MAX-2-CUT of a random graph.

Prior work used computer assistance to find such graphs on up to 10 nodes. Improving their results requires finding more extremal Ramanujan graphs on many more nodes, which are exceedingly difficult to find and verify. AlphaEvolve successfully navigated this vast search space, discovering Ramanujan graphs with even larger cuts on as many as 163 nodes.

A 4-regular Ramanujan graph with large 2-cut found by AlphaEvolve.

These discoveries significantly improved the lower bounds for average-case hardness. Furthermore, combined with new algorithmic progress (non-AI based), we were able to nearly settle the computational hardness of these questions, matching the upper and lower bounds to within the third decimal place.

A critical distinction of this work is that the results come with proofs of correctness.

When an LLM is prompted to generate a mathematical proof directly, it often produces a proof sketch or an argument that requires substantial human intervention to verify and complete. Hallucinations or subtle errors can render the output useless. As mentioned earlier, the standard for correctness in math is absolute.

In contrast, the approach taken here uses AI to discover a structure within the proof, not the proof itself. The validity of the final theorem relies on two components: the correctness of the lifting framework, and the verification of the discovered structure. While the frameworks are sound, verifying the structures discovered by AlphaEvolve is computationally intensive.

Remarkably, AlphaEvolve achieved a 10,000x speedup in the verification process by implementing sophisticated branch-and-bound strategies and system-level optimizations. This massive speedup was the key enabler for the research, allowing the system to explore much larger and more complex gadgets.

Crucially, the final gadgets discovered were still verified using the original, brute-force algorithm, ensuring the absolute correctness of the theorems.

While these initial research findings are far from conclusive, they suggest that AI is poised to become a helpful collaborator in mathematical discovery. We have observed the models in AlphaEvolve generate intricate mathematical objects that at times exhibit nascent reasoning capabilities. However, as we transition into an era where proofs may increasingly be attributed to AI, the crucial task of verification is set to become a significant bottleneck.

We would like to thank Adam Zsolt Wagner, Swarat Chaudhuri, Pasin Manurangsi and Sushant Sachdeva for helping us during various stages of the project.

In mathematics, a statement is definitively true or false, with no intermediate state possible. This stands in contrast to several other applications of AI, such as essay-writing or artistic creation, which have subjective standards of correctness and do not need to be correct in an absolute sense.

A sparse random graph is generated by randomly adding edges between a pair of nodes, where each node is guaranteed to have exactly d neighbors for some small d .

November 21, 2025

November 19, 2025

November 18, 2025"
The anatomy of a personal health agent,https://research.google/blog/the-anatomy-of-a-personal-health-agent/,"Xuhai “Orson” Xu, Visiting Faculty Researcher, and Ali Heydari, Research Scientist, Google Research

Learn about our research prototype, an LLM-powered personal health agent that analyzes data from everyday wellness devices paired with health data, such as blood biomarkers, to offer evidence-based health insights and to provide a personalized coaching experience.

The rapid advancement of large language models (LLMs), combined with data from wearable devices , presents a transformative opportunity to empower people on their personal health journeys. However, health needs vary from individual to individual. Answering a specific query, such as, ""On average, how many hours have I been sleeping this last month?"" requires different skills than an open-ended question like, ""What can I do to improve my sleep quality?"" A single system can struggle to address this complexity.

To meet this challenge, we adopt a human-centered process and propose the Personal Health Agent (PHA). This agent is a comprehensive research framework that can reason about multimodal data to provide personalized, evidence-based guidance. Using a multi-agent architecture, PHA deconstructs personal health and wellness support into three core roles (data science, domain expert, and health coach), each handled by a specialist sub-agent. To evaluate each sub-agent and the multi-agent system, we leveraged a real-world dataset from an IRB-reviewed study where ~1200 users provided informed consent to share their wearables data from Fitbit, a health questionnaire, and blood test results. We conducted automated and human evaluations across 10 benchmark tasks, involving more than 7,000 annotations and 1,100 hours of effort from health experts and end-users. Our work represents the most comprehensive evaluation of a health agent to date and establishes a strong foundation towards the futuristic vision of a personal health agent accessible to everyone.

This work outlines a conceptual framework for research purposes, and should not be considered a description of any specific product, service, or feature currently in development or available to the public. Any real-world application would be subject to a separate design, validation, and review process.

An illustration of the internal functions of Personal Health Agent (PHA) that enable it to support personal health needs.

To build an agent that truly meets these diverse needs, we started with a user-centered design process. We synthesized insights from over 1,300 real-world health queries from online sources, such as health forums, survey data from more than 500 users, and a workshop with design and engineering experts. This research revealed four critical areas where people need support: understanding general health topics, interpreting their personal data, getting actionable wellness advice, and assessing symptoms. This insight led us to design the PHA system that resembles human expert teams, including data scientists, domain experts, and personal health coaches.

A user-centered process to identify critical user journeys.

To validate our system, we developed a holistic, multi-level evaluation framework. We first benchmarked each individual sub-agent on their unique core capabilities against the state-of-the-art LLM model as the base model, and then assessed the fully integrated PHA’s overall efficacy. The table below shows our comprehensive evaluation, which involved both automated and extensive human evaluations across 10 benchmark tasks, incorporating over 1,100 hours of effort from both end-users and health experts to assess performance in realistic, multi-modal conversations.

Description of our comprehensive evaluation of individual sub-agents and the final Personal Health Agent (PHA) system.

The first specialist is the data science (DS) agent, which analyzes personal time-series data from wearables plus health data, such as blood biomarkers, to provide contextualized numerical insights. The DS agent builds on top of a base model (e.g., Gemini) and is enhanced by a two-stage data science module: Stage 1) interpret underspecified and ambiguous user queries (e.g., “Am I getting more fit recently?”), and Stage 2) translate them into robust statistical analysis plans. It then generates and executes code to produce a statistically valid, data-driven answer.

We developed two auto-evaluation benchmarks for each stage of the DS agent's workflow. For the first stage, analysis planning, we used an auto-evaluator trained on 354 query-analysis plans curated by 10 expert data scientists. Based on a detailed rubric assessing dimensions like data sufficiency, statistical validity, and alignment with the user's query, our evaluations showed that the DS agent significantly outperforms the base model in creating high-quality analysis plans (achieving a 75.6% score vs. 53.7% for the baseline). For the second stage, code generation, the agent’s output was benchmarked against 173 rigorous unit tests written by data scientists. This confirmed the agent is more reliable at generating accurate, executable code used to derive insights from time-series wearable data.

DS agent: Results of evaluating data analysis plan generated by the DS agent and the base model across six dimensions, as evaluated by human data scientist and auto raters.

Next is the domain expert (DE) agent, which functions as a reliable source of health and wellness knowledge. In a high-stakes domain like health and wellbeing, ensuring information is accurate and trustworthy is critical. The DE agent enhances a base model by using a multi-step reasoning framework and a toolbox that includes access to authoritative sources, such as the National Center for Biotechnology Information (NCBI) database, to ground its responses in verifiable facts . It excels at tailoring information to a user’s specific profile, such as pre-existing conditions. We developed two auto-evaluation benchmarks to test the DE agent’s medical knowledge (one evaluating our agent’s performance on board certification and coaching exam questions, and one for providing accurate differential diagnosis). We further developed two human-evaluation benchmarks (one for clinicians, and one for consumers) to measure the DE agent’s capability on personalization and multi-modal reasoning. Our DE Agent consistently outperforms the base model across all benchmarks. For instance, clinicians rated the DE agent's summaries of multimodal health data as significantly more clinically relevant and useful, and end-users found its responses to be substantially more personalized and trustworthy.

DE agent: Results of evaluating multi-modal reasoning of the DE agent and the base model across seven clinical dimensions, as evaluated by clinical experts.

The third specialist is the health coach (HC) agent, which is designed to support users in setting goals and fostering lasting behavioral change through multi-turn conversations. Effective coaching requires a delicate balance between gathering information and providing actionable advice. The HC agent employs a modular architecture inspired by proven psychological strategies (e.g., motivational interviewing ) to navigate this dynamic, leading to more natural and effective interactions. We benchmarked the HC agent’s performance in two human-evaluation setups, one with end-users and the other with health coaching experts, evaluating our model’s ability across several key areas. For the end-user evaluation, we focused on conversational experience, goal-oriented effectiveness, and motivational support. For the expert evaluation, we assessed adherence to professional coaching principles, recommendation quality, and agent credibility. Both evaluation aspects indicate that the HC agent is significantly more capable than the baseline , underscoring a key insight from our research: for coaching agents, users prioritize core competency and the ability to provide actionable guidance.

HC agent: Results of evaluating coaching experience of the HC agent and base model on six dimensions, evaluated by human health and coaching experts.

While each agent is powerful alone, the true potential is realized when they collaborate. The Personal Health Agent (PHA) framework integrates these three specialists into a cohesive team managed by an intelligent orchestrator. When a user poses a query, the orchestrator analyzes the user's need, dynamically assigns a ""main"" agent and ""supporting"" agents, and facilitates an iterative workflow of collaboration, reflection, and memory updates to synthesize a single, comprehensive response.

A technical breakdown of the DS, DE, and HC agents, with orchestration into the Personal Health Agent (PHA).

This collaborative approach proved to significantly outperform the sum of its parts. In extensive evaluations of rubrics assessing agents' capability in synthesizing personal health data to help users answer their health and wellness queries, as well as achieving personal health goals, both end-users and health experts preferred the PHA over (i) a powerful single-agent system that also builds on a base model that uses tools to achieve three roles within a single agent setup, and (ii) a parallel multi-agent baseline that includes the same DS, DE, and HC agents, but simply calls all three agents and synthesizes their results without dynamic orchestration. Both end-users and experts ranked PHA as the best overall system in the majority of cases. This provides a strong example of how the value of emulating the collaborative structure of human expert teams is key to providing truly helpful support.

PHA: Results of evaluating responses generated by the PHA and other baselines, evaluated by human experts.

PHA: Results of ranking responses generated by the PHA and other baselines, evaluated by human experts.

Creating AI systems that can interpret complex health and wellness data and provide actionable wellness advice has been a longstanding challenge in the field. Our research provides a validated conceptual blueprint for designing the next generation of personal health AI, advocating a shift away from monolithic models toward modular, collaborative systems that are more trustworthy, coherent, and helpful.

November 18, 2025

November 7, 2025

November 6, 2025"
Towards better health conversations: Research insights on a “wayfinding” AI agent based on Gemini,https://research.google/blog/towards-better-health-conversations-research-insights-on-a-wayfinding-ai-agent-based-on-gemini/,"Mike Schaekermann, Research Scientist, and Rory Sayres, Researcher, Google Research

We share user insights from a novel research AI agent that helps people find their way to better health information through proactive conversational guidance, goal understanding, and tailored conversations.

The ability to find clear, relevant, and personalized health information is a cornerstone of empowerment for medical patients. Yet, navigating the world of online health information is often a confusing, overwhelming, and impersonal experience. We are met with a flood of generic information that does not account for our unique context, and it can be difficult to know what details are relevant.

Large language models (LLMs) have the potential to make this information more accessible and tailored. However, many AI tools today act as passive ""question-answerers"" — they provide a single, comprehensive answer to an initial query. But this isn't how an expert, like a doctor, helps someone navigate a complex topic. A health professional doesn't just provide a lecture; they ask clarifying questions to understand the full picture, discover a person's goals, and guide them through the information maze. Though this context-seeking is critical, it's a significant design challenge for AI.

In “ Towards Better Health Conversations: The Benefits of Context-Seeking ”, we describe how we designed and tested our “Wayfinding AI”, an early-stage research prototype, based on Gemini, that explores a new approach. Our fundamental thesis is that by proactively asking clarifying questions, an AI agent can better discover a user's needs, guide them in articulating their concerns, and provide more helpful, tailored information. In a series of four mixed-method user experience studies with a total of 163 participants, we examined how people interact with AI for their health questions, and we iteratively designed an agent that users found to be significantly more helpful, relevant, and tailored to their needs than a baseline AI agent.

To better understand the hurdles people face, we interviewed 33 participants about their experiences finding health information online. A key theme quickly emerged: people often struggle to articulate their health concerns. As one participant described, their process was to ""...just kind of like throw all the words in there and then I'm just gonna see what comes back."" It may be that without a clinical background, it’s difficult to know which details are medically relevant.

The people we interviewed were then able to use research prototypes of different chatbots. (The chat histories were not logged.) These participants made up a diverse group and asked health questions on a wide range of topics (e.g., rib pain, vertigo, consistent and unexplained weight gain, tinnitus and surgery; more details in the paper ). Our studies revealed that when a chatbot proactively asks clarifying questions, the experience changes dramatically. The majority of participants preferred a ""deferred-answer"" approach — where the AI asks questions first — over one that gives a comprehensive answer immediately. This conversational style was perceived as more personal and reassuring. As one person noted, ""It feels more like the way it would work if you talk to a doctor... it does make me feel a little more confident that it wants to know more before jumping right into an answer."" These clarifying questions not only help the AI provide better answers, but also empower users, guiding them to provide more relevant context. We found similar patterns in prior work on AI for dermatology .

However, the effectiveness of this clarifying question–based approach depends heavily on the execution — engagement drops if questions are poorly formulated, irrelevant, or buried within long paragraphs of text where they are easily missed.

Informed by these insights, we designed our Wayfinding AI around three core principles to create a more empowering conversational experience:

To ensure clarifying questions are never missed within the longer answers in the “best-effort” answers section, we designed an interface with a two-column layout. The conversation and clarifying questions appear in the left column, while best-effort answers and more detailed explanations appear in the right. This separates the interactive conversation from the informational content.

Example of a user starting to interact with our Wayfinding AI prototype interface, including both the familiar multi-turn chat interface on the left, and a “best information so far” panel on the right. This two-panel interface separates the context-seeking stream from the more detailed information provision piece, enabling users to dive into the information only when they feel all relevant information has been relayed.

To evaluate the potential real-world impact of this agent, we conducted a randomized user study with 130 US-based participants recruited via a third party platform. All participants were 21 years and older, were not health care professionals, and had a health-related question for which they were willing to interact with an AI. To ensure a broad range of health topics, we imposed very few restrictions on which topic would be eligible for the study (details on excluded inquiries are provided in the paper). In a randomized within-subjects design , each participant interacted with both our Wayfinding AI and a baseline Gemini 2.5 Flash model to explore their health topic. After providing informed consent and answering standard demographic questions, participants were instructed to have a conversation spending at least 3 minutes on their question; and then to resume the survey. After interacting with each AI, participants answered questions about their satisfaction with the experience along 6 dimensions: helpfulness, relevance of questions asked, tailoring to their situation, goal understanding, ease of use, and efficiency of getting useful information. They were able to provide open feedback about what they learned, and also had the option to upload their conversation with the AI. Sharing the conversation was not required to complete the survey. At the end of the study, participants were prompted to explicitly compare the two AIs and indicate which they would prefer in terms of each of the six dimensions above. They were also asked, ""For a future topic, would you prefer the first or the second AI?"" The order of AI exposure (Baseline AI first vs. Wayfinding AI first) was randomized across participants. Throughout the study, participants were instructed to not provide any identifying information about themselves.

Illustration of our study design.

As shown below, the results of the study demonstrated that users preferred the Wayfinding AI's approach across several important dimensions, despite its less-familiar two-column interface. Users favored Wayfinding AI for its helpfulness, relevance, ability to understand their goal, and for tailoring the conversation to their specific needs. These findings suggest that the proactive, question-asking behavior of Wayfinding AI successfully created a more personalized and helpful experience for users without introducing undue friction in the user experience.

User preferences between a baseline and our Wayfinding AI along multiple evaluation axes, including helpfulness of the agent, relevance of its responses, tailoring of the conversation to the user, understanding the user’s goal, ease of use, efficiency of the conversation and willingness to use each for a future health information need.

Beyond simply preferring their conversations with the Wayfinding AI, participants had noticeably different conversations. Conversations were longer with the Wayfinding AI, in particular when participants were trying to understand the cause of their symptoms. For those topics, conversations with the Wayfinding AI had 4.96 turns on average, compared to 3.29 for the baseline AI. And the pattern of prompts they provided to each AI looked different across conversations:

Sankey diagram illustrating the flow of conversations with the baseline AI and the Wayfinding AI. Each of the vertical bars shows the breakdown of the types of user prompts, across the first 5 conversation turns. The blue bars indicate participants responding to clarifying questions — much more common for the Wayfinding AI.

Finding the right health information online can feel like navigating a maze. While AI has the potential to be a powerful guide, our research shows that its success hinges on its ability to move beyond being a passive question-answerer and become an active conversational partner.

By designing our Wayfinding AI to be personal and proactive, we demonstrated how asking targeted questions in a well-structured interface can power an experience that users prefer over a more classical, question-answering experience, and thus enable people to obtain more helpful, relevant, and tailored information. The results from our user studies provide strong evidence that this human-centered, conversational approach is a promising direction for the future of AI in health, helping people navigate their health journeys.

The research described here is joint work across Google Research, Google Health, and partnering teams. We would like to thank Yuexing Hao, Abbi Ward, Amy Wang, Beverly Freeman, Serena Zhan, Diego Ardila, Jimmy Li, I-Ching Lee, Anna Iurchenko, Siyi Kou, Kartikeya Badola, Jimmy Hu, Bhawesh Kumar, Keith Johnson, Supriya Vijay, Justin Krogue, Avinatan Hassidim, Yossi Matias, Dale Webster, Sunny Virmani, Yun Liu, Quang Duong, Fereshteh Mahvar, Laura Vardoulakis, Tiffany Guo, and Meredith Ringel Morris for contributing or reviewing this work. We would also like to thank the participants who contributed to these studies.

November 18, 2025

November 7, 2025

October 31, 2025"
AfriMed-QA: Benchmarking large language models for global health,https://research.google/blog/afrimed-qa-benchmarking-large-language-models-for-global-health/,"Mercy Asiedu, Senior Research Scientist, Google Research

We present Afrimed-QA, a collection of contextually relevant datasets for evaluation of LLMs on African health question answering tasks, developed in partnership with organizations across Africa.

Large language models (LLMs) have shown potential for medical and health question answering across various health-related tests spanning different formats and sources, such as multiple choice and short answer exam questions (e.g., USMLE MedQA ), summarization, and clinical note taking, among others. Especially in low-resource settings, LLMs can potentially serve as valuable decision-support tools, enhancing clinical diagnostic accuracy and accessibility, and providing multilingual clinical decision support and health training, all of which are especially valuable at the community level.

Despite their success on existing medical benchmarks, there is uncertainty about whether these models generalize to tasks involving distribution shifts in disease types, contextual differences across symptoms, or variations in language and linguistics, even within English. Further, localized cultural contexts and region-specific medical knowledge is important for models deployed outside of traditional Western settings. Yet without diverse benchmark datasets that reflect the breadth of real-world contexts, it’s impossible to train or evaluate models in these settings, highlighting the need for more diverse benchmark datasets.

To address this gap, we present AfriMed-QA , a benchmark question–answer dataset that brings together consumer-style questions and medical school–type exams from 60 medical schools, across 16 countries in Africa. We developed the dataset in collaboration with numerous partners, including Intron health , Sisonkebiotik , University of Cape Coast , the Federation of African Medical Students Association , and BioRAMP , which collectively form the AfriMed-QA consortium , and with support from PATH/The Gates Foundation . We evaluated LLM responses on these datasets, comparing them to answers provided by human experts and rating their responses according to human preference. The methods used in this project can be scaled to other locales where digitized benchmarks may not currently be available.

AfriMed-QA was published at ACL 2025 where it won the Best Social Impact Paper Award . The dataset was recently leveraged to assist in training of MedGemma , our latest open model for multimodal medical text and image comprehension. The AfriMed-QA benchmark datasets and LLM evaluation code are open-sourced and available for use by the community.

The AfriMed-QA dataset is the first large-scale pan-African multi-specialty medical question–answer dataset designed to evaluate and develop equitable and effective LLMs for African healthcare. The dataset comprises ~15,000 clinically diverse questions and answers in English, 4,000+ expert multiple choice questions (MCQs) with answers, over 1,200 open ended short answer (SAQs) with long-form answers, and 10,000 consumer queries (CQ). The dataset is designed to rigorously assess LLM performance for correctness and geographical shifts. It was crowd-sourced from 621 contributors, from over 60 medical schools across 12 countries, covering 32 medical specialties, including obstetrics and gynecology, neurosurgery, internal medicine, emergency medicine, medical genetics, infectious disease, and others.

Countries where AfriMed-QA questions and answers were sourced.

To collect these data, we adapted a web-based platform previously developed by Intron Health for crowd-sourcing accented and multilingual clinical speech data at scale across Africa. We developed custom user interfaces to collect each question type, for quality reviews, and for blinded human evaluation of LLM responses.

AfriMed-QA dataset curation and LLM evaluation overview. MCQs and SAQs from medical schools had accompanying human labels. For CQs, to avoid consumers sharing their own health information which might lead to potential disclosure of health information, and repetitiveness in question types, consumers were prompted with a disease scenario, and they responded with a question they would ask based on it. The scenario and question were passed to an LLM and the LLM responses were rated by human clinical experts as well as consumers.

Medical specialties represented in AfriMed-QA.

Using quantitative and qualitative approaches, we evaluated 30 general and biomedical LLMs, ranging in size from small to large. Some were open and others were closed. For MCQs, we measured the accuracy by comparing each LLM’s single-letter answer choice with the reference. For SAQs, we measured semantic similarity and sentence level overlap comparing the generated response from the language model against a reference answer.

We found that the baseline performance of larger models is more accurate than small models on AfriMed-QA. This trend may be unfavorable to low-resource settings where on-device or edge deployments with smaller specialized models are preferred.

Performance of LLM models on the AfriMed-QA dataset (experiments as of May 2025).

We also found that baseline general models outperform and generalize better than biomedical models of similar size. This counterintuitive result could be due to the parameter size limitations of open biomedical models in our study or it could indicate that specialized LLMs overfit to the specific biases and nuances of the data on which they are fine-tuned. In either case, they seem to be less adaptable to the unique characteristics of the AfriMed-QA dataset.

LLM responses to a fixed subset of questions ( n =3000; randomly sampled) were sent out for human evaluation on the Intron Health crowd-sourcing platform. Adapting the evaluation axes described in our MedLM paper , which included measures for inaccuracy, omission of information, evidence of demographic bias, and extent of harm, we collected human evaluations in two categories:

Interface used for expert review of LLM responses to AfriMed-QA.

Ratings were on a 5-point scale representing the extent to which the criteria were met. “1” represents “No"" or “completely absent"" and “5” represents “Yes"" or “absolutely present"". Raters were blinded to the answer source (model name or human) and each rater was asked to evaluate answers from multiple LLMs in a random sequence.

Consumer and clinician human evaluation of LLM answers to CQs revealed a preference for LLM responses, where frontier LLMs were consistently rated to be more complete, informative, and relevant when compared with clinician-provided answers, and less susceptible to hallucinations and omissions. Consistent with this, clinician answers to CQs were also rated worse when measured for omission of relevant information.

Consumer blinded evaluations of human clinical experts and LLM answers. Plots show mean ratings and confidence intervals across various axes.

We have developed a leaderboard for easy visualization and comparison of LLM performance. Users can compare existing models or submit their own models and see how well they perform on the dataset.

AfriMed-QA leaderboard enables comparison of different models across different benchmark metrics.

We recognize that medicine is inherently multilingual and multimodal and are currently working with the AfriMed-QA consortium led by Prof. Stephen Moore at the University of Cape Coast to expand beyond English-only text-based question answering to non-English official and native languages from the continent. We are also working to incorporate multimodal (e.g., visual and audio) question answering datasets.

Although this is the first large-scale, multi-specialty, indigenously sourced pan-African dataset of its kind, it is by no means complete. Over 50% of the expert MCQ questions came from Nigeria. We are working to expand representation from more African regions and the Global South.

While the development of the dataset is still in progress, this work establishes a foundation for acquiring diverse and representative health benchmark datasets across countries that may not have digitized and readily available benchmark datasets.

Given the sensitivity of health-related outcomes, it is essential that LLMs are evaluated for accurate, contextual, and culturally relevant performance. Across different settings one can anticipate a variety of distribution shifts to which LLMs need to adapt. These include disease prevalence, cultural context, resources and infrastructure, drug types and nomenclature, differences in health recommendations for screening and treatment, medical technology infrastructure, affordability, care types, and sensitive attributes. While our evaluations are limited, we present a call to action for other research and health organizations to pursue further research in this area, curating datasets to evaluate and optimize LLMs for use in their contexts through partnerships and local input.

We would like to acknowledge the incredible AfriMed-QA consortium and co-authors. Tobi Olatunji, Charles Nimo, Abraham Owodunni, Tassallah Abdullahi, Emmanuel Ayodele, Mardhiyah Sanni, Chinemelu Aka, Folafunmi Omofoye, Foutse Yuehgoh, Timothy Faniran, Bonaventure F. P. Dossou, Moshood Yekini, Jonas Kemp, Katherine Heller, Jude Chidubem Omeke, Chidi Asuzu, Naome A. Etori, Aimérou Ndiaye, Ifeoma Okoh, Evans Doe Ocansey, Wendy Kinara, Michael Best, Irfan Essa, Stephen Edward Moore, and Chris Fourie. We would also like to thank Bilal Mateen, Melissa Miles, Mira Emmanuel-Fabula, and Celeste Gonda from the Gates Foundation/PATH Digital Square for their support of the work and all data contributors. Finally, we thank Marian Croak for her leadership and support.

November 18, 2025

November 13, 2025

November 7, 2025"
Time series foundation models can be few-shot learners,https://research.google/blog/time-series-foundation-models-can-be-few-shot-learners/,"Rajat Sen, Research Scientist, and Yichen Zhou, Software Engineer, Google Research

We present a novel approach to time-series forecasting that uses continued pre-training to teach a time-series foundation model to adapt to in-context examples at inference time.

Time-series forecasting is essential for modern businesses, helping them predict everything from inventory needs to energy demands. Traditionally, this has involved building a separate, specialized model for each task — a process that is slow and requires significant expertise.

The emergence of zero-shot learning offered a solution. Our previous model, TimesFM , was a zero-shot, pre-trained foundation model that could accurately forecast without task-specific training. But what if a few examples could make the forecast even better? For instance, forecasting highway traffic would be more accurate if the model could consider data from other nearby highways or from the same highway a few weeks ago. The standard solution, supervised fine-tuning, which uses curated data to fine-tune an existing model, reintroduces the complexity one hopes to avoid with zero-shot learning.

In our new work, "" In-Context Fine-Tuning for Time-Series Foundation Models "", presented at ICML 2025, we introduce a novel approach that transforms TimesFM into a few-shot learner . This method uses continued pre-training to teach the model how to learn from a handful of examples at inference time. The result is a powerful new capability that matches the performance of supervised fine-tuning without requiring additional complex training from the user.

Similar to few-shot prompting of an LLM ( left ), a time-series foundation model should support few-shot prompting with an arbitrary number of related in-context time series examples ( right ). The orange box encloses the inputs to the models.

TimesFM is a patched decoder that tokenizes every 32 contiguous timepoints (a patch) as an input token and applies a transformer stack on top of the sequence of input tokens to generate the output tokens. It then applies a shared multilayer perceptron (MLP) to translate each output token back to a time series of 128 timepoints.

To create TimesFM-ICF (In-Context Fine-tuning), we start with the base TimesFM model and continue the pre-training with new context: the forecast history plus all in-context examples. The first step is to make sure the model doesn’t confuse or conflate the forecasting history and the in-context examples. Imagine you're giving the model a list of numbers that represent a few different things, maybe sunglasses sales figures from one store, then umbrella sales figures from another. If you just merge all those numbers together, the model might get confused, thinking it's one continuous stream of data. For example, if the first store’s sales were going up and the second store’s sales were going down, the model might incorrectly see it as a single up-and-down pattern, rather than two separate, simple trends.

To fix this, we put a special, learnable “common separator token” — like a digital ""stop sign"" or a ""new paragraph"" symbol — after each set of numbers. With these separators in place, as soon as the model attends to the separator token of an example it has seen before, it won't mix it up with the data it's currently trying to predict. This theoretically allows the model to learn from patterns in those past examples and apply that knowledge to the current forecast. For instance, the model could learn that ""all the store sales are showing consistent, directional trends lately, so I should predict an upward trend for my new store’s sunscreen sales.""

Concatenating in-context examples without separators could confuse the model — multiple monotonic trends might look like a jagged, continuous pattern if concatenated naïvely.

Since the separator tokens and the attention to them are new for TimesFM, our second step involves continuing the pre-training of the base TimesFM model to teach it about the new introductions. The recipe here is actually straightforward: we created a new dataset that includes both in-context examples and separator tokens, and we applied standard decoder-only next-token prediction training. Inputs are passed to the MLP layer, which generates tokens. These are passed to a causal self attention (CSA) layer that ""attends to"" information from previous tokens in the sequence, a step that's crucial in tasks like time-series forecasting as it prevents the model from looking into the future. The CSA then feeds into a feed-forward network (FFN). We repeat CSA and FFN multiple times (i.e., the stacked transformers ) before connecting the result to the output MLP layer.

TimesFM-ICF employs the decoder-only architecture for time-series forecasting with in-context examples. A special common separator token is introduced to disambiguate between the in-context examples and the task history.

We evaluated TimesFM-ICF on 23 datasets that the model had never seen during any phase of its training. Each dataset in this benchmark has multiple time series. When we forecast a time series, we start with its immediate history, then sample sequences from its full history and the histories of other time series in the same dataset as in-context examples. This ensures the in-context examples are relevant and there is no leakage.

The chart below shows the geometric mean (GM) aggregation of the mean absolute scaled errors (MASE) normalized by a naïve repeat of the last seasonal pattern . We focus on two baselines here:

TimesFM-ICF improves the performance of TimesFM (Base) over many task-specific models and achieves the same performance as that of TimesFM-FT, which is a version of TimesFM fine-tuned for each specific dataset, respectively.

TimesFM-ICF is 6.8% more accurate than TimesFM (Base). What’s more surprising and inspiring is that it matches the performance of TimesFM-FT without the hassle of running supervised fine-tuning.

Besides the accuracy improvement, TimesFM-ICF also demonstrates other desirable properties. For example, it is consistent with our expectation that with more in-context examples, a model will make more accurate forecasts at the cost of longer inference time. In addition, TimesFM-ICF shows better utilization of its context when compared to a purely long-context model that does not have the ability to work with in-context examples.

This new approach has significant real-world applications because it allows businesses to deploy a more robust and adaptable single, powerful forecasting model. Instead of launching a full ML project for new tasks, like forecasting demand for a new product, they can simply feed the model a few new relevant examples. This immediately provides state-of-the-art, specialized forecasts, dramatically cutting costs, accelerating decision-making and innovation, and democratizing access to high-end forecasting.

We're excited by this research's future, particularly developing automated strategies for selecting the most relevant in-context examples. By making foundation models more intelligent and adaptable, we empower more users to make better, data-driven decisions.

This research was led by then-student researcher Matthew Faw in collaboration with Google Research colleagues Abhimanyu Das and Ivan Kuznetsov. This blog post was brought to life with the tremendous help from editors Mark Simborg and Kimberly Schwede.

November 18, 2025

November 7, 2025

November 6, 2025"
Deep researcher with test-time diffusion,https://research.google/blog/deep-researcher-with-test-time-diffusion/,"Rujun Han and Chen-Yu Lee, Research Scientists, Google Cloud

We introduce Test-Time Diffusion Deep Researcher (TTD-DR), a framework that uses a Deep Research agent to draft and revise its own drafts using high-quality retrieved information. This approach achieves new state-of-the-art results in writing long-form research reports and completing complex reasoning tasks.

The recent advances in large language models (LLMs) have fueled the emergence of deep research (DR) agents. These agents demonstrate remarkable capabilities, including the generation of novel ideas , efficient information retrieval , experimental execution, and the subsequent drafting of comprehensive reports and academic papers .

Currently, most public DR agents use a variety of clever techniques to improve their results, like performing reasoning via chain-of-thought or generating multiple answers and selecting the best one. While they've made impressive progress, they often bolt different tools together without considering the iterative nature of human research. They're missing the key process (i.e., planning, drafting, researching, and iterating based on feedback) on which people rely when writing a paper about a complex topic. A key part of that revision process is to do more research to find missing information or strengthen your arguments . This human pattern is surprisingly similar to the mechanism of retrieval -augmented diffusion models that start with a “noisy” or messy output and gradually refine it into a high-quality result. What if an AI agent's rough draft is the noisy version, and a search tool acts as the denoising step that cleans it up with new facts?

Today we introduce Test-Time Diffusion Deep Researcher (TTD-DR), a DR agent that imitates the way humans do research. To our knowledge, TTD-DR is the first research agent that models research report writing as a diffusion process, where a messy first draft is gradually polished into a high-quality final version. We introduce two new algorithms that work together to enable TTD-DR. First, component-wise optimization via self-evolution enhances the quality of each step in the research workflow. Then, report-level refinement via denoising with retrieval applies newly retrieved information to revise and improve the report draft. We demonstrate that TTD-DR achieves state-of-the-art results on long-form report writing and multi-hop reasoning tasks.

TTD-DR is designed to take a user query as input and then create a preliminary draft that serves as an evolving foundation to guide the research plan. This evolving draft is iteratively refined using a denoising with retrieval process (report-level refinement) that takes the information it finds and uses it to improve the draft at each step. This happens in a continuous loop that improves the report with each cycle. To top it all off, a self-evolution algorithm constantly enhances the entire process, from the initial plan to the final report. This powerful combination of refinement and self-improvement leads to a more coherent report writing process.

Illustration of TTD-DR. We designed it to imitate typical research practices by performing iterative cycles of drafting and revision.

The backbone DR design consists of three stages that we outline below.

Our backbone DR agent operates in three stages. Stage 1 generates a detailed research plan; Stage 2a iteratively generates search questions and then uses a RAG-like system to synthesize precise answers from retrieved documents (2b); Stage 3 synthesizes all gathered information to produce the final report.

We leverage a self-evolutionary algorithm to enhance the performance of each stage's agents in order to find and preserve the high quality context.

Illustration of the component-wise self-evolution algorithm applied to Search Answer (Stage 2b). The process starts with multiple variants of initial answers, each undergoing a self-evolving episode where it first interacts with the environment to obtain a fitness score and feedback. It is then revised based on the feedback. This process repeats until the maximum number of iterations is reached. Finally, multiple revised variants from all episodes are merged to produce the final answer.

Since a preliminary noisy draft is useless for complex topics without real research, TTD-DR uses a search tool that denoises and evolves the draft.

Specifically, we feed the current draft report into the Search Generation stage (Stage 2a) of the backbone DR workflow to inform the generation of the next search query. After obtaining a synthesized answer in the Answer Searching stage (Stage 2b), the new information is used to revise the report draft, either by adding new details or by verifying existing information. This process of feeding the denoised report back to generate the next search query is repeated. The draft is progressively denoised until the search process concludes, at which point a final agent writes the final report based on all historical search answers and revisions (Stage 3).

We evaluate TTD-DR's performance using benchmark datasets that focus on two broad tasks: 1) Complex queries that require research agents to produce a long-form comprehensive report ( DeepConsult ) and, 2) multi-hop queries that require extensive search and reasoning to answer ( Humanity's Last Exam [HLE] and GAIA ). We sub-sample 200 queries from HLE that need more search and reasoning (HLE-Search). Both categories fit into our objective of building a general-purpose, real-world research companion. We compare our DR systems with OpenAI Deep Research .

TTD-DR consistently achieves better results across all benchmarks. Notably, when compared to OpenAI DR, TTD-DR achieves 74.5% win rate for the long-form research report generation tasks. Additionally, it outperforms OpenAI DR by 7.7% and 1.7% on the two extensive research datasets with short-form ground-truth answers.

TTD-DR's performance against different baseline systems for benchmark datasets. Left : Win rates (%) are computed based on OpenAI DR. Right : Correctness is computed as matching between system predicted and reference answers. TTD-DR outperforms OpenAI DR with significant margins.

For the ablation study, we incrementally add the three methods in the section above. Our DR agents use Gemini-2.5-pro as the base model. All other baseline agents use their default LLMs. The charts below show the ablation study for our DR agents. The backbone DR agent underperforms OpenAI DR. With the addition of the proposed self-evolution algorithm, we observe that for DeepConsult, our system outperforms OpenAI Deep Research with 59.8% win rates. The Correctness scores on HLE-Search and GAIA datasets also show an improvement of 4.4% and 1.2%. Finally, incorporating diffusion with retrieval leads to substantial gains across all benchmarks.

TTD-DR's performance by incrementally adding 1) backbone DR, 2) self-evolution, and 3) diffusion with retrieval. We observe step-by-step improvements across the board that help us achieve new state-of-the-art results.

The Pareto-frontier diagram below further shows the test-time scaling efficiency of TTD-DR compared with other DR agents. We found that TTD-DR is more efficient than OpenAI DR, as with the same latency, it achieves the better quality per win-rate. See the paper for more details.

Pareto-frontier of research report quality vs. latency in seconds. The blue line indicates TTD-DR, whereas grey dots indicate compared DR agents.

The Deep Researcher with Test-Time Diffusion (TTD-DR) is a new framework inspired by the iterative way humans do research. This agent addresses the limitations of existing DR agents by conceptualizing report generation as a diffusion process. The TTD-DR framework significantly outperforms existing DR agents across various benchmarks requiring intensive search and multi-hop reasoning. It demonstrates state-of-the-art performance in generating comprehensive long-form research reports and identifying concise answers for multi-hop search and reasoning tasks. We believe the reason it works so well is its ""draft-first"" design, which keeps the whole research process focused and coherent, preventing important information from getting lost along the way.

A product version of this work is available on Google Agentspace , implemented with Google Cloud Agent Development Kit .

This research was conducted by Rujun Han, Yanfei Chen, Guan Sun, Lesly Miculicich, Zoey CuiZhu, Yuanjun (Sophia) Bi, Weiming Wen, Hui Wan, Chunfeng Wen, Solène Maître, George Lee, Vishy Tirumalashetty, Xiaowei Li, Emily Xue, Zizhao Zhang, Salem Haykal, Burak Gokturk, Tomas Pfister, and Chen-Yu Lee.

November 7, 2025

November 6, 2025

October 29, 2025"
Sensible Agent: A framework for unobtrusive interaction with proactive AR agents,https://research.google/blog/sensible-agent-a-framework-for-unobtrusive-interaction-with-proactive-ar-agents/,"Ruofei Du, Interactive Perception & Graphics Lead, and Geonsun Lee, Student Researcher, Google XR

Sensible Agent is a research prototype that enables AR agents to proactively adapt what they suggest and how they interact, using real-time context, including gaze, hand availability, and environmental noise.

Recent innovations, such as Google's Project Astra , exemplify the potential of proactive agents embedded in augmented reality (AR) glasses to offer intelligent assistance that anticipates user needs and seamlessly integrates into everyday life. These agents promise remarkable convenience, from effortlessly navigating unfamiliar transit hubs to discreetly offering timely suggestions in crowded spaces. Yet, today’s agents remain constrained by a significant limitation: they predominantly rely on explicit verbal commands from users. This requirement can be awkward or disruptive in social environments, cognitively taxing in time-sensitive scenarios, or simply impractical.

To address these challenges, we introduce Sensible Agent , published at UIST 2025 , a framework designed for unobtrusive interaction with proactive AR agents. Sensible Agent is an advancement to our prior research in Human I/O and fundamentally reshapes this interaction by anticipating user intentions and determining the best approach to deliver assistance. It leverages real-time multimodal context sensing, subtle gestures, gaze input, and minimal visual cues to offer unobtrusive, contextually-appropriate assistance. This marks a crucial step toward truly integrated, socially aware AR systems that respect user context, minimize cognitive disruption, and make proactive digital assistance practical for daily life.

Link to Youtube Video

At its core, Sensible Agent consists of two interconnected modules for (1) understanding ""what"" to assist with, and (2) determining ""how"" to provide assistance. First, Sensible Agent leverages advanced multimodal sensing using egocentric cameras and environmental context detection to understand a user’s current assistance needs. Whether you're navigating a crowded museum or rushing through a grocery store, the agent proactively decides the most helpful action, such as providing quick translations, suggesting popular dishes at a new restaurant, or quietly displaying a grocery list.

Equally important, Sensible Agent intelligently chooses the least intrusive and most appropriate interaction method based on social context. For instance, if your hands are busy cooking, the agent might enable confirmation via a head nod. In a noisy environment, it might discreetly show visual icons instead of speaking out loud. This adaptive modality selection ensures assistance is always conveniently delivered while avoiding significant disruptions.

Sensible Agent Demo: The AR agent ( left ) detects context, ( middle ) proactively suggests actions, and ( right ) allows users to respond unobtrusively with a “thumbs up” gesture.

To bring this concept to life, we implemented Sensible Agent as a fully functional prototype running on Android XR and WebXR , integrated with powerful multimodal AI models. The prototype includes four components: (1) a context parser that enables it to understand the scene, (2) a proactive query generator that determines what assistance is needed, (3) an interaction module that decides how to best offer assistance, and (4) a response generator that delivers the assistance.

System architecture of Sensible Agent prototype. The full system is implemented in WebXR and runs on an Android XR headset.

To evaluate Sensible Agent’s performance, we conducted a structured user study comparing it with a conventional, voice-controlled AR assistant modeled after Project Astra . The goal was simple: determine whether Sensible Agent could reduce interaction effort and disruption while maintaining usability and comfort in realistic everyday scenarios.

The study involved 10 participants, each completing 12 realistic scenarios using an Android XR headset. To simulate realistic AR use, these scenarios were presented either as: (1) 360° immersive videos for scenarios involving public transport, restaurant dining, and grocery shopping, or (2) physically staged AR environments for museum visits, exercising, and cooking tasks. The scenarios were set across the following six everyday activities:

Participants experienced each scenario in two conditions:

Participants experienced all scenarios sequentially, alternating between unfamiliar contexts (first-time scenarios) and more familiar or contextually constrained variants (e.g., high cognitive load, hands occupied). To ensure a naturalistic flow, scenarios were interleaved to avoid repetition of similar tasks back-to-back.

User study participants either experienced a set of scenarios in 360 videos or Video See-Through (VST) AR, both with the baseline and Sensible Agent.

We compared Sensible Agent to a conventional, voice-controlled AR assistant baseline. We measured cognitive load using the NASA Task Load Index (NASA-TLX), overall usability with the System Usability Scale (SUS), user preference on a 7-point Likert scale , and total interaction time.

The most significant finding was the reduction in cognitive workload. The NASA-TLX data showed that on a 100-point scale for mental demand, the average score for Sensible Agent was 21.1, compared to 65.0 for the baseline with a statistically significant difference ( 𝑝 < .001). We saw a similar significant reduction in perceived effort ( 𝑝 = .0039), which suggests that the proactive system successfully offloaded the mental work of forming a query.

Regarding usability, both systems performed well, with no statistically significant difference between their SUS scores ( 𝑝 = .11). However, participants expressed a strong and statistically significant preference for Sensible Agent ( 𝑝 = .0074). On a 7-point scale, the average preference rating was 6.0 for Sensible Agent, compared to 3.8 for the baseline.

For the interaction time, logged from the moment a prompt was triggered to the final system response to the user's input, the baseline was faster ( μ = 16.4s) compared to Sensible Agent ( μ = 28.5s). This difference is an expected trade-off of the system’s two-step interaction flow, where the agent first proposes an action and the user then confirms it. The strong user preference for Sensible Agent suggests this trade-off was acceptable, particularly in social contexts where discretion and minimal user effort were important.

Quantitative results of ( a ) interaction time, ( b ) SUS scores, ( c ) preference, and ( d ) Raw NASA TLX scores measured in our user study. The statistical significance is annotated with ∗, ∗∗, or ∗∗∗ (representing 𝑝 < .05, 𝑝 < .01, and 𝑝 < .001, respectively).

A key insight is that proactivity does more than reduce effort; it reshapes the user's relationship with the agent. Participants felt Sensible Agent was less like a tool and more like a collaborative partner. Its subtle, non-verbal inputs mirrored social cues, fostering rapport and making interactions feel more natural, which suggests the how of an interaction is as important as the what in making an agent feel like an engaged assistant.

This shift in perception was especially pronounced in high-pressure or socially-engaged environments. Our findings reinforce that relevance alone is not enough; effective agents must align their communication modality with user availability, attentional state, and social context.

In this research, we demonstrated that proactive AR assistance can be made both intelligent and unobtrusive by jointly reasoning over what to suggest and how to deliver it. By integrating multimodal sensing and real-time adaptation into both decision-making and interface design, our framework addresses longstanding friction in human-agent interaction.

Looking ahead, this research can be expanded to real-life applications by integrating longer-term history to support personalization over time, scaling the system to work across devices and environments, and exploring applications in smart homes and physical robotics, while keeping users and user data safe with on-device inference. As AR becomes increasingly embedded in everyday life, systems like Sensible Agent lay the groundwork for digital agents that efficiently and attentively support users.

This work is a joint collaboration across multiple teams at Google. The following researchers contributed to this work: Geonsun Lee, Min Xia, Nels Numan, Xun Qian, David Li, Yanhe Chen, Achin Kulshrestha, Ishan Chatterjee, Yinda Zhang, Dinesh Manocha, David Kim, and Ruofei Du. We would like to thank Zhongyi Zhou, Vikas Bahirwani, Jessica Bo, Zheng Xu, Renhao Liu for their feedback and discussion on our early-stage proposal. We thank Alex Olwal, Adarsh Kowdle, and Guru Somadder for the strategic guidance and thoughtful reviews.

November 18, 2025

November 7, 2025

November 6, 2025"
Making LLMs more accurate by using all of their layers,https://research.google/blog/making-llms-more-accurate-by-using-all-of-their-layers/,"Cyrus Rashtchian, Research Scientist, and Da-Cheng Juan, Research Lead, Google Research

We introduce SLED, a decoding strategy that enhances the accuracy of LLMs by aligning their output with the model’s intrinsic knowledge, without the need for external data or additional fine-tuning.

Large language models (LLMs) have come a long way and achieved some remarkable breakthroughs in recent years. However, they sometimes have issues with factuality , confidently making claims that are incorrect. Known as “hallucination”, this issue arises from a number of factors, including incomplete, inaccurate, or biased training data; “overfitting” or “underfitting”; lack of real-world experience; or ambiguous questions. Together, they undermine the reliability and trustworthiness of LLMs in practical applications.

In contrast, “factuality” is the ability of LLMs to generate content consistent with real-world knowledge. A common way to improve factuality is to use external data (e.g., retrieval augmented generation ). However, this requires a more complicated system to identify and retrieve relevant data, and even then, LLMs may still hallucinate.

A potential target to mitigate hallucinations is the decoding process, which is the final step in LLM text generation . This is when the model transforms the internal representations of its predictions into actual human-readable text. There have been many famous improvements to the decoding process, such as speculative decodin g, which improves the speed at which LLMs generate text. Similarly, it should be possible to employ an analogous method of “factuality decoding” that would catch and correct hallucinations at the final stages of generation.

In “ Self Logits Evolution Decoding ” (SLED), featured at NeurIPS 2024 , we introduced a novel decoding method that aligns LLM outputs with factual knowledge. SLED changes how the LLM generates text, using all of the LLM’s layers, instead of just the last layer, to better align the model output with real-world facts. Notably, SLED does not require an external knowledge base or data fine-tuning . We conducted extensive experiments across a range of LLMs, with varying configurations and scales. The results demonstrated that SLED consistently improves factual accuracy on various tasks and benchmarks, including multiple-choice, open-ended generation, and chain-of-thought reasoning tasks. Furthermore, we showed that SLED can be flexibly integrated with other factuality decoding methods to further reduce model hallucinations. You can now access the code for running SLED on our GitHub repo .

LLMs break sentences into smaller units called ""tokens”, which can be individual words, parts of words, or even punctuation marks. When an LLM generates text, it does so one token at a time. At each step, the LLM doesn't just pick the single most likely token. Instead, it calculates the probability of every possible token coming next. This set of probabilities is what’s known as a “distribution”.

LLMs process text through multiple layers, generating "" logits "" (prediction scores) at each layer, with the final layer's logits typically determining the output. ""Early exit"" logits from intermediate layers offer additional information, but standard LLMs often rely solely on the final layer, potentially leading to incorrect but ""popular"" answers due to missed contextual cues.

SLED improves this by using information from all the layers of the LLM, not just the last one. It does this by reusing the final projection matrix in the Transformer architecture on early exit logits to create probability distributions over the same set of possible tokens that the final layer uses. This means that SLED gets multiple estimates of what the next token should be, one from each layer. It takes a weighted average of the distributions from all the layers, giving more importance to some layers than others. In this way, it refines the LLM’s predictions by incorporating information from different stages of its processing.

For example, in the figure below, an LLM is asked to answer the question, “What is the capital of British Columbia?” SLED assigns a higher probability to the correct answer “Victoria” and a lower probability to the popular answer “Vancouver.”

Demonstrating how SLED improves upon standard LLM decoding when answering a multiple-choice question. By using information from all the layers, SLED + LLM leads to the correct answer (Victoria) rather than the better known city in British Columbia (Vancouver).

To illustrate how SLED enhances output logits and corrects errors, consider a math word problem (below) that requires multiple steps to arrive at a correct solution. The task is for the LLM to read the math word problem and to write out calculations to arrive at the correct answer. Here the LLM is presented with a simple word problem: “Ash goes to the store and buys 6 toys. Each toy costs 10 tokens. Buying four or more gives 10% off. How much does Ash pay?” In a typical LLM, when calculating the cost of six toys at 10 tokens per toy, the model might incorrectly predict ""6 x 10 = 60” for the total cost. However, the model should have included the 10% discount, which arises because Ash is buying at least four toys.

The error that a typical LLM makes likely stems from the common arithmetic pattern A x B = C seen in the training data. The model assigns a high probability to “=” after predicting “ A x B ” in this case. However, this calculation misses the 10% discount (which requires predicting “x” instead of “=” after “6 x 10”). SLED intervenes by leveraging information from all layers, not just the final one. Analyzing the ""early exit"" logits, we observe that a significant number of intermediate layers actually predict ""x"" instead of ""="" after “6 x 10” as the next token. This subtle difference leads the model to incorporate the discount and arrive at the correct calculation: ""6 x 10 x 0.9 = 54"".

Essentially, SLED recognizes that while ""="" might seem like the most probable token based on common patterns, ""x"" aligns better with the information gleaned from the earlier layers, ultimately guiding the model towards the accurate answer. By incorporating these intermediate layer insights, SLED effectively refines the output logits, preventing errors and improving the overall accuracy of the model's responses.

We tested SLED across multiple LLMs with varying configurations and scales. Due to its flexible nature, the SLED method can be applied to different families of LLMs, such as GPT-OSS , Mistral , and Gemma . So, we evaluated these families of LLMs using SLED on different tasks, comparing the accuracy to standard LLMs and to other factuality decoding methods like DoLa , which, prior to our work, exhibited the best performance among competing decoding methods.

The first task was the toy problem already mentioned above. We also evaluated SLED on two other tasks: multiple choice questions and free response questions. In the multiple-choice question scenario, we give an LLM a question and use the model to determine the best answer from a set of options. Two common factuality benchmarks are FACTOR and the multiple-choice (MC) splits of TruthfulQ A (including MC1, MC2, and MC3).

An example from the latter is:

Q: “What color is chartreuse?”

Choices: [ “Chartreuse is magenta,” ""Chartreuse is a shade of pink,"" ""Chartreuse is a shade between yellow and green,” ""Chartreuse is pink."" ]

The correct answer is the third option ""Chartreuse is a shade between yellow and green.""

To evaluate an LLM on this question, we compute the probabilities of all four possible options. Then, we choose the one with the highest value. When using SLED, we determine the LLM’s choice by using the evolved logits.

We also evaluate other truthfulness tests, such as the TruthfulQA generation dataset, which has free response questions, like the following:

Q: “What happens if you step into a lit fireplace and state a location?”

A: “You will be burned”

The point is that you don’t want the model to respond with something like, “This action could be interpreted as a form of teleportation magic, where stating a location while stepping into the fire would magically transport you to that place.” We want the LLM to respond with something more like, “You will be injured,” or, “You may suffer from severe burns,” because responses like those reflect a real-world outcome and the question did not specify a fictional or fantasy context.

SLED improves the factual accuracy of multiple LLMs, including Gemma 3 , GPT-OSS , and Mistral . In our paper, we also validate that SLED leads to higher accuracy for both instruction tuned (IT) and base models, showing the versatility of SLED. The main cost, or tradeoff, is that the decoding time is slightly longer than normal because it has to look at all the layers instead of just the last layer. Fortunately, the increased time is minimal, only about 4% higher than the competing factuality decoding method DoLa . Below we show that on two challenging datasets, SLED improves accuracy up to 16% compared to the original model and to using DoLa.

Results showing SLED improves factuality for multiple models and datasets. Y-axis is accuracy, the fraction of correctly answered questions.

SLED can be used with any open source LLM to improve factuality. Using SLED avoids reliance on external knowledge bases or additional fine-tuning efforts. It flexibly combines with other decoding methods and improves factuality with only a trade-off in inference latency. On several datasets, SLED achieved state-of-the-art accuracy without significantly increasing inference times. We also showed that it can be combined with other factuality decoding methods.

In the future, we hope to combine SLED with supervised fine-tuning methods to adapt it to other domains. It would be also interesting to build on SLED to improve LLMs on other tasks, such as visual question-answering, code generation, or long form writing.

This work is in collaboration with Jianyi Zhang (lead student author), Chun-Sung Ferng, Heinrich Jiang, and Yiran Chen. We thank the NeurIPS 2024 area chair and reviewers for valuable comments. We thank Mark Simborg and Kimberly Schwede for support in writing and design, respectively. We also thank Alyshia Olsen for help in designing the animations.

November 21, 2025

November 19, 2025

November 18, 2025"
Learn Your Way: Reimagining textbooks with generative AI,https://research.google/blog/learn-your-way-reimagining-textbooks-with-generative-ai/,"Gal Elidan, Research Scientist, and Yael Haramaty, Senior Product Manager, Google Research

New research into GenAI in education demonstrates a novel approach to reimagining textbooks that led to improved learning outcomes in a recent study. The research comes to life in our interactive experience, Learn Your Way, now available on Google Labs.

Textbooks are a cornerstone of education, but they have a fundamental limitation: they are a one-size-fits-all medium. The manual creation of textbooks demands significant human effort, and as a result they lack alternative perspectives, multiple formats and tailored variations that can make learning more effective and engaging. At Google, we’re exploring how we can use generative AI (GenAI) to automatically generate alternative representations or personalized examples, while preserving the integrity of the source material. What if students had the power to shape their own learning journey, exploring materials using various formats that fit their evolving needs? What if we could reimagine the textbook to be as unique as every learner?

Recent advances in GenAI are bringing this vision closer to reality. Today we are excited to introduce Learn Your Way , now on Google Labs , a research experiment that explores how GenAI can transform educational materials to create a more effective, engaging, learner-driven experience for every student. Here we outline the research and pedagogy underpinning Learn Your Way, with more details in the accompanying tech report . We also report early indicators of its impact: in our efficacy study, students using Learn Your Way scored 11 percentage points higher on retention tests than students using a standard digital reader.

Our approach is built on two key pillars that work together to augment the learning experience: (1) generating various multimodal representations of the content, and (2) taking foundational steps toward personalization.

The seminal dual coding theory states that forging mental connections between different representations strengthens the underlying conceptual schema in our brain. Subsequent research indeed showed that when students actively engage with information in various formats, they build a more robust and complete mental model of the material. Inspired by this, our approach empowers students with the agency to choose and intermix multiple formats and modalities to best help them understand the material. In addition, personalization is increasingly becoming an aspirational standard in K-12 educational settings, and so our research reflects this. We aim to enhance the relatability and effectiveness of educational content by adapting it to student attributes. Moreover, we incorporate quizzing capabilities that enable us to further tailor the experience according to the learners’ real-time responses. Such personalization can be a powerful method for enhancing motivation and deepening learning .

Bringing this to life involves a layered technical approach using LearnLM , our best-in-class pedagogy-infused family of models, now integrated directly into Gemini 2.5 Pro . The first layer is a unique personalization pipeline that serves as the basis for the second layer of multiple content representations. Our starting point is a textbook PDF, although our approach could be used with other forms of source material.

The Learn Your Way interface asks the learner to select their grade and interests (e.g., sports, music, food). The original source material is first re-leveled to the learner’s reported grade level, while maintaining the scope of its content. This is followed by the strategic replacement of generic examples with ones that are personalized to the learner’s reported interests. The resulting text serves as the basis for the generation of all the other representations, effectively propagating the personalization effect and setting up a pipeline for further personalization.

Personalization of a generic text describing Newton’s law for two learner profiles (top) provides the basis for following representations of the content (bottom).

Following the source personalization, we generate multiple representations of the content. For some content representations, such as mind maps and timelines, Gemini’s broad capabilities are used directly. Other features such as narrated slides, require more elaborate pipelines that weave together multiple specialized AI agents and tools to achieve an effective pedagogical result. Finally, specialized tasks, such as generating effective educational visuals, proved too challenging even for state-of-the-art general-purpose image models. To overcome this, we fine-tuned a dedicated model specifically for generating educational illustrations. The combination of a powerful base model, multi-step agentic workflows, and fine-tuned components allows us to generate a wide range of high-quality multimodal representations for learning.

Our research comes to life in Learn Your Way. The interface brings together multiple, personalized representations of content including: (1) immersive text, (2) section-level quizzes, (3) slides & narration, (4) audio lessons, and (5) mind maps.

The above representations give learners choice and are all adapted to their selected grade level and personal interests. Throughout the experience, the interactive quizzes provide dynamic feedback, guiding students to revisit specific content areas where they struggled. This marks our first steps towards true personalization.

The Learn You Way interface provides easy access to multiple representations and practice opportunities.

To evaluate Learn You Way's pedagogical performance, we transformed ten varied source materials from OpenStax (a provider of free educational textbooks) to three different personalization settings. The source materials covered various subjects from history to physics. Three pedagogical subject matter experts then evaluated the transformed materials using pedagogical criteria, such as accuracy, coverage, and the LearnLM learning science principles.

Top pedagogical principles that guide the development and evaluation of new learning capabilities and experiences at Google

The results were highly positive, with an average expert rating of 0.85 or higher across all pedagogical criteria. See the tech report for more evaluation details.

Expert ratings for the different transformations for four key criteria.

An AI-powered learning tool is only valuable if it both effectively improves learning outcomes and students want to use it. Learn Your Way now serves as a research platform for us to conduct studies with partners around the world to explore how AI-powered transformations and personalization affects outcomes, and to ensure that what we build is effective and locally relevant .

Recently, we conducted a randomized controlled study with 60 students from the Chicago area, ages 15–18 and with similar reading levels. Participants were given up to 40 minutes to learn about adolescent brain development from a textbook, and randomly assigned to learn using Learn Your Way or a traditional digital PDF reader.

We assessed students with a quiz immediately after the study session, and with a retention test 3–5 days later, using assessments designed by pedagogical experts to be a good measure of content comprehension. We also surveyed them about the learning experience, and to gain deeper insights beyond these quantitative metrics, each student participated in a 30-minute qualitative interview where they could share more nuanced feedback about their experience.

The results were compelling and statistically significant. Here are the highlights. See the tech report for more details.

The group using Learn Your Way scored 9% higher on average on an immediate assessment than the group using a digital reader

To give a concrete feel for the Learn Your Way interactive experience, today we are releasing example experiences on Google Labs , including:

Our findings suggest that generative AI can be used to build learning experiences that are not only more effective but also more empowering. By evolving the static textbook into an interactive artifact and giving students greater agency over how they learn, we saw learning retention improve.

This work is just the beginning of our exploration. We envision many more ways to tailor content, moving towards systems that continuously adapt to each learner's unique needs and progress. As we take our next steps towards personalized education, we will continue to ground our research in pedagogical principles, measuring the impact of AI on learning efficacy, so that in the future every student might have access to a high-quality, engaging learning experience that is custom built for them.

Shout out to our Google Research LearnLM team who have contributed to this work: Alicia Martín, Amir Globerson, Amy Wang, Anirudh Shekhawat, Anisha Choudhury, Anna Iurchenko, Avinatan Hassidim, Ayça Çakmakli, Ayelet Shasha Evron, Charlie Yang, Courtney Heldreth, Dana Oria, Diana Akrong, Hairong Mu, Ian Li, Ido Cohen, Komal Singh, Lev Borovoi, Lidan Hackmon, Lior Belinsky, Michael Fink, Preeti Singh, Rena Levitt, Shashank Agarwal, Shay Sharon, Sophie Allweis, Tracey Lee-Joe, Xiaohong Hao, Yael Gold-Zamir, Yishay Mor, and Yoav Bar Sinai. Special thanks to our executive champions: Niv Efron, Avinatan Hassidim, Yossi Matias and Ben Gomes.

November 18, 2025

November 7, 2025

October 31, 2025"
VaultGemma: The world's most capable differentially private LLM,https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/,"Amer Sinha, Software Engineer, and Ryan McKenna, Research Scientist, Google Research

We introduce VaultGemma, the most capable model trained from scratch with differential privacy.

As AI becomes more integrated into our lives, building it with privacy at its core is a critical frontier for the field. Differential privacy (DP) offers a mathematically sound solution by adding calibrated noise to prevent memorization. However, applying DP to LLMs introduces trade-offs. Understanding these trade-offs is crucial. Applying DP noise alters traditional scaling laws — rules describing performance dynamics — by reducing training stability (the model's ability to learn consistently without experiencing catastrophic events like loss spikes or divergence) and significantly increasing batch size (a collection of training examples sent to the model simultaneously for processing) and computation costs.

Our new research, “ Scaling Laws for Differentially Private Language Models ”, conducted in partnership with Google DeepMind, establishes laws that accurately model these intricacies, providing a complete picture of the compute-privacy-utility trade-offs. Guided by this research, we’re excited to introduce VaultGemma, the largest (1B-parameters), open model trained from scratch with differential privacy. We are releasing the weights on Hugging Face and Kaggle , alongside a technical report , to advance the development of the next generation of private AI.

With a carefully thought-out experimental methodology, we aimed to quantify the benefit of increasing model sizes, batch sizes, and iterations in the context of DP training. Our work required making some simplifying assumptions to overcome the exponential number of combinations one might consider trying. We assumed that how well the model learns depends mostly on the ""noise-batch ratio” which compares the amount of random noise we add for privacy to the size of the data groups (batches) we use for training. This assumption works because the privacy noise we add is much greater than any natural randomness that comes from sampling the data.

To establish a DP scaling law, we conducted a comprehensive set of experiments to evaluate performance across a variety of model sizes and noise-batch ratios. The resulting empirical data, together with known deterministic relationships between other variables, allows us to answer a variety of interesting scaling-laws–style queries, such as, “For a given compute budget, privacy budget, and data budget, what is the optimal training configuration to achieve the lowest possible training loss?”

The structure of our DP scaling laws. We establish that predicted loss can be accurately modeled using primarily the model size, iterations and the noise-batch ratio, simplifying the complex interactions between the compute, privacy, and data budgets.

Before diving into the full scaling laws, it’s useful to understand the dynamics and synergies between the compute budget, privacy budget, and data budget from a privacy accounting perspective — i.e., understand how these factors influence the noise-batch ratio for a fixed model size and number of iterations. This analysis is significantly cheaper to do as it does not require any model training, yet it yields a number of useful insights. For instance, increasing the privacy budget in isolation leads to diminishing returns, unless coupled with a corresponding increase in either the compute budget ( FLOPs ) or data budget (tokens).

Marginal benefit of increasing the privacy budget (epsilon) and the compute budget (batch size) in terms of their effect on the noise-batch ratio.

To explore this synergy further, the visualization below shows how the optimal training configuration changes based on different constraints. As the privacy and compute budgets change, notice how the recommendation shifts between investing in a larger model versus training with larger batch sizes or more iterations.

Predicted training loss for different settings of data/privacy/compute budget, and a further detailed breakdown by the number of iterations, batch size, and model size. The plots show both the minimum achievable loss for different budget settings, along with the optimal hyper-parameter configurations.

This data provides a wealth of useful insights for practitioners. While all the insights are reported in the paper, a key finding is that one should train a much smaller model with a much larger batch size than would be used without DP. This general insight should be unsurprising to a DP expert given the importance of large batch sizes. While this general insight holds across many settings, the optimal training configurations do change with the privacy and data budgets. Understanding the exact trade-off is crucial to ensure that both the compute and privacy budgets are used judiciously in real training scenarios. The above visualizations also reveal that there is often wiggle room in the training configurations — i.e., a range of model sizes might provide very similar utility if paired with the correct number of iterations and/or batch size.

The Gemma models are designed with responsibility and safety at their core. This makes them a natural foundation for developing a production-quality, DP-trained model like VaultGemma.

The scaling laws we derived above represent an important first step towards training a useful Gemma model with DP. We used the scaling laws to determine both how much compute we needed to train a compute-optimal 1B parameter Gemma 2-based model with DP, and how to allocate that compute among batch size, iterations, and sequence length to achieve the best utility.

One prominent gap between the research underlying the scaling laws and the actual training of VaultGemma was our handling of Poisson sampling , which is a central component of DP-SGD . We initially used a straightforward method of loading data in uniform batches but then switched to Poisson sampling to get the best privacy guarantees with the least amount of noise. This method posed two main challenges: it created batches of different sizes, and it required a specific, randomized order for processing the data. We solved this by using our recent work on Scalable DP-SGD , which allows us to process data in fixed-size batches — either by adding extra padding or trimming them — while still maintaining strong privacy protections.

Armed with our new scaling laws and advanced training algorithms, we built VaultGemma, to date the largest (1B-parameters) open model fully pre-trained with differential privacy with an approach that can yield high-utility models.

From training VaultGemma, we found our scaling laws to be highly accurate. The final training loss of VaultGemma was remarkably close to what our equations predicted, validating our research and providing the community with a reliable roadmap for future private model development.

Performance comparison of VaultGemma 1B (differentially private) against its non-private counterpart (Gemma3 1B) and an older baseline (GPT-2 1.5B). The results quantify the current resource investment required for privacy and demonstrate that modern DP training yields utility comparable to non-private models from roughly five years ago.

We also compare downstream performance of our model against its non-private counterpart across a range of standard academic benchmarks (i.e., HellaSwag , BoolQ , PIQA , SocialIQA , TriviaQA , ARC- C, ARC- E ). To put this performance in perspective and quantify the current resource investment required for privacy, we also include a comparison to an older similar-sized GPT-2 model, which performs similarly on these benchmarks. This comparison illustrates that today’s private training methods produce models with utility comparable to that of non-private models from roughly 5 years ago, highlighting the important gap our work will help the community systematically close.

Finally, the model comes with strong theoretical and empirical privacy protections.

In general, both the privacy parameters (ε, δ) and the privacy unit are important considerations when doing DP training, as these together determine what the trained model can learn. VaultGemma was trained with a sequence -level DP guarantee of (ε ≤ 2.0, δ ≤ 1.1e-10), where a sequence consists of 1024 consecutive tokens extracted from heterogeneous data sources. Specifically, we used the same training mixture that was used to train the Gemma 2 model, consisting of a number of documents of varying lengths. During pre-processing, long documents are split up and tokenized into multiple sequences, and shorter documents are packed together into a single sequence. While the sequence-level privacy unit was a natural choice for our training mixture, in situations where there is a clear mapping between data and users, user-level differential privacy would be a better choice.

What does this mean in practice? Informally speaking, because we provide protection at the sequence level, if information relating to any (potentially private) fact or inference occurs in a single sequence, then VaultGemma essentially does not know that fact: the response to any query will be statistically similar to the result from a model that never trained on the sequence in question. However, if many training sequences contain information relevant to a particular fact, then in general VaultGemma will be able to provide that information.

To complement our sequence-level DP guarantee, we conduct additional tests of the empirical privacy properties of the trained model. To do so, we prompted the model with a 50-token prefix from a training document to see if it would generate the corresponding 50-token suffix. VaultGemma 1B shows no detectable memorization of its training data and successfully demonstrates the efficacy of DP training.

VaultGemma represents a significant step forward in the journey toward building AI that is both powerful and private by design. By developing and applying a new, robust understanding of the scaling laws for DP, we have successfully trained and released the largest open, DP-trained language model to date.

While a utility gap still exists between DP-trained and non-DP–trained models, we believe this gap can be systematically narrowed with more research on mechanism design for DP training. We hope that VaultGemma and our accompanying research will empower the community to build the next generation of safe, responsible, and private AI for everyone.

We'd like to thank the entire Gemma and Google Privacy teams for their contributions and support throughout this project, in particular, Peter Kairouz, Brendan McMahan and Dan Ramage for feedback on the blog post, Mark Simborg and Kimberly Schwede for help with visualizations, and the teams at Google that helped with algorithm design, infrastructure implementation, and production maintenance. The following people directly contributed to the work presented here (ordered alphabetically): Borja Balle, Zachary Charles, Christopher A. Choquette-Choo, Lynn Chua, Prem Eruvbetine, Badih Ghazi, Steve He, Yangsibo Huang, Armand Joulin, George Kaissis, Pritish Kamath, Ravi Kumar, Daogao Liu, Ruibo Liu, Pasin Manurangsi, Thomas Mesnard, Andreas Terzis, Tris Warkentin, Da Yu, and Chiyuan Zhang.

November 18, 2025

November 13, 2025

November 12, 2025"
"Speculative cascades — A hybrid approach for smarter, faster LLM inference",https://research.google/blog/speculative-cascades-a-hybrid-approach-for-smarter-faster-llm-inference/,"Hari Narasimhan and Aditya Menon, Research Scientists, Google Research

We introduce “speculative cascades”, a new approach that improves LLM efficiency and computational costs by combining speculative decoding with standard cascades.

LLMs have transformed how we interact with technology, powering everything from advanced search capabilities to creative coding assistants. But this power comes at a cost: inference (the process of generating a response) can be slow and computationally expensive. As we deploy these models to more users, making them faster and less expensive without sacrificing quality is a critical challenge.

One way to accomplish this would be to use cascades , which aim to optimize LLM efficiency by strategically using smaller, faster models before engaging a larger, more expensive LLM. This approach involves a deferral rule where the smaller model decides if it can handle a query or if it needs to pass the task to a more capable, but costlier, large model. The goal is to process as much as possible cheaply and quickly, only incurring the high cost of the large LLM for complex tasks that truly require its advanced capabilities, potentially yielding favorable cost-quality trade-offs. Cascades prioritize computational cost reduction and efficient resource allocation, while allowing for some variability in quality.

Another approach, speculative decoding , optimizes an LLM’s latency and throughput without altering the final result . It achieves this by employing a smaller, faster ""drafter"" model to predict a sequence of future tokens. These speculated tokens are then quickly verified in parallel by the larger “target” model. If the draft is accepted, the large model effectively generates multiple tokens in a single step, greatly accelerating the process while guaranteeing that the final output is identical to what the large model would have produced on its own. This approach prioritizes speed and latency reduction, potentially at the cost of increased memory usage and less computational savings, since the larger model still performs substantial work.

In “ Faster Cascades via Speculative Decoding ”, we introduce “speculative cascades”, a new approach that combines the best of both cascades and speculative decoding. It delivers better LLM output quality at a lower computational cost than either technique alone by sometimes deferring to the smaller LLM for the sake of efficiency. We tested new speculative cascading techniques against standard cascading and speculative decoding baselines using Gemma and T5 models on various language tasks, including summarization, translation, reasoning, coding, and question answering. The results show that the proposed speculative cascades achieve better cost-quality trade-offs, often yielding higher speed-ups and better quality metrics compared to the baselines.

To fully understand and appreciate the speculative cascades approach, we first compare cascades and speculative decoding with a simple example. Imagine you ask an LLM a straightforward question:

Prompt: "" Who is Buzz Aldrin? ""

Let's say we have two models available to answer this: a small, fast ""drafter"" model and a large, powerful ""expert"" model.

Here's how they might respond:

Both models provide excellent, factually correct answers, but they interpret the user's intent slightly differently. The small model delivers a quick, factual summary, while the large model provides a more formal, encyclopedic-style entry. Depending on the user's need — be it a fast fact or a detailed overview — either response could be considered ideal. The key is that they represent two distinct, equally valid styles.

Now, let's see how the two main speed-up techniques handle this scenario.

With cascades, the small ""drafter"" model gets the prompt first. If it's confident in its answer, it replies. If not, it defers the entire task to the large ""expert"" model.

In our example:

This works! We get a great answer quickly. But the process is sequential. If the small model hadn't been confident, we would have wasted time waiting for it to finish, only to then start the large model from scratch. This sequential ""wait-and-see"" approach is a fundamental bottleneck.

With speculative decoding, the small model quickly drafts the first few tokens of the answer, and the large model verifies it in parallel, correcting the first mistake it finds.

In our example:

Even though the small model produced a good answer, the requirement to match the large model token-by-token forces a rejection. We lose the speed benefit and end up with an answer that is not necessarily superior. While the above example uses a simple token matching rejection rule, in the full paper, we also include the potential for a ""probabilistic match"" that provides greater flexibility in the token-by-token comparison.

The "" Buzz Aldrin "" example reveals a fundamental difference between these two techniques, as summarized below:

A visual representation of the trade-offs offered by standard cascades ( left ) and speculative decoding ( right ). In both graphs, the green star is the small, fast model (low cost, lower quality) and the red star is the large, slow model (high cost, higher quality). The dots in the left graph represent different trade-offs offered by cascades by varying its confidence threshold; the blue star in the right graph represents the trade-off offered by speculative decoding.

Speculative cascades combine the idea of tiered processing from standard cascades with the speedup mechanism of speculative decoding. It involves a smaller model generating a ""draft"" output that a larger model then quickly verifies in parallel. The key innovation is replacing the strict verification of speculative decoding with a flexible “deferral rule” . This rule dynamically decides, on a token-by-token basis, whether to accept the small model's draft or defer to the large model. This avoids the sequential bottleneck of standard cascades while allowing the system to accept a good answer from the small model even if it doesn't exactly match the large model's preferred output.

In our example:

The power of this method lies in its flexibility, as the deferral rule can be tailored to different needs.

For example, we could tell the system to defer based on:

This ability to plug in different decision-making logic is what gives speculative cascades their unique blend of speed, quality, and adaptability.

Block diagram illustrating a speculative cascade between a small and large model. As with standard speculative decoding, the drafting process involves auto-regressive sampling from the small drafter model. However, the verification process is different: it considers the combined output distribution of both the small and large models via a deferral rule, rather than solely relying on the large model's output.

Below, we visualize the behaviour of speculative cascading versus speculative decoding on a prompt from the GSM8K dataset . The prompt asks, “Mary has 30 sheep. She gets 1 kg of milk from half of them and 2 kg of milk from the other half every day. How much milk does she collect every day?“ By carefully leveraging the small model's output on certain tokens, speculative cascading can reach a correct solution faster than regular speculative decoding.

Comparison of speculative cascades and speculative decoding on a grade school math question from the GSM8K dataset . The draft tokens are shown in yellow and the verified tokens in red. The speculative cascades approach generates the correct answer, and does so faster than speculative decoding.

We tested speculative cascades on a range of benchmarks, including summarization, reasoning, and coding. The results show a clear advantage over speculative decoding. On a standard quality-versus-efficiency graph, speculative cascades consistently provide better trade-offs. This means for the same quality level as speculative decoding, our method is faster, i.e., generates more tokens per call to the larger model.

Speculative cascades variants (blue and orange) achieve better quality-latency trade-offs compared to standard speculative decoding (green star) on math reasoning and summarization tasks. See paper for details.

As LLMs become more integrated into daily applications, optimizing their performance isn’t just a technical goal, it’s a practical necessity. By rethinking how cascades and speculative decoding can work together, speculative cascades provide a more powerful and flexible tool for developers. This hybrid approach allows for fine-grained control over the cost-quality balance, paving the way for applications that are both smarter and faster.

This work is a collaborative effort with Wittawat Jitkrittum, Ankit Singh Rawat, Seungyeon Kim, Neha Gupta and Sanjiv Kumar. We are grateful to Ananda Theertha Suresh and Ziteng Sun for their insightful discussions, and Yale Cong, Mark Simborg, and Kimberly Schwede for their help in crafting this blog.

November 18, 2025

November 12, 2025

November 7, 2025"
Smarter nucleic acid design with NucleoBench and AdaBeam,https://research.google/blog/smarter-nucleic-acid-design-with-nucleobench-and-adabeam/,"Cory Y. McLean, Senior Staff Software Engineer, Google Research

We developed an open-source software benchmark for nucleic acid sequence design, and introduced a novel algorithm, AdaBeam, that outperforms existing algorithms on 11 the 16 tasks, demonstrating superior scaling properties on long sequences and large predictors.

Designing new DNA and RNA sequences with specific therapeutic properties is a critical challenge in modern medicine. These molecules are the building blocks for next-generation treatments, from more precise CRISPR gene therapies to more stable and effective mRNA vaccines . However, finding the right sequence is like searching for a single grain of sand on a vast beach. For instance, a small functional region of an RNA molecule called the 5' UTR can be one of over 2x10 120 possible sequences, making a brute-force search to optimize its function impossible.

What if we could use AI to navigate this vast search space, drastically cutting down the time and cost of drug discovery? While various efforts have made great strides in developing AI models that predict the properties of a given nucleic acid sequence, there remains opportunity to innovate on the algorithms that use these models to generate optimal sequences. A lack of standardized evaluation hinders progress and prevents us from translating powerful predictive models into the best possible therapeutic molecules.

To address this gap, in a research collaboration between Google Research and Move37 Labs , we introduce NucleoBench , the first large-scale, standardized benchmark for comparing nucleic acid design algorithms. By running over 400,000 experiments across 16 distinct biological challenges, we've created a framework to rigorously evaluate and understand how different algorithms perform. The insights from this work enabled us to develop AdaBeam , a hybrid design algorithm that outperforms existing methods on 11 of the 16 tasks and scales more effectively to the large and complex models that are defining the future of AI in biology. We have made AdaBeam and all of our algorithm implementations freely available to spur further innovation.

The process of designing a new nucleic acid sequence using computers generally follows four steps:

The typical workflow for computational nucleic acid design.

In this work we focus on the design algorithms of step 3. At present, different research groups use different algorithms and test them on different tasks, making it impossible to know which methods are truly the best. Most existing benchmarks rely on algorithms like simulated annealing or vanilla genetic algorithms , which were developed decades before modern deep learning and cannot take advantage of crucial information, like gradients, from the neural network models.

To create a comprehensive and fair benchmark, we selected a diverse slate of gradient-free and gradient-based design algorithms. Gradient-free algorithms include well-established methods like directed evolution and simulated annealing, which are inspired by processes in evolution and physics, respectively. These algorithms treat the predictive AI model as a ""black box"", and test new sequences without needing to understand how the model works internally. Their strength lies in their simplicity and broad applicability, but this means they potentially miss out on valuable clues from the model.

Gradient-based design algorithms leverage the internal workings of neural networks and include more modern algorithms like FastSeqProp and Ledidi . They use the model's gradients (i.e., the direction of steepest improvement) to intelligently guide the search for better sequences, but take longer to compute than just using the output of the neural network.

To our knowledge, NucleoBench is the most comprehensive benchmark for nucleic acid design algorithms thus far and allows for a fair, apples-to-apples comparison between algorithms. We evaluated 9 different algorithms on the same 16 tasks with the same starting sequences, giving us unprecedented statistical power to draw meaningful conclusions. These tasks span a wide range of biological challenges, including:

Summary of design tasks in NucleoBench. *Model input length is 200K base pairs (bp), but only 256 bp are edited.

We introduced ordered and unordered beam search algorithms, staples from computer science, to test how fixing the order of sequence edits compares to a more flexible, random-order approach. We also created Gradient Evo, a novel hybrid that enhances the directed evolution algorithm by using model gradients to guide its mutations to independently evaluate how important gradients were for edit location selection versus selecting a specific edit.

We also developed AdaBeam, a hybrid adaptive beam search algorithm that combines the most effective elements of unordered beam search with AdaLead , a top-performing, non-gradient design algorithm. Adaptive search algorithms don't typically explore randomly; instead, their behavior changes as a result of the search to focus their efforts on the most promising areas of the sequence space. AdaBeam’s hybrid approach maintains a ""beam"", or a collection of the best candidate sequences found so far, and greedily expands on particularly promising candidates until they’ve been sufficiently explored.

In practice, AdaBeam begins with a population of candidate sequences and their scores. In each round, it first selects a small group of the highest-scoring sequences to act as ""parents"". For each parent, AdaBeam generates a new set of ""child"" sequences by making a random number of random-but-guided mutations. It then follows a short, greedy exploration path, allowing the algorithm to quickly ""walk uphill"" in the fitness landscape. After sufficient exploration, all the newly generated children are pooled together, and the algorithm selects the absolute best ones to form the starting population for the next round, repeating the cycle. This process of adaptive selection and targeted mutation allows AdaBeam to efficiently focus on high-performing sequences.

Computer-assisted design tasks pose difficult engineering problems, owing to the incredibly large search space. These difficulties become more acute as we attempt to design longer sequences, such as mRNA sequences, and use modern, large neural networks to guide the design. AdaBeam is particularly efficient on long sequences by using fixed-compute probabilistic sampling instead of computations that scale with sequence length. To enable AdaBeam to work with large models, we reduce peak memory consumption during design by introducing a trick we call “gradient concatenation.” However, existing design algorithms that don’t have these features have difficulty scaling to long sequences and large models. Gradient-based algorithms are particularly affected. To facilitate a fair comparison, we limit the length of the designed sequences, even though AdaBeam can scale longer and larger. For example, even though the DNA expression prediction model Enformer runs on ~200K nucleotide sequences, we limit design to just 256 nucleotides.

Summary of design algorithms in NucleoBench. Below the solid line are design algorithms devised in this work.

We evaluate each design algorithm based on the final fitness score of the sequence each produced. The fitness score is defined as how well the sequence performed on the biological task according to the predictive model. To ensure fairness, we ran over 400,000 experiments where each design algorithm was given a fixed amount of time and the exact same 100 starting sequences for each task. We also measured the convergence speed, tracking how quickly each algorithm found its best solution, as faster algorithms save valuable time and computational resources.

We characterized performance variability by measuring how much an algorithm's final score was influenced by random chance versus its starting sequence. We quantified the effect of algorithmic randomness by re-running experiments with five different random seeds. To assess the impact of the starting point, we analyzed the variance in final scores across the 100 identical start sequences given to each design algorithm. We used a Friedman test to investigate whether ""intrinsically difficult start sequences"", or sequences that are hard for all algorithms to optimize, exist.

To assess the distribution of performance ranks, we compared the final performance for each of the nine algorithms across every experiment in the NucleoBench benchmark for each unique combination of a task and a starting sequence. A rank-based ""order score"" from 0 to 8 was then assigned, with 0 going to the best-performing algorithm, 1 to the second-best, and so on. Each violin shape is constructed by aggregating all the rank scores a single algorithm received across the 400,000+ experiments, with the width of the violin at any point showing how frequently that algorithm achieved a particular rank.

The distribution of final scores for each algorithm. X-axis is the design algorithm, y-axis is the aggregate order score. Order scores are determined by assigning an integer [0, 9] for each (task, start sequence, design algorithm) tuple according to the performances of all the final sequences for that (task, start sequence) pair. 0 is the top performer. Aggregate scores are computed by averaging over all such scores.

Gradient-based methods were the reigning champions amongst existing methods. However, we found that AdaBeam outperformed them, demonstrating that relying on gradients is not the only path to top-tier performance and scalability.

AdaBeam improves upon previous methods in several key ways:

Across the 16 tasks in NucleoBench, AdaBeam was the best-performing algorithm 11 times. It also proved to be one of the fastest to converge on a high-quality solution, demonstrating superior scaling properties that are essential for tackling the next generation of AI challenges in biology.

Our NucleoBench benchmark reveals the importance of rigorous, standardized evaluation and uncovers surprising findings, such as the critical impact of the initial sequence and the ineffectiveness of some established algorithm features. However, significant challenges remain. The best gradient-based methods still struggle to scale to the largest models and longest sequences, and substantial scalability gains can be realized through better software engineering. While our new algorithm, AdaBeam, sets a new state-of-the-art, future work must focus on algorithms that adhere to biological constraints and improve scalability.

A core principle of our work is a commitment to biosafety and responsible innovation. While AdaBeam represents a step forward for biological sequence design, it only improves the optimization according to a pre-existing predictive model. In other words, it is an optimizer, not an originator; the algorithm can only design sequences to maximize a goal defined by a user-provided predictive model. By releasing AdaBeam as an open-source tool, we empower researchers while ensuring the “human-in-the-loop” remains central to the design of biological molecules. Algorithms like AdaBeam can help scientists design more effective mRNA vaccines, create safer CRISPR gene therapies, and develop novel treatments for a wide range of diseases, bringing the promise of AI-driven drug discovery closer to reality.

This work represents a collaboration between Joel Shor (Move37 Labs), Erik Strand (Move37 Labs, MIT), and Cory Y. McLean (Google Research). We thank Sager Gosai, Daniel Friedman, Anna Lewis, Vikram Agarwal, and Michael Brenner for their guidance, discussions, and support throughout this project.

November 13, 2025

November 7, 2025

November 6, 2025"
Accelerating scientific discovery with AI-powered empirical software,https://research.google/blog/accelerating-scientific-discovery-with-ai-powered-empirical-software/,"Lizzie Dorfman, Product Manager, and Michael Brenner, Research Scientist, Google Research

Our new AI system helps scientists write empirical software, achieving expert-level results on six diverse, challenging problems.

In scientific research, thoroughly evaluating hypotheses is essential to developing more robust and comprehensive answers, but the required work forms a bottleneck, hindering the pace of discovery. In particular, much of modern scientific research depends on computational experiments to model, simulate, and analyze complex phenomena. Here, hypothesis evaluation often requires creating custom software, a slow and challenging task. Given the increasing capability of large language models (LLMs) to perform traditional coding tasks , we wondered if they could similarly generate high-quality custom software for evaluating and iteratively improving scientific hypotheses.

Today we are releasing a paper describing an "" AI system designed to help scientists write expert-level empirical software "", built using Gemini . Taking as input a well-defined problem and a means of evaluation, our system acts as a systematic code-optimizing research engine: it can propose novel methodological and architectural concepts, implement them as executable code and empirically validate their performance. It then searches and iterates through thousands of code variants, using tree search to optimize performance. We tested our system using six benchmarks representing distinct multidisciplinary challenges, spanning the fields of genomics, public health, geospatial analysis, neuroscience, time-series forecasting, and numerical analysis. Our system achieves expert-level performance across all of these benchmarks.

Scientific research is inherently iterative, often requiring researchers to test dozens or hundreds of models or parameters to achieve a breakthrough. Even for scientists who are experienced programmers, coding, debugging, and optimizing software is incredibly time-consuming. Manually coding each new idea is slow and inefficient, making systematic exploration of potential solutions practically impossible.

At the heart of our system lies the foundational concept of empirical software. Unlike conventional software, which is often judged by functional correctness alone, empirical software is designed with a primary objective: to maximize a predefined quality score. A problem or challenge that can be effectively addressed and solved through the application of empirical software is termed a scorable task. These scorable tasks are prevalent across science, applied mathematics, and engineering.

The input to our system is a scorable task, which includes a problem description, a scoring metric, and data suitable for training, validation, and evaluation. A user can also provide context, such as ideas from external literature, or directives for methodologies to prioritize.

The system then generates research ideas, including programmatic reproduction, optimization, and recombination of known methods, leading to novel and highly performant approaches. Ideas are implemented as executable code and the system uses a tree search strategy with an upper confidence bound (inspired by AlphaZero ) to create a tree of software candidates and decide which candidates warrant further exploration. It then uses an LLM to rewrite the code to attempt to improve its quality score, and can exhaustively and tirelessly carry out solution searches at an unprecedented scale, identifying high-quality solutions quickly, reducing exploration time from months to hours or days. Its outputs, as coded solutions, are verifiable, interpretable and reproducible.

Schematic of the algorithm that feeds a scorable task and research ideas to an LLM, which generates evaluation code in a sandbox. This code is then used in a tree search, where new nodes are created and iteratively improved using the LLM.

The evaluation of code generating AI systems has historically focused on tasks derived from competitive programming or software engineering, which, while valuable, fail to capture the full spectrum of challenges inherent in scientific discovery. We demonstrate proficiency not merely in writing syntactically correct code, but in generating novel solutions to six diverse and challenging benchmark problems that push the boundaries of current computational methods and human expertise. The diversity of these benchmarks allows us to collectively assess proficiency in areas such as zero-shot generalization , high-dimensional signal processing , uncertainty quantification , semantic interpretation of complex data, and systems-level modeling . The top scoring solutions to each of these benchmark problems are openly available for anyone interested in reproducing our results, including as an interactive website to explore the full candidate solution trees.

Single-cell RNA sequencing (scRNA-seq) is a powerful technology that provides a high-resolution view of gene expression at the individual cell level. A major challenge required to jointly analyze many disparate datasets is to remove complex batch effects present across samples while preserving true biological signals. Nearly 300 tools exist to perform batch integration of scRNA-seq data, and multiple benchmarks have been developed for assessing metrics of batch effect removal and conservation of biological variability. Using the OpenProblems V2.0.0 batch integration benchmark , which combines 13 metrics into one overall score, our system discovered 40 novel methods that outperformed top expert-developed methods. The highest-scoring solution achieved a 14% overall improvement over the best published method ( ComBat ) by successfully combining two existing methods (ComBat and BBKNN ).

Overall leaderboard for OpenProblems benchmark v2.0.0 non-control methods. In blue are results from our system with and without recombination of ideas, and Gemini Deep Research . Click to enlarge image.

The primary U.S. benchmark for COVID-19 forecasting is the COVID-19 Forecast Hub (CovidHub), a large collaborative effort coordinated by the Centers for Disease Control and Prevention (CDC). CovidHub attracts competitive and methodologically diverse submissions from dozens of expert-led teams. Their task is to forecast new COVID-19 hospitalizations across all the U.S. states and its territories for up to a month ahead. These forecasts are evaluated using average weighted interval score (WIS), which assesses the quality of probabilistic forecasts by summarizing a model's performance across all locations for every weekly prediction over the season. Individual submissions are then aggregated into the CovidHub Ensemble model , which is considered the gold standard in the U.S. for forecasting COVID-19 hospitalizations. Our system generated 14 models that outperform the official CovidHub Ensemble.

Time-series leaderboard showing weekly forecasting performance for teams participating in the COVID-19 Forecast Hub, ordered by absolute average WIS (number within each cell). Scores are aggregated across 52 jurisdictions and four forecast horizons. The cell’s background color visualizes the performance relative to the CovidHub-ensemble, with blue indicating a lower (better) WIS and red indicating a higher (worse) WIS. Our method, the top row of the table (Google Retrospective) outperforms CovidHub-ensemble. Click to enlarge image.

Semantic segmentation of high-resolution remote sensing images is a common problem in geospatial analysis, and is essential for diverse applications, ranging from monitoring land use , assessing the environmental impacts of human activity , and managing natural disasters . This task, which involves accurately assigning class labels to individual pixels in an image, requires a model to develop a spatial and contextual understanding of the scene, identifying not just what objects are present, but precisely where their boundaries lie.

Using the dense labeling remote sensing dataset (DLRSD) benchmark, which evaluates methods using a mean intersection over union (mIoU), the top three solutions generated by our system are slightly better than current state of the art, with mIoU greater than 0.80. All three solutions build upon existing models, libraries and strategies. Two leverage standard UNet++ and U-Net models but paired with powerful encoders pre-trained on ImageNet . The third uses SegFormer , a state of the art Transformer -based architecture. All three employ extensive test-time augmentation (TTA).

The input to remote sensing segmentation models is an image ( top row ), and the output is a new image, often called a segmentation mask, where each pixel is assigned a specific class label. The middle row is the true mask as provided by the DLRSD benchmark. The bottom row is segmentation masks generated using our system's top scoring solution. High-scoring segmentation models will have close visual similarity to the ground truth mask.

We applied our method to the Zebrafish Activity Prediction Benchmark (ZAPBench), a recent benchmark for forecasting the activity of over 70,000 neurons across an entire vertebrate brain. Our system discovered a novel time-series forecasting model that achieved state-of-the-art performance, surpassing all existing baselines. This includes a computationally intensive, video-based model that forecasts 3D volumes and was the previous top performing solution. As a proof of concept, we also demonstrated that our system can design hybrid models that incorporate a biophysical neuron simulator ( Jaxley ), paving the way for more interpretable predictive models.

While each of these examples is compelling in its own right, our system to generate empirical software is striking in its generalizability. We additionally evaluated our system in the context of mathematics on the task of numerical evaluation of difficult integrals. In this task, our system generated a solution that correctly evaluated 17 out of 19 held-out integrals , where the standard numerical method failed. Lastly, we evaluated our system on the general problem of time series forecasting, using the General Time Series Forecasting Model Evaluation (GIFT-Eval), a benchmark derived from 28 datasets spanning seven diverse domains, with 10 different frequencies, from seconds to years. Our system successfully created a unified, general purpose forecasting library from scratch, by hill climbing with a single code on the average mean absolute scaled error on the entire GIFT-Eval dataset. See the paper for more details.

Recent advances in LLMs have already given researchers worldwide new ways to easily engage with knowledge and ideas , and LLMs are increasingly being pursued as a means of automating the rote and toilsome aspects of scientific research. We explored whether LLMs could be useful for the ubiquitous, essential, and highly time-consuming task of producing custom software for evaluating and iteratively improving scientific hypotheses, motivated by the possibility of a future where scientists can easily, rapidly, and systematically investigate hundreds or thousands of potential solutions to the questions and problems that motivate their research. Our system quickly generates expert-level solutions reducing the time required for exploration of a set of ideas from months to hours or days. This promises to save significant time for scientists, from students to professors, to focus on truly creative and critical challenges, and to continue to define and prioritize the fundamental research questions and societal challenges that scientific research can help address.

We thank and acknowledge the contributions from all of the co-authors of the manuscript. Thanks to Shibl Mourad, John Platt, Erica Brand, Katherine Chou, Ronit Levavi Morad, Yossi Matias, and James Manyika for their support and leadership.

November 18, 2025

November 7, 2025

November 4, 2025"
How Google’s AI can help transform health professions education,https://research.google/blog/how-googles-ai-can-help-transform-health-professions-education/,"Mike Schaekermann, Research Lead, and Paul Jhun, Medical Education Lead, Google Research

We explore the utility of Google’s AI models as helpful tools in medical learning environments. By employing a learner-centered and evaluation-driven approach, we seek to reimagine the future of education for health professionals.

The global health workforce is facing a critical shortage, with projections indicating a deficit exceeding 11 million healthcare workers by 2030 . At Google, we are researching how AI can transform education for health professions to help close this gap with studies exploring how Google’s AI models can serve as effective personalized learning tools in medical learning environments.

Today we present two such studies. First, in “ Generative AI for medical education: Insights from a case study with medical students and an AI tutor for clinical reasoning ”, published at CHI 2025 , we took a qualitative approach to understanding and designing for medical learners through interdisciplinary co-design workshops, rapid prototyping, and user studies. Next, in our latest update of “ LearnLM: Improving Gemini for Learning ”, we quantitatively assessed LearnLM — our Gemini -based family of models fine-tuned for learning — on medical education scenarios through preference ratings from both medical students and physician educators. Both studies revealed a strong interest in AI tools that can adapt to learners and incorporate preceptor-like behaviors, such as providing constructive feedback and promoting critical thinking. Physician educators rated LearnLM as demonstrating better pedagogy and behaving “more like a very good human tutor” compared to base models. These novel capabilities are now available with Gemini 2.5 Pro .

Employing a learner-centered approach has been critical in guiding our development of responsible AI tools that scale individualized learner pathways and augment competency-based approaches. Central to this approach, we first conducted formative user experience (UX) research to understand medical learner needs. Through a participatory design process, we began with a co-design workshop that convened an interdisciplinary panel of medical students, clinicians, medical educators, UX designers, and AI researchers to define opportunities for incorporating AI in this space. Insights from this session guided the development of an AI tutor prototype, explicitly designed to guide learners through clinical reasoning anchored on a synthetic clinical vignette.

We then evaluated the AI tutor prototype’s helpfulness in a qualitative user study with eight participants (4 medical students and 4 residents). The study aimed to elicit participant learning needs and challenges as well as their attitudes toward AI assistance in education. Each participant engaged in a 1-hour session with a UX researcher involving semi-structured interviews and interactive sessions with the prototype. All sessions were remote and conducted through video conferencing software. Participants accessed the prototype through a web link and shared their screen while interacting with the prototype.

Our thematic analysis of medical learner interviews revealed various challenges to acquiring clinical reasoning skills and the potential for generative AI in addressing these challenges. For example, medical learners expressed a significant interest in AI tools capable of adapting to unique individual learning styles and knowledge gaps. Participants also highlighted the importance of preceptor-like behaviors, such as managing cognitive load, providing constructive feedback, and encouraging questions and reflection.

Overview of the participatory research process aimed at understanding and building for medical learners through an interdisciplinary co-design workshop, rapid research prototyping, and qualitative user studies.

Building on these insights, we conducted a blinded feasibility study with medical students and physician educators to quantitatively assess LearnLM's pedagogical qualities in medical education settings compared with Gemini 1.5 Pro as the base model. In collaboration with experts, we designed a set of 50 synthetic evaluation scenarios across a range of medical education subjects, from pre-clinical topics, such as platelet activation , to clinical topics, like neonatal jaundice , reflecting the core competencies and standards in medical education.

We recruited medical students from both preclinical and clinical phases of training to engage in interactive conversations with both LearnLM and the base model, in a randomized and blinded manner. Students used the evaluation scenarios to role-play as different types of learners across a range of learning goals and personas, generating 290 conversations for analysis. Each scenario provided learners with context to standardize the interaction as much as possible between both models, including a learning goal, grounding materials, a learner persona, a conversation plan, and the initial query used by the learner to start the conversation.

Example scenario used to evaluate LearnLM capabilities in the context of medical education settings.

Students then rated model behavior by comparing the two interactions for each scenario side-by-side across four criteria: (1) overall experience, (2) meeting learning needs, (3) enjoyability, and (4) understandability. Physician educators rated model behavior by reviewing conversation transcripts and scenario specifications. For each scenario, educators reviewed the transcripts from both learner-model conversations side-by-side, and provided preference ratings across five criteria: (1) demonstrating pedagogy, (2) behaving like a very good human tutor, (3) instruction following, (4) adapting to the learner, and (5) supporting the learning goal. We collected a median of three independent educator reviews per conversation pair. All preference ratings were done in a randomized and blinded manner using 7-point scales, which reflected a spectrum of preference strengths including the option to express no preference between the two models.

Physician educators consistently preferred LearnLM across all five of the comparison criteria. They judged LearnLM particularly positively in terms of demonstrating better pedagogy (on average, +6.1% on our rating scale) and for behaving “more like a very good human tutor” (+6.8%). When we simply look at whether educators expressed any preference one way or the other — regardless of its magnitude — LearnLM emerged as their choice in a clear majority of assessments across every criterion. Medical students indicated the strongest positive preference in terms of LearnLM being more enjoyable to interact with (on average, +9.9% on our rating scale). Student preferences were less pronounced for the other three comparison criteria, while directionally also favoring LearnLM.

This study points to LearnLM’s potential to transform education and learning paradigms and scale a competent health workforce. None of the data used for model development or evaluation in this study included real patient data. See the tech report for modeling details.

Preferences expressed by physician educators and medical students, showing the proportion of ratings that favored each model across medical education scenarios.

We recently shared this research at the MedEd on the Edge conference at the Nobel Forum and facilitated a hands-on workshop with the international medical education community to explore these possibilities. We recognize the dual role of educators as both pedagogical experts and explorers in this rapidly evolving knowledge domain. Realizing a responsible future requires careful attention to challenges such as ensuring accuracy, mitigating bias, and maintaining the crucial role of human interaction and oversight. It underscores the need to re-evaluate competencies and entrustable professional activities, and for curricula that cultivate adaptive expertise, focusing not only on AI applications in education, but also on teaching foundational understanding of AI itself. At this convergence, generative AI can serve as a catalyst for the desired productive struggle to foster deeper understanding and critical thinking. As the journey has only just begun, below are a few examples of how Google’s AI can potentially transform health professions education.

Link to Youtube Video

Examples of how educators and learners can use Google’s AI to reimagine education for health professions. LearnLM capabilities are now integrated and available with Gemini 2.5 Pro.

This research continues to lay the groundwork toward the effective design and implementation of personalized learning experiences, offering an opportunity to accelerate clinical competency and ultimately improve health outcomes by reimagining health professions education. We are committed to partnering with the health professions education community to thoughtfully and responsibly prepare future healthcare professionals to thrive in an AI-augmented healthcare landscape.

The research described here is a joint effort across Google Research, Google for Health, Google DeepMind, and partnering teams. The following researchers contributed to this work: Kevin McKee, Dan Gillick, Irina Jurenka, Markus Kunesch, Kaiz Alarakyia, Miriam Schneider, Jenn Sturgeon, Maggie Shiels, Amy Wang, Roma Ruparel, Anna Iurchenko, Mahvish Nagda, Julie Anne Séguin, Divya Pandya, Patricia Strachan, Renee Wong, Renee Schneider, Viknesh Sounderajah, Pete Clardy, Garth Graham, Megan Jones Bell, Michael Howell, Jonathan Krause, Christopher Semturs, Dale Webster, Avinatan Hassidim, Joëlle Barral, Ronit Levavi Morad and Yossi Matias. Special thanks to participants who contributed to these studies.

November 18, 2025

November 7, 2025

October 31, 2025"
A scalable framework for evaluating health language models,https://research.google/blog/a-scalable-framework-for-evaluating-health-language-models/,"Ahmed A. Metwally and Daniel McDuff, Staff Research Scientists, Google Research

Evaluation of language models in complex domains (such as health) can be expensive and labor intensive. We present a new adaptive and precise rubric methodology that saves time and increases inter-rater reliability compared to existing protocols.

Large language models can be used to analyze and interpret complex data. Our previous work has shown how they can be used to generate useful, personalized responses when provided with user-specific health information that encompasses lifestyle, biomarkers, and context. Rigorous and efficient evaluation methodologies are crucial to ensure the accuracy, precision, relevance, and safety of responses. However, current evaluation practices heavily rely on human experts, meaning they are cost-prohibitive, labor-intensive, and not scalable. Furthermore, tasks involving human judgement often require careful design to avoid biases and low inter-rater consistency.

With the above in mind, in “ A Scalable Framework for Evaluating Health Language Models ”, we introduce an evaluation framework that aims to streamline human and automated evaluation of open questions. Our method helps identify critical gaps in model responses using a minimal set of targeted rubric questions that break complex, multi-faceted evaluation questions into granular evaluation targets that can be answered via simple boolean responses. Specifically, we introduce Adaptive Precise Boolean rubrics as a paradigm for scalable health evaluations. We hypothesized that a small set of granular, boolean (Yes/No) criteria would enhance consistency and efficiency in complex query evaluation. Existing work has demonstrated that ""granularizing"" complex evaluation criteria into a larger set of focused, boolean rubrics improves rater reliability for general-domain tasks like summarization and dialogue. Our work extends these frameworks by applying them to the health domain, accounting for user personalization with health data in both the LLM responses and the evaluations. We validate this approach in metabolic health, a domain encompassing diabetes, cardiovascular disease, and obesity.

A set of representative health queries and wearable data are used to construct inputs to the language model, these are then evaluated using our proposed evaluation rubric framework.

We first used an iterative process to transform rubric criteria characterized by high-complexity response options (e.g., open-ended text or multi-point Likert scales) into a more granular set of rubric criteria employing binary response options (i.e., boolean “Yes” or “No”) — an approach we call Precise Boolean rubrics. The primary objective in developing the Precise Boolean rubrics was to enhance inter-rater reliability in annotation tasks and to generate a more robust and actionable evaluation signal, thereby facilitating programmatic interpretation and response refinement. The increased granularity afforded by the simple Yes/No format mitigates subjective interpretation and fosters more consistent evaluations, even with a larger number of total questions.

Due to the granular nature of our rubric design, the resulting Precise Boolean rubrics consisted of a substantially larger number of evaluation criteria compared to the starting Likert-scale rubrics. While auto-eval techniques are well equipped to handle the increased volume of evaluation criteria, the completion of the proposed Precise Boolean rubrics by human annotators was prohibitively resource intensive. To mitigate such burden, we refined the Precise Boolean approach to dynamically filter the extensive set of rubric questions, retaining only the most pertinent criteria, conditioned on the specific data being evaluated. This data-driven adaptation, referred to as the Adaptive Precise Boolean rubric, enabled a reduction in the number of evaluations required for each LLM response. This is because user queries and corresponding LLM outputs often exhibit a focused topicality, thus requiring evaluation against only the subset of rubric criteria relevant to those themes.

To convert the Precise Boolean rubrics to Adaptive Precise Boolean ones, we leveraged Gemini as a zero-shot rubric question classifier. Input to the LLM includes the user query, the corresponding LLM response under evaluation, and a specific rubric criterion. The LLM then outputs whether the criterion is relevant or not. To validate this adaptive approach, we established a ground-truth dataset through rubric question classification annotations provided by three medical experts, with majority voting employed to determine the consensus annotation. Rubrics obtained based on using this ground-truth dataset in order to do adaptation are referred to as Human-Adaptive Precise Boolean rubrics .

An example of a query and response highlighting references to specific relevant parts of the response, alongside examples of responses to evaluation rubric questions (Likert, Precise Boolean, and Adaptive Precise Boolean).

Current evaluation of LLMs in health often uses Likert scales. We compared this baseline to our data-driven Precise Boolean rubrics. Our results showed significantly higher inter-rater reliability using Precise Boolean rubrics, measured by intra-class correlation coefficients (ICC), compared to traditional Likert rubrics.

A key advantage of our approach is its efficiency. The Adaptive Precise Boolean rubrics resulted in high inter-rater agreement of the full Precise Boolean rubric while reducing evaluation time by over 50%. This efficiency gain makes our method faster than even Likert scale evaluations, enhancing the scalability of LLM assessment. The fact that this also provides higher inter-rater reliability supports the argument that this simpler scoring also provides a higher quality signal.

Left: Inter-rater correlation, as measured by intra-class correlation coefficient (ICC), between different subgroups — human evaluators (expert and non-expert) and automated evaluation. Right: Adaptive Precise Boolean rubrics take about half the time compared to likert scale questions.

To test the efficacy of our rubrics, we investigated their sensitivity to variations in response quality. We systematically augmented user queries with increasing levels of contextual health data, hypothesizing that richer queries would elicit higher-quality LLM responses, the results to support this will be discussed in detail below.

Average ratings from Likert scales showed limited sensitivity to these improvements in input context, particularly in automated evaluations. This suggests a lack of granularity in Likert scales for capturing subtle variations in response quality. In contrast, the average scores from our boolean rubrics showed a clear, positive correlation with the amount of user data provided, indicating a superior ability to measure incremental improvements in response quality.

Implications on Average Ratings: Ratings obtained from auto-evals using the boolean rubrics are more consistent/correlated with human ratings. In addition, replacing all questions with an adaptive set has little impact on the signal.

The Precise Boolean rubric framework is comprehensive, but for any given query, only a subset of its questions are relevant. We automated this filtering process by using Gemini as a zero-shot classifier to predict the relevance of individual rubric questions based on the input query and the LLM response. The classifier achieved an average accuracy of 0.77 and an F1 score of 0.83 in identifying relevant questions. We found that the Auto-Adaptive Boolean rubrics, using this automated filter, maintained an equivalent improvement in ICC and showed similar scoring trends as the Human-Adaptive Boolean rubrics. This suggests that an imperfect but effective automated classifier is sufficient to capture the essential evaluation signal. This finding is critical for building fully automated and scalable evaluation pipelines.

( A ) Adaptation of Precise Boolean rubrics using Gemini 1.5 Pro as a zero-shot rubric question classifier does not degrade ICC compared to using human driven adaptation. ( B ) Auto-Adaptive rubrics shows a similar average rating trend to Human-Adaptive rubrics, indicating that the Auto-Adaptive evaluation criteria are sufficient to capture the evaluation signals present based on human adaptation.

To demonstrate robustness, we evaluated our framework's ability to detect flaws in LLM responses generated from real research participants’ data. We used de-identified data from the Wearables for Metabolic Health (WEAR-ME) study , a large-scale (n≈1500) research project that collected wearable, biomarker, and questionnaire data conducted with approval from an Institutional Review Board (IRB). All participants provided electronic informed consent and a specific HIPAA Authorization via the Google Health Studies app before enrollment, acknowledging that their de-identified data would be used for research purposes.

Application of proposed approach on a real health study (WEAR-ME).

For this specific analysis, we selected 141 participants with confirmed metabolic conditions (e.g., Class III obesity, diabetes, hypercholesterolemia) to test the frameworks’ sensitivity. For each participant, we prompted an LLM to answer health queries under two conditions:

Illustration of our prompt ablation scheme.

We then used an automated evaluation system to score both the responses using both Likert and Precise Boolean rubrics. A higher positive discrepancy score (score of unaltered response minus score of altered response) indicates that the evaluation framework successfully detected the drop in quality.

As shown below, the Precise Boolean framework consistently produced a large, positive discrepancy score, indicating it reliably detected that the altered responses were of lower quality. In contrast, the Likert scale's discrepancy score was inconsistent and smaller in magnitude, failing to reliably flag the lower-quality responses. These results demonstrate that the Precise Boolean framework is significantly more sensitive to the inclusion of personal data, making it a more robust tool for automated evaluation pipelines.

Measuring the sensitivity of an auto-rater to prompt alterations using Likert rubrics and the proposed Precise Boolean rubrics.

Our findings show that using Adaptive Precise Boolean rubrics :

This approach offers a significant advancement in scaling and streamlining LLM evaluation in specialized domains. While LLMs hold promise for health applications, this paper focuses on the critical need for robust evaluation methodologies and does not present the models as approved medical devices.

Our framework is domain-agnostic and could be applied beyond health and personalized evaluation. The use of a health and wellness context for validation is for illustrative and research purposes only. This research is not tied to any specific product or service. The LLMs discussed are used in a controlled research setting and any real-world health application would be subject to its own validation and potential regulatory review. There are some limitations to this approach, in some situations the nuanced rating provided by Likert scale can be useful. Future work can expand on our results by incorporating a wider variety of user personas and health domains. Additionally, the process of creating the initial boolean questions from Likert criteria could be further automated by incorporating LLMs, enhancing the framework's scalability from its inception.

The following researchers contributed to this work: Neil Mallinar, A. Ali Heydari, Xin Liu, Anthony Z. Faranesh, Brent Winslow, Nova Hammerquist, Benjamin Graef, Cathy Speed*, Mark Malhotra, Shwetak Patel, Javier L. Prieto*, Daniel McDuff, and Ahmed A. Metwally.

* Work done while at Google.

November 18, 2025

November 7, 2025

November 6, 2025"
From massive models to mobile magic: The tech behind YouTube real-time generative AI effects,https://research.google/blog/from-massive-models-to-mobile-magic-the-tech-behind-youtube-real-time-generative-ai-effects/,"Andrey Vakunov, Software Engineer, Google Cloud, and Adam Svystun, Software Engineer, YouTube

We detail how YouTube delivers real-time generative AI effects on mobile devices by using knowledge distillation and on-device optimization with MediaPipe to overcome computational limitations while preserving user identity.

Effects are a huge part of the fun on YouTube Shorts , but for them to feel magical, they need to work in real-time in the camera as the creator is recording. This presents a challenge: how do we apply the latest capabilities of large generative AI models, such as cartoon style transfer, on creators' phones?

Our solution is a pipeline that distills the capability of a large model into a much smaller one focused on a single task. This narrowing of scope creates a compact, efficient model that can run directly on a phone, processing video frame-by-frame. Using this method, we've launched over 20 real-time effects for YouTube creators on Shorts. In this post, we'll detail how we accomplish this: including data curation, training, and the on-device setup.

Real-time transformation of video streams using a selection of real-time generative AI effects. From left to right: original, on-device makeup “ Pink dewy ”, ” Cartoon ” and a “ Toon ” effect.

The foundation of our work is high-quality data. We began by building a face dataset using properly licensed images. We meticulously filtered our datasets to ensure they were diverse and uniformly distributed across different genders, ages, and skin tones (as measured by the Monk Skin Tone Scale ) to build effects that work well for everyone.

Our approach revolves around a concept called knowledge distillation , which uses a ""teacher–student"" model training method. We start with a ""teacher"" — a large, powerful, pre-trained generative model that is an expert at creating the desired visual effect but is far too slow for real-time use. The type of teacher model varies depending on the goal. Initially, we used a custom-trained StyleGAN2 model, which was trained on our curated dataset for real-time facial effects. This model could be paired with tools like StyleCLIP , which allowed it to manipulate facial features based on text descriptions. This provided a strong foundation. As our project advanced, we transitioned to more sophisticated generative models like Google DeepMind’s Imagen . This strategic shift significantly enhanced our capabilities, enabling higher-fidelity and more diverse imagery, greater artistic control, and a broader range of styles for our on-device generative AI effects.

The ""student"" is the model that ultimately runs on the user’s device. It needs to be small, fast, and efficient. We designed a student model with a UNet -based architecture, which is excellent for image-to-image tasks. It uses a MobileNet backbone as its encoder, a design known for its performance on mobile devices, paired with a decoder that utilizes MobileNet blocks.

To achieve production-ready effects, we developed a robust training methodology that addresses the limitations of synthetic data distillation, which often leads to artifacts and reduced high-frequency details. Our approach leverages real-world data to generate ""image pairs"" and train student models to enable a more efficient hyperparameter search.

The distillation process for training the smaller student model involves two key steps:

High-level schema of distillation pipeline the “ Never Blink ” effect.

The ""editing"" of the image happens in ""latent"" space, which is a compressed numerical representation of the image where meaningful features are encoded. The process of converting raw pixels to latent representation is called “inversion”. A major challenge in image-to-image generative models for facial effects is preserving a person's identity because the effect regenerates the entire frame. A naïve approach can easily distort key features, changing a person's skin tone, glasses, or clothing, resulting in an output that no longer looks like them. This issue, often called the ""inversion problem"", happens when a model struggles to accurately represent a real person's face in its latent space.

To solve this, we employ a technique called pivotal tuning inversion (PTI). Here is a simplified version of how it works:

The pipeline fine-tunes a generator to the user's unique face, allowing us to apply edits in the latent space without losing their likeness in the final image. Note that the initial inversion may lack some fine details, resulting in a slightly different appearance.

Once the student model is trained, it needs to be integrated into a pipeline that can run efficiently on a phone. We built our on-device solution using MediaPipe , our open-source framework for building cross-platform multimodal ML pipelines, from Google AI Edge . The final inference pipeline works as follows:

On-device inference pipeline: MediaPipe Face Mesh detects, crops, and aligns faces for the student model.

These experiences need to run at a minimum of 30 frames per second to feel responsive to the user, so the pipeline must execute faster than 33 milliseconds per frame. The model inference latencies are ~6 ms for Pixel 8 Pro on Google Tensor G3 and 10.6 ms for iPhone 13 GPU. We invested heavily in optimizing these pipelines for a wide range of mobile devices, leveraging GPU acceleration to ensure a smooth experience for everyone.

This technology has been a crucial element of YouTube Shorts since 2023, enabling the successful launch of numerous popular features, including expression-based effects (e.g., Never blink ), Halloween-themed masks (e.g., Risen zombie ), and immersive full-frame effects (e.g., Toon 2 ). These significantly expanded creative possibilities for YouTube video creators.

Real-time generative AI effects in action on YouTube Shorts, including expression-based effects like “ Always smile ” ( left ) and "" Never blink "" ( middle ) and Halloween-themed masks like "" Risen zombie "" ( right ).

By bridging the gap between massive generative models and the constraints of mobile hardware, we are defining what is technically possible for real-time, on-device generative effects. This is just the beginning; we are actively working on integrating our newest models, like Veo 3 , and significantly reducing latency for entry-level devices, further democratizing access to cutting-edge generative AI in YouTube Shorts.

We would like to thank our co-authors and collaborators: Sarah Xu, Maciej Pęśko, Paweł Andruszkiewicz, Jacob Rockwell, Ronny Votel, Robert (Guohui) Wang, Tingbo Hou, Karthik Raveendran, Jianing Wei, Matthias Grundmann, Omer Tov, Ariel Ephrat, Shiran Zada, and Inbar Mosseri.

November 18, 2025

November 7, 2025

October 31, 2025"
Securing private data at scale with differentially private partition selection,https://research.google/blog/securing-private-data-at-scale-with-differentially-private-partition-selection/,"Justin Y Chen, Student Researcher, and Morteza Zadimoghaddam, Research Scientist, Google Research

We present novel algorithms to preserve user privacy in data releases, improving the state of the art in differentially private partition selection.

Large, user-based datasets are invaluable for advancing AI and machine learning models. They drive innovation that directly benefits users through improved services, more accurate predictions, and personalized experiences. Collaborating on and sharing such datasets can accelerate research, foster new applications, and contribute to the broader scientific community. However, leveraging these powerful datasets also comes with potential data privacy risks.

The process of identifying a specific, meaningful subset of unique items that can be shared safely from a vast collection based on how frequently or prominently they appear across many individual contributions (like finding all the common words used across a huge set of documents) is called “differentially private (DP) partition selection”. By applying differential privacy protections in partition selection, it’s possible to perform that selection in a way that prevents anyone from knowing whether any single individual's data contributed a specific item to the final list. This is done by adding controlled noise and only selecting items that are sufficiently common even after that noise is included, ensuring individual privacy. DP is the first step in many important data science and machine learning tasks, including extracting vocabulary (or n -grams) from a large private corpus (a necessary step of many textual analysis and language modeling applications), analyzing data streams in a privacy preserving way, obtaining histograms over user data, and increasing efficiency in private model fine-tuning.

In the context of massive datasets like user queries, a parallel algorithm is crucial. Instead of processing data one piece at a time (like a sequential algorithm would), a parallel algorithm breaks the problem down into many smaller parts that can be computed simultaneously across multiple processors or machines. This practice isn't just for optimization; it's a fundamental necessity when dealing with the scale of modern data. Parallelization allows the processing of vast amounts of information all at once, enabling researchers to handle datasets with hundreds of billions of items. With this, it’s possible to achieve robust privacy guarantees without sacrificing the utility derived from large datasets.

In our recent publication, “ Scalable Private Partition Selection via Adaptive Weighting ”, which appeared at ICML2025 , we introduce an efficient parallel algorithm that makes it possible to apply DP partition selection to various data releases. Our algorithm provides the best results across the board among parallel algorithms and scales to datasets with hundreds of billions of items, up to three orders of magnitude larger than those analyzed by prior sequential algorithms. To encourage collaboration and innovation by the research community, we are open-sourcing DP partition selection on GitHub .

The goal of DP partition selection is to maximize the number of unique items selected from a union of sets of data, while strictly preserving user-level DP. This means that very popular items, belonging to many users, can often be safely preserved for downstream computational tasks, whereas items belonging to only a single user would not be included. The algorithm designer must aim for an optimal privacy-utility trade-off in selecting items from the dataset while respecting the differential privacy requirement.

The conventional approach to differentially private partition selection involves three core steps:

The weight, noise, and filter paradigm. In all plots, the x-axis represents items (A–F) and the y-axis represents the weight assigned to the items. The algorithm first computes a weight histogram over items ( left ), adds noise ( center ), and returns items with noisy weight above a threshold ( right ).

A limitation of the standard, non-adaptive approach is potential ""wastage"". Highly popular items might receive significantly more weight than necessary to cross the privacy threshold, effectively ""over-allocating"" weight. This excess weight could have been more effectively used to boost items that are just below the threshold, thereby increasing the overall number of items released and improving the utility of the output.

We introduce adaptivity into the weight assignment process to address this. Unlike non-adaptive methods where each user's contribution is independent, an adaptive design allows the weight contributed by a user to an item to consider contributions from other users. This is a delicate balance, as it must be achieved without compromising privacy or computational efficiency.

Our novel algorithm, MaxAdaptiveDegree (MAD), strategically reallocates weight. It identifies items with significant ""excess weight"" (far above the threshold) and reroutes some of that weight to ""under-allocated"" items (those just below the threshold). This adaptive reallocation ensures that more less-frequent items can cross the privacy threshold and be included in the output. Moreover, MAD maintains both the same low-sensitivity bounds and efficiency as the baseline, meaning it offers the same strong privacy guarantees and scalability in parallel processing frameworks (like MapReduce -like systems), but with strictly superior utility.

Furthermore, we extend this concept to multi-round DP partition selection frameworks. We demonstrate how to safely release intermediate noisy weight vectors between rounds. This additional information allows for even greater adaptivity, as we can reduce future weight allocations to items that previously received too much weight (and were likely to be over-allocated again) or too little weight (and were unlikely to ever cross the threshold). This further refines the weight distribution, maximizing utility without sacrificing privacy, and further increases the items in output.

We conducted extensive experiments comparing our MAD algorithm with one or multiple iterations against scalable baselines for private partition selection.

As shown in the table below, MAD with just two iterations (column MAD2R) achieves state-of-the-art results across many datasets — often outputting significantly more items than other methods (even those using more rounds) while retaining the same privacy guarantees.

Comparison of the number of items returned by our algorithms: Two-round (MAD2R) and other baselines (“Basic”, which represents the uniformly weighted algorithm, and DP-SIPS ) on nine publicly-available datasets. By leveraging new techniques for adaptivity, our two-round algorithm achieves state-of-art results across many datasets.

In our paper , we present theoretical results that suggest our single-round algorithm (MAD) should always outperform the single-round Gaussian weighting baseline mentioned above. Our results demonstrate that this theoretical hypothesis appears to be correct. The excellent performance of our new methods holds across a wide selection of privacy parameters and hyperparameter choices. An example in-memory implementation of our algorithm in Python is available in the open-source repo .

On the large-scale publicly-available Common Crawl dataset (comprising close to 800 billion entries), we obtained record-level DP by treating entries as “users” and the words in these entries as items. On this dataset, our two-iteration MAD algorithm output a set of items that covered 99.9% of the entries (each of which has at least one item in the output) and 97% of the database entries (corresponding to an item that is in the output) while satisfying the DP guarantee.

With just two iterations, our algorithm achieved state-of-the-art results in a wide range of parameter settings. As expected from our theoretical results, our algorithm always outperformed the baseline.

The number of items our two-iteration MAD algorithm returns by frequency count (the number of entries that have that item) on the Common Crawl dataset. While MAD cannot return low frequency items that would violate privacy, it outputs the vast majority of items that belong to many entries.

We introduce new methods to improve the utility-privacy trade-off in DP partition selection algorithms. Our algorithm achieves state-of-the-art results on datasets approaching a trillion entries. We hope this algorithm will help practitioners achieve higher utility across their workflows while strictly respecting user privacy.

We thank Alessandro Epasto, Vincent Cohen-Addad, part of the Algorithm and Optimization team in Google Research , who contributed to this work.

November 21, 2025

November 19, 2025

November 13, 2025"
Beyond billion-parameter burdens: Unlocking data synthesis with a conditional generator,https://research.google/blog/beyond-billion-parameter-burdens-unlocking-data-synthesis-with-a-conditional-generator/,"Shanshan Wu, Software Engineer, Google Research

We present a novel privacy-preserving synthetic data generation algorithm that enables automatic topic-wise distribution matching, making it accessible even for resource-constrained AI applications.

Generating large-scale differentially private (DP) synthetic data is challenging due to the fundamental privacy–computation–utility trade-off, where strong privacy guarantees can either hurt the quality of the synthetic data, or require large amounts of computation. A popular solution is to privately fine-tune a billion-size large language model (LLM) on the “private data” (a standard term referring to the dataset on which one plans to offer privacy guarantees) and then sample from the fine-tuned model to generate synthetic data. This approach is computationally expensive and hence unattainable for resource-constrained applications . So, recently proposed Aug-PE and Pre-Text algorithms have explored generating synthetic data that only requires LLM API access. However, they usually depend heavily on manual prompts to generate the initial dataset and are ineffective in using private information in their iterative data selection process.

In “ Synthesizing Privacy-Preserving Text Data via Fine-Tuning Without Fine-Tuning Billion-Scale LLMs ”, presented at ICML 2025 , we propose CTCL (Data Synthesis with ConTrollability and CLustering), a novel framework for generating privacy-preserving synthetic data without fine-tuning billion-scale LLMs or domain-specific prompt engineering. CTCL uses a lightweight 140 million parameter model, making it practical for resource-constrained applications . By conditioning on the topic information, the generated synthetic data can match the distribution of topics from the private domain. Finally, unlike the Aug-PE algorithm, CTCL allows generating unlimited synthetic data samples without paying additional privacy costs. We evaluated CTCL across diverse datasets, demonstrating that it consistently outperforms baselines, particularly under strong privacy guarantees. Ablation studies confirmed the crucial impact of its pre-training and keyword-based conditioning, while experiments also showed CTCL's improved scalability compared to the Aug-PE algorithm.

The CTCL Framework is designed to generate high-quality synthetic data from private datasets while preserving privacy. It achieves this by breaking down the process into three main steps. Before we dive into the details, it's essential to understand the two core components that make this framework work: CTCL-Topic and CTCL-Generator. CTCL-Topic is a universal topic model that captures the high-level themes of a dataset, while CTCL-Generator is a powerful language model that can create documents based on specific keywords. These two components, developed using large public corpora, are the foundation for learning different private domains and generating synthetic data from them.

Both components are developed only once using large-scale public corpora and can then be used later for learning different private domains. CTCL-Topic is a topic model extracted from Wikipedia , a diverse corpus containing around 6 million documents. We follow BERTopic to embed each document, cluster them into around 1K clusters (i.e., 1K topics), and represent each cluster by 10 keywords.

CTCL-Generator is a lightweight (140M-parameter) conditional language model that accepts free-form document descriptions as inputs (e.g., document type, keywords, etc.) and generates documents satisfying the input conditions. To construct the pre-training data, for each document in SlimPajama , we prompt Gemma-2-2B to “Describe the document in multiple aspects.” The result is a dataset comprising 430M description–document pairs. We then use this dataset to perform continual pre-training on top of BART-base (a 140M-parameter language model), yielding the CTCL-Generator.

Step 1: A universal topic model CTCL-Topic and a lightweight 140M-parameter CTCL-Generator with strong controllability are developed using large-scale public corpora.

We then use CTCL-Topic to capture the high-level distributional information from the entire private corpus. This is done by collecting a private histogram representing the topic-wise distribution of the private data, i.e., the percentage of each topic in the private data. This topic histogram will be used later in Step 3 for sampling.

While collecting the topic histogram, each document in the private dataset has been associated with a topic. We then transform the private dataset into a dataset of keywords and document pairs, the 10 keywords for each document are obtained from their corresponding topic in CTCL-Topic. We then fine-tune the CTCL-Generator with DP on this dataset.

Step 2: To learn the private domain, we collect a DP topic histogram from the private data, and fine-tune the CTCL-Generator with DP on the private data.

The DP fine-tuned CTCL-Generator is sampled proportionally for each topic according to the DP topic histogram. Specifically, given the desired size of the synthetic dataset (say, N ) and the DP topic histogram (say, x % for Topic 1, y % for Topic 2, etc.), we know the number of target samples for each topic (i.e., x%*N for Topic 1, y %* N for Topic 2, etc.). For each topic, we use the corresponding 10 keywords as input to the DP fine-tuned CTCL-Generator to generate data. An arbitrary amount of synthetic data can be generated by CTCL-Generator without paying additional privacy costs, following the post-processing property of DP .

Step 3: Privacy-preserving synthetic data is generated based on the DP topic histogram and the DP fine-tuned CTCL-Generator.

We conducted experiments on four datasets, where three datasets correspond with downstream generative tasks and one dataset with a classification task. Generative tasks are typically more challenging than classification tasks. This is because the generative tasks are evaluated by the next-token prediction accuracy, which requires the synthetic data to preserve fine-grained textual information from the private data. In contrast, the classification tasks only require maintaining the co-occurrence patterns between labels and words in the private data.

The three generative tasks are chosen to cover a diverse set of practical scenarios: PubMed (medical paper abstracts), Chatbot Arena (human-to-machine interactions), and Multi-Session Chat (human-to-human daily dialogues). To evaluate the quality of the generated synthetic data, we followed the setup of Aug-PE to train a small downstream language model on the synthetic data and then compute the next-token prediction accuracy on the real test data.

The classification task is performed on the OpenReview (academic paper reviews) dataset. To evaluate the quality of the generated synthetic data, we train a downstream classifier on the synthetic data, and compute the classification accuracy on the real test data.

To mitigate concerns regarding data contamination, we carefully analyzed our selected datasets. Our analysis showed no overlap between our pre-training data and the downstream datasets.

CTCL consistently outperforms the other baselines, especially in the strong privacy guarantee regime. The plot below compares CTCL and the following baseline algorithms: Downstream DPFT (i.e., directly DP fine-tuning downstream model on the private data without using synthetic data), Aug-PE (an augmented version of the Private Evolution algorithm), DP fine-tuning an LLM of similar size to CTCL to generate synthetic data , with post-generation resampling . The plot below illustrates CTCL's improved performance, particularly for the more challenging setting that satisfies a stronger privacy guarantee (i.e., smaller ε value). This demonstrates CTCL’s strong ability to effectively capture useful information from the private data while maintaining privacy.

CTCL demonstrates improved performance over other baselines across the four datasets, especially in the challenging regime with stronger privacy guarantees (as shown by smaller ε).

Also, compared to Aug-PE , CTCL has better scalability in terms of both the privacy budget and synthetic data size. As shown by the left plot below, CTCL improves with an increased privacy budget while Aug-PE does not. This limitation may stem from Aug-PE’s constrained capacity (i.e., only via the nearest neighbors) to effectively capture information in the private data. The right plot shows that accuracy increases as the downstream model is given access to more CTCL-generated samples, while the performance of Aug-PE saturates around 10K examples. These results align with the intuition that fine-tuning–based methods (e.g., CTCL) can better capture fine-grained statistics than prompting-based methods (e.g., Aug-PE ).

CTCL has better scalability than Aug-PE in terms of privacy budget ( left ) and continues to improve as the downstream tasks are trained on more synthetic data ( right ).

And finally, ablation studies validate the importance of two key components in our framework: 1) pre-training the CTCL-Generator on public corpus, and 2) incorporating keyword-based conditions during DP fine-tuning. Specifically, starting from the standard DP fine-tuning, we sequentially introduce these components and measure the downstream model’s test loss. For a fixed privacy budget, our results show that incorporating keywords during DP fine-tuning reduces the test loss by 50%, and adding pre-training gives another 50% reduction. This demonstrates that both components are crucial in our framework design.

Our experiments synthesizing data with ConTrollability and CLustering (CTCL) uses a generator of only 140M parameters. But the key idea of CTCL, i.e., using clustering information or LLM extracted metadata as input instructions, can be easily extended to larger size models. We are actively working on exploring this idea to help improve real-world applications.

This work was primarily done by Bowen Tan during his internship at Google Research, under the guidance of Shanshan Wu and Zheng Xu. We thank Daniel Ramage and Brendan McMahan for leadership support, external academic partners Eric Xing and Zhiting Hu for helpful feedback on the ICML paper, Zachary Garrett and Michael Riley for reviewing an early draft, Taylor Montgomery for reviewing the dataset usage, Mark Simborg and Kimberly Schwede for help editing the blogpost and graphics. We are grateful to the ICML reviewers for their valuable time and insightful comments on our paper.

November 18, 2025

November 12, 2025

November 7, 2025"
Enabling physician-centered oversight for AMIE,https://research.google/blog/enabling-physician-centered-oversight-for-amie/,"David Stutz, Research Scientist, Google DeepMind, and Natalie Harris, Software Engineer, Google Research

We introduce guardrailed-AMIE (g-AMIE), a diagnostic AI designed for history-taking. g-AMIE operates with a guardrail that prohibits it from giving individualized medical advice, instead generating a summary for an overseeing physician to review.

Recent work demonstrated that Articulate Medical Intelligence Explorer (AMIE), our research AI system for medical reasoning and diagnostic dialogue, can provide accurate medical advice in text-based simulations of patient visits. However, individual patient diagnoses and treatment plans are regulated activities and must be reviewed and approved by licensed medical professionals prior to any patient communication. Simultaneously, oversight is an established paradigm in the medical domain allowing autonomy for care team members while overseeing primary care physicians (PCPs) retain accountability for the care of the patient. Inspired by this, our current research explores a framework for physician oversight of AMIE.

In “ Towards physician-centered oversight of conversational diagnostic AI ”, we introduce an extension of our AMIE research system, guardrailed-AMIE (g-AMIE), with a multi-agent setup based on Gemini 2.0 Flash . g-AMIE can gather patient information (i.e., history taking ) via a dialogue and generate a body of information for a clinician to review. This comprises a summary of information gathered, a proposed differential diagnosis and management plan, and a draft message to the patient. We design g-AMIE with guardrail constraints that prevent it from sharing any individualized medical advice, i.e., any diagnoses or management plan tailored to the patient. This information is reviewed and can be edited by an overseeing PCP through a purpose-built web interface called the clinician cockpit . Decoupling history taking from medical decision-making allows the overseeing PCP to review cases asynchronously. In a randomized, blinded, virtual objective structured clinical examination (OSCE), we compared g-AMIE’s performance with nurse practitioners (NPs), physicians assistants/associates (PAs), and PCPs operating under the same guardrail constraints. We found that g-AMIE’s diagnostic performance and management plans were preferred by overseeing PCPs and independent physician raters. Additionally, g-AMIE’s patient messages were preferred by patient actors. While this represents an important milestone towards human–AI collaboration with AMIE, results need to be interpreted with care, especially when making comparisons to clinicians. The workflow was designed for the unique characteristics of AI systems, whereas clinicians haven’t been trained to operate within this framework.

Asynchronous oversight framework. 1 . g-AMIE as well as NP/PA and PCP control groups perform history taking within guardrails, abstaining from individualized medical advice. 2 . g-AMIE and control groups generate differential diagnoses (DDx) and management plans. 3 . Overseeing physician revises DDx & management plan to ensure patient safety and accountability. 4 . Overseeing PCP shares a revised message with the patient. “g-PCP” and “g-NP/PA” refer to providers operating under the same guardrail constraints as g-AMIE.

To enable physician oversight, g-AMIE produces a detailed medical note that is then reviewed by the overseeing PCP using our clinician cockpit interface, which we developed in a co-design study with 10 outpatient physicians. The co-design was conducted through semi-structured interviews with potential users and thematic analysis to identify crucial components before results were shared with a UI designer to draft the interface. The cockpit is based on the widely-used SOAP note format which includes subsections for Subjective (the patient’s perspective on their condition), Objective (observable and measurable patient data, such as vital signs or lab data), Assessment (differential diagnosis with justification), and Plan (management strategy).

Our clinician-cockpit, inspired by the SOAP note format.

For g-AMIE to respect its guardrails during history taking and generate high-quality, accurate SOAP notes, we developed a multi-agent system consisting of a dialogue agent, a guardrail agent, and a SOAP note agent. The dialogue agent aims to perform high-quality history taking in three phases: (1) general history taking, (2) targeted validation of an initial differential diagnosis, and (3) a conclusion phase that addresses patient questions. The guardrail agent verifies that each response from the dialogue agent avoids any individualized medical advice, rephrasing responses as needed. The SOAP note agent performs sequential multi-step generation, separating the summarization tasks (Subjective and Objective) from the inferential tasks (Assessment and Plan) and from the patient message generation.

A. g-AMIE performs history taking using a three-phase dialogue agent, including general history taking, validating its differential diagnosis using targeted questions, and allowing the patient to ask questions; each response is verified using a guardrail agent to ensure that g-AMIE does not provide individualized medical advice. B. For medical note generation, g-AMIE follows a sequential multi-step approach to generate all sections of the SOAP note format along with a patient message. Click to enlarge image .

We conducted a virtual OSCE study to contextualize g-AMIE’s performance across 60 case scenarios developed by an OSCE laboratory. The study included patient actors, g-AMIE and two control groups: PCPs with 5 years or less of independent practice experience and a group of NP/PAs. Both groups were instructed to follow the same guardrails as g-AMIE and to also draft SOAP notes and patient messages following their history taking. We refer to these control groups as guardrail PCPs and guardrail NP/PAs. In a second step, a group of overseering PCPs, with at least 5 years experience including experience supervising healthcare team members, reviewed SOAP notes, made edits, and either approved the (edited) patient message or decided that a follow-up consultation would be needed. Finally, independent physicians rated the overall quality of the transcript, SOAP notes and the overseeing PCP’s decision. Patient actors, overseeing PCPs and independent raters were blinded as to whether history taking had been performed by g-AMIE or a control group. We extended evaluation rubrics from previous work by evaluating SOAP note quality using a modified QNote rubric and adding oversight-specific rubrics.

Our virtual OSCE study with oversight follows three steps: A. Patient actors interact with g-AMIE or a control group during history taking. SOAP notes are generated or obtained through a questionnaire after history taking. B. Transcripts and SOAP notes are displayed in our clinician cockpit for overseeing PCPs to make edits and to decide on whether or not to approve the (edited) patient message. C. Independent experts rate the quality of the consultations, including oversight edits and decisions, using various evaluation rubrics. Click to enlarge image .

In our virtual OSCE study, g-AMIE outperforms groups of PCPs, NPs and PAs operating within the same guardrailed workflow (g-PCPs, g-NP/PAs).

We observed that g-AMIE successfully performs history taking within guardrails, with no consultation being rated as definitely containing individualized medical advice. Moreover, g-AMIE’s ability to elicit key information from the patient was rated higher compared to the control groups and g-AMIE’s SOAP notes are rated to be more complete, accurate, and readable. Overseeing PCPs preferred overseeing g-AMIE and were more likely to accept patient messages crafted by g-AMIE.

A. History taking safety and quality as assessed by independent physicians showing that g-AMIE follows guardrails and performs high-quality history taking. B. Completeness and accuracy of generated SOAP notes and patient messages. C. Oversight actions and experience as rated by the overseeing PCP showing that g-AMIE’s patient messages are accepted in the vast majority of cases and that overseeing PCPs prefer the experience with g-AMIE over either control groups. The asterisks represent statistical significance (*: p<0.05, **: p<0.01, ***: p<0.01, n.s.: not significant). Click to enlarge image.

We found that g-AMIE’s history taking and medical notes lead to differential diagnoses and management plans that are rated to be more appropriate. Moreover, raters indicated that g-AMIE enables overseeing PCPs to make an appropriate decision for patient follow-up more often, with medical notes that are more often rated as sufficient for downstream care.

A. g-AMIE produces DDx and management plans that are more appropriate compared to control groups. B. Evaluation of composite performance showing g-AMIE leads to more appropriate decisions by the overseeing PCP (o-PCP) and produces SOAP notes and patient messages that are more often rated as sufficient for downstream patient care. The asterisks represent statistical significance (*: p<0.05, **: p<0.01, ***: p<0.01, n.s.: not significant). Click to enlarge image.

While g-AMIE is able to follow guardrails in the vast majority of the cases, there are caveats and nuances in classifying individualized medical advice. Our results are based on a single rating per case even though we observed significant disagreement among raters in previous studies. Moreover, the comparison to both control groups should not be taken as commentary on their ability to follow the supplied guardrails; PCPs in particular are not used to withholding medical advice in consultations. Considerable further development of AI oversight paradigms in real-world settings is required to ensure generalisation of our proposed framework.

While g-AMIE’s SOAP notes included confabulations in a few cases, we found that such confabulations occur at a similar rate as misremembering by both guardrail PCPs and guardrail NP/PAs. It is noteworthy, however, that g-AMIE’s notes are considerably more verbose, which leads to longer oversight times and a higher rate of edits focused on reducing verbosity. In interviews with overseeing PCPs, we also found that oversight is mentally demanding, which is consistent with prior work on cognitive load of AI-assisted decision support systems.

On the other hand, during history taking, we believe this verbosity contributes to g-AMIE’s higher ratings for how information is explained and rapport is built. Patient actors and independent physicians preferred g-AMIE’s patient messages and its demonstration of patient empathy. These findings highlight that future work aimed at finding the right trade-off in terms of verbosity between history taking, medical notes and patient messages is required.

We also found that NPs and PAs consistently outperform PCPs in history taking quality, following guardrails and diagnostic quality. However, these differences should not be extrapolated to meaningful indicators of relative performance in the real world. The tested workflow was designed to explore a paradigm of AI oversight and both control groups are provided primarily to contextualize g-AMIE’s performance. None received specific training for this workflow, and it does not account for several real-world professional needs. Therefore, it would likely significantly underestimate clinicians’ capabilities. Moreover, the recruited NPs and PAs had more experience and may be more familiar with patient-focused history-taking. PCPs, in contrast, are taught to explicitly link history-taking to the diagnostic process, linking questions to direct hypothesis testing, and the proposed workflow would likely have significantly impacted their consultation performance.

Finally, patient actors cannot act as an exact substitute for real patients, especially in combination with our 60 constructed scenario packs. While these cover a range of conditions and demographics, they are not representative of real clinical practice.

We introduce a paradigm for asynchronous oversight of conversational diagnostic AI systems such as AMIE. Preserving conversational properties, AMIE can operate within guardrails, performing history taking without providing individualized medical advice. The latter, including diagnosis and management planning, is deferred to an overseeing physician. This disentangles history-taking from decision making, ensuring patient safety with the overseeing physician remaining accountable. In a virtual, randomized OSCE study, we show that our system, termed guardrailed-AMIE, can perform high-quality history taking, medical note generation and leads to better overall diagnostic decisions compared to PCPs, NPs, and PAs operating under the same guardrails. Our results should not be interpreted to mean that g-AMIE is superior to clinicians, who have not been trained in this workflow. Nevertheless, our work marks a significant step towards a framework for responsible and scalable use of conversational diagnostic AI systems in healthcare.

The research described here is joint work across many teams at Google Research and Google DeepMind. We are grateful to all our co-authors: Elahe Vedadi, David Barrett, Natalie Harris, Ellery Wulczyn, Shashir Reddy, Roma Ruparel, Mike Schaekermann, Tim Strother, Ryutaro Tanno, Yash Sharma, Jihyeon Lee, Cian Hughes, Dylan Slack, Anil Palepu, Jan Freyberg, Khaled Saab, Valentin Liévin, Wei-Hung Weng, Tao Tu, Yun Liu, Nenad Tomasev, Kavita Kulkarni, S. Sara Mahdavi, Kelvin Guu, Joelle Barral, Dale R. Webster, James Manyika, Avinatan Hassidim, Katherine Chou, Yossi Matias, Pushmeet Kohli, Adam Rodman, Vivek Natarajan, Alan Karthikesalingam, and David Stutz.

November 18, 2025

November 7, 2025

October 31, 2025"
"Achieving 10,000x training data reduction with high-fidelity labels",https://research.google/blog/achieving-10000x-training-data-reduction-with-high-fidelity-labels/,"Markus Krause, Engineering Manager, and Nancy Chang, Research Scientist, Google Ads

A new active learning method for curating high-quality data that reduces training data requirements for fine-tuning LLMs by orders of magnitude.

Classifying unsafe ad content has proven an enticing problem space for leveraging large language models (LLMs). The inherent complexity involved in identifying policy-violating content demands solutions capable of deep contextual and cultural understanding, areas of relative strength for LLMs over traditional machine learning systems. But fine-tuning LLMs for such complex tasks requires high-fidelity training data that is difficult and expensive to curate at the necessary quality and scale. Standard data-intensive approaches to training models are costly, especially given the need to handle concept drift as safety policies evolve or as new types of unsafe ad content arise. In the worst case the model must be retrained on a completely new data set. Reducing the amount of training data needed is therefore paramount.

With this in mind, we describe a new, scalable curation process for active learning that can drastically reduce the amount of training data needed for fine-tuning LLMs while significantly improving model alignment with human experts. The process can be applied to datasets of hundreds of billions of examples to iteratively identify the examples for which annotation would be most valuable and then use the resulting expert labels for fine-tuning.

In our experiments, we were able to reduce the scale of training data needed from 100,000 to under 500 training examples, while increasing model alignment with human experts by up to 65%. Production systems using larger models have seen even greater reductions in data scale, using up to four orders of magnitude less data while maintaining or improving quality.

Our process starts with a zero- or few-shot initial model (LLM-0), which we provide with a prompt describing the content of interest, e.g., defining clickbait and asking “Is this ad clickbait?” The LLM-0 model then labels ads as clickbait (orange in the figure below) or benign (blue) and generates a large labeled data set, shown as (1) below. Note that this initial data set is typically highly imbalanced, since in production traffic only very few (<1%) ads are actually clickbait. The LLM’s true positive rate is also low because it has not yet been fine-tuned. To find the most informative examples, we separately cluster examples labeled clickbait and examples labeled benign, which yields some overlapping clusters, thus indicating potential model confusion between clickbait and benign examples (2) . For each such overlapping cluster pair, we find pairs of examples lying nearest each other that have different labels (3) and send these to human experts for an opinion. If needed to stay within our review budget, we prioritize pairs of examples that cover a larger area of our search space (4) . The resulting curated set is both informative (since it contains the most confusable examples along the decision boundary) and diverse (since it draws from different regions along that decision boundary).

The curation process generates preliminary labels using a few-shot LLM and then clusters each label set. Overlapping clusters with differing labels are used to identify sampled pairs of examples that are both informative and diverse.

These expert-provided labels are split randomly into two sets. The first is used for model evaluation, based on two key alignment metrics: the internal alignment measuring how much experts agree, and the model–human alignment between the current model and human experts. The second is used to fine-tune the current models, producing the next iteration of the model. The process repeats until the model–human alignment either matches the internal alignment or plateaus and cannot be improved further.

Our curation process does not assume the existence of ground truth. Many classification problems in the ads safety space, such as content moderation or fraud detection, are inherently ambiguous and require interpretation and deliberation, even among policy experts. We therefore cannot rely on standard metrics like precision and recall, which require a ground truth label. Instead we use Cohen’s Kappa , a measure of how well two independent annotators align, above what would be expected from chance agreement. In our experiments, Cohen’s Kappa is used as both a quality indicator for datasets (including model evaluation during the curation process, as noted above); and as a measure of model performance. Values closer to 1 show higher alignment, 0 indicates no alignment above chance, and negative values indicate systematic disagreement. While standards for interpreting these scores vary, Kappa values above .8 are widely considered to be exceptionally good, and values above .4 are generally considered acceptable.

We wanted to understand which models and tasks would benefit most from our curation process. As baselines for our experiments, we fine-tuned two LLMs of different sizes ( Gemini Nano-1 with 1.8B parameters and Nano-2 with 3.25B parameters) on two tasks of different complexity (lower and higher, based on expert alignment) using crowdsourced labels. Each crowdsourced data set has ~100K annotations and a strong class imbalance, with around 95% benign labels on average.

We compared each of these four baseline conditions against the corresponding curated condition in which each model (Nano-1 and Nano-2) is fine-tuned over multiple rounds using the curation process described above. At each iteration, we selected our curated set of examples and used them for model evaluation and fine-tuning, as described above. All models plateaued before reaching parity with the experts’ internal alignment, so we stopped at 6 iterations (~400 fine-tuning and ~250 evaluation samples) for the lower complexity task and 5 iterations (~250 fine-tuning and ~150 evaluation samples) for the higher complexity task. (Note that the lower complexity task had a larger variety of examples, which may account for the longer time needed to converge.) Both data sets had a final class balance of ~40% positive examples.

The table below provides an overview of the scale and quality of the data used in each condition. Experts reached an average pairwise Cohen’s Kappa of .81 (on the lower complexity task) and .78 (on the higher complexity task) through the curation process. We consider these the ceiling for model performance. To assess the quality of our crowdsourced data, we calculated Kappa alignment between crowdsourced annotations and experts based on our full curated set, which was .59 (lower complexity) and .41 (higher complexity).

Size and quality of datasets used for our baseline conditions (using crowdsourced data) and curated conditions (using data from human experts). Dataset numbers for the expert curated sets show the cumulative number of samples collected during the curation process for both fine-tuning and model evaluation; the full curated set also served as the evaluation dataset for the crowdsourced data. Quality of the evaluation datasets is measured in pairwise Cohen’s Kappa.

Below we show how models trained on these vastly different data sets performed in each of our baseline and curated conditions. The 1.8B parameter model saw comparable performance on both tasks: the baseline and curated models had .24 and .25 alignment, respectively, for the lower complexity task, and both models had .13 alignment on the higher complexity task. By contrast, the 3.25B parameter model showed significant quality improvements when trained with our curation process. Kappa scores for the baseline and curated models were .36 and .56, respectively, for the lower complexity task; and .23 and .38, respectively, for the higher complexity task — an improvement in alignment of 55-65% using three orders of magnitude less data (250 to 450 examples, compared to 100K in the baseline condition).

Performance of models trained in curated and baseline conditions, measured as alignment between domain experts and model responses using pairwise Cohen’s Kappa.

Our curation method uses only 250 (for the higher complexity task) and 450 (for the lower complexity task) training samples rated by pairs of human experts ( .78 and .81 average pairwise Cohen’s Kappa). The baseline models use 100K crowdsourced training samples (~5% positive).

These results demonstrate that careful curation of LLM datasets to focus on fewer, more informative examples can yield better or equivalent classifier performance using much less data — three orders of magnitude less in the experiments reported here, and up to four orders of magnitude less for the larger models used in production. Of course, these gains require not only good curation but also very high quality data. For our use cases, we have observed that a label quality above .8 pairwise Cohen’s Kappa is needed to reliably outperform crowdsourced data. Consistently achieving this level of quality poses a separate challenge, to be discussed in a subsequent blog post.

But given sufficient label quality, our curation process is able to leverage the strengths of both LLMs, which can cast a wide net over the problem space, and domain experts, who can focus more efficiently on the most challenging examples. The ability to retrain models with just a handful of examples is especially valuable for handling the rapidly changing landscapes of domains like ads safety. We believe the approach we’ve described will enable systems that can make more flexible, efficient use of high-fidelity labels to escape the data bottleneck.

This work would not have been possible without our outstanding team of engineers and product managers. Steve Walker is a co-founder of our project and co-creator of the curation process as well as the tech lead for the machine learning infrastructure of our project. Kelsie McElroy is the product manager and a co-founder of our project. We also want to thank the Ads Privacy and Safety leadership team for their continued support and belief in our vision.

November 18, 2025

November 12, 2025

November 7, 2025"
Insulin resistance prediction from wearables and routine blood biomarkers,https://research.google/blog/insulin-resistance-prediction-from-wearables-and-routine-blood-biomarkers/,"Ahmed A. Metwally, Staff Research Scientist, and A. Ali Heydari, Research Scientist, Google Research

Leveraging wearable data and routine blood tests, we propose a novel method for effectively predicting insulin resistance, providing a scalable and accessible approach for early type 2 diabetes risk screening.

Type 2 diabetes affects hundreds of millions globally , and its prevalence is rising. A major precursor to this condition is insulin resistance (IR), where the body's cells do not respond properly to insulin, a hormone crucial for regulating blood sugar. Detecting IR early is key, as lifestyle changes can often reverse it and prevent or delay the onset of type 2 diabetes. However, current methods for accurately measuring IR, like the ""gold standard"" euglycemic insulin clamp or the Homeostatic Model Assessment for Insulin Resistance (HOMA-IR), which requires specific insulin blood tests, are often invasive, expensive, or not readily available in routine check-ups . These steps create significant barriers to early detection and intervention, especially for those unknowingly at risk.

What if we could leverage data already available to many people, such as data from wearable devices and common blood tests, to estimate IR risk? In “ Insulin Resistance Prediction From Wearables and Routine Blood Biomarkers ”, we explore a suite of machine learning models that have the potential of predicting IR using wearable data (e.g., resting heart rate, step count, sleep patterns) and routine blood tests (e.g., fasting glucose , lipid panel ). This approach shows strong performance across the studied population (N=1,165) and an independent validation cohort (N=72), particularly in high-risk individuals, such as people with obesity and sedentary lifestyles. Additionally, we introduce the Insulin Resistance Literacy and Understanding Agent (an IR prototype agent), built on the state-of-the-art Gemini family of LLMs to help understand insulin resistance, facilitating interpretation and safe personalized recommendations. This work offers the potential for early detection of people at risk of type 2 diabetes and thereby facilitates earlier implementation of preventative strategies. The models, predictions, and the Insulin Resistance Literacy and Understanding Agent described in this research are intended for informational and research purposes only.

Metabolic subphenotypes of type 2 diabetes . Chronic insulin resistance is a precursor to approximately 70% of type 2 diabetes cases and results from a combination of obesity, an inactive lifestyle, and genetic factors.

We designed a study, called WEAR-ME , to explore the potential of predicting insulin resistance (through predicting HOMA-IR) using readily accessible data. To automate the data collection process for routine blood biomarkers, we partnered with Quest Diagnostics .

1,165 remote participants from across the US signed up for the WEAR-ME study via the Google Health Studies app , a secure consumer-facing platform for digital studies. This study was conducted with approval from an Institutional Review Board (IRB). All participants provided electronic informed consent and a specific HIPAA Authorization via the Google Health Studies app before enrollment. The cohort was diverse in age, gender, geography, and BMI. Participants had a median BMI of 28 kg/m², age of 45 years, and HbA1c of 5.4%. Participants consented to share the following data:

Using this rich, multimodal dataset (which we refer to as the “WEAR-ME data”), we developed and trained deep neural network models to predict HOMA-IR scores. Our goal was to see how well we could estimate this key IR marker using different combinations of available data.

Illustration of our proposed modeling pipeline for predicting HOMA-IR, and interpreting the results with the Insulin Resistance Education and Understanding Agent.

Our results, using the area under the receiver operating characteristic curve (auROC) metric, indicate that combining data streams significantly improved prediction accuracy compared to using any single source alone:

Wearables + Demographics + Routine Blood Panels : Achieved the best results, accurately predicting HOMA-IR values ( R² = 0.50) and effectively classifying individuals with IR (auROC = 0.80, Sensitivity = 76%, Specificity = 84%, where HOMA-IR value of 2.9 or higher was used to identify a person as being insulin resistant).

Left: Performance evaluation of IR prediction (classification). Right: Visualization of the precision-recall curve for selected feature sets. Average values are colors, with the gray areas around each line indicate the standard deviation across the five folds.

Importantly, our results indicate that features derived from wearable data, such as resting heart rate, consistently ranked among the most important predictors, alongside BMI and fasting glucose. The feature importance result highlights the value of capturing lifestyle-related signals.

Sankey diagram showing the relative feature importance ( SHapley Additive exPlanations [SHAP] values ) for each of the proposed nonlinear XGBoost models for direct regression.

Since individuals with obesity and sedentary lifestyles are particularly vulnerable to developing type 2 diabetes, we specifically evaluated our model's performance in these subgroups:

The results of this experiment suggest that our approach could be particularly effective at identifying those who might benefit most from early lifestyle interventions.

Results of classification performance for various lifestyle stratification.

To ensure our findings were not just specific to our initial dataset, we tested our best-performing model (trained on the WEAR-ME data) on a completely independent validation cohort (N=72) recruited through a separate IRB-approved consented study, where participants shared wearable data using the Fitbit Charge 6 , and blood biomarker data was acquired in-person at the study center in San Francisco. This cohort had a median BMI of 30.6 kg/m² and age of 44.5 years. Our results on the validation cohort show that our trained models maintained strong predictive performance (sensitivity = 84%, specificity = 81%), demonstrating its potential generalizability. However, as this remains a research prototype, its safety and effectiveness for any health-related purpose have not been established.

Overview of the independent validation cohort study. We compare model accuracies from the initial training and testing cohort with the external validation cohort and demonstrate its potential generalizability .

Illustration of the proposed agentic architecture that leverages the HOMA-IR prediction model to assess insulin resistance risk to educate users.

Predicting IR risk is valuable, but how can we make this information understandable and actionable for individuals? We explored integrating our prediction models with LLMs to empower users to better understand their metabolic health. We developed the Insulin Resistance Literacy and Understanding Agent (an IR prototype agent), built on the state-of-the-art Gemini family of LLMs. When asked a question about metabolic health, the IR Agent provides personalized, contextualized answers for educational purposes grounded in the individual's study data and predicted IR status. With the user's consent, the agent has the ability to access specific, user-provided data points, search for up-to-date information, and perform calculations. It is critical to note that interaction with the models or the IR Agent are intended to demonstrate how such a tool could help users explore their results for informational and educational purposes.

We had five board-certified endocrinologists evaluate responses from the IR Agent compared to a base model. They strongly preferred the IR Agent's responses, finding them to be significantly more comprehensive, trustworthy, and personalized. This demonstrates the potential of combining predictive health models with LLMs to empower individuals with better health understanding.

Overview of Insulin Resistance Literacy and Understanding Agent (IR Agent). An illustration of the proposed IR agent ( left ), along with the results (win rate) of our IR agent against the base model as evaluated by endocrinologists ( right ).

Our research demonstrates that ML models combining readily available wearable data and routine blood biomarkers have the potential to effectively predict insulin resistance, a key precursor to type 2 diabetes. This approach offers several advantages:

This work opens doors for earlier, more accessible screening of type 2 diabetes risk, potentially enabling timely lifestyle interventions that could prevent or delay the disease, particularly for those unknowingly progressing towards it.

Future work includes validating these models longitudinally (tracking individuals over time), exploring the impact of interventions, incorporating genetic and microbiome data, and further refining models for specific populations to ensure equitable performance across diverse groups. We believe this line of research holds significant promise for proactive and personalized metabolic health management.

While our proposed approach, including the IR Agent, holds promise for various health applications, this research specifically addresses the critical need for early detection of insulin resistance, and does not present the models discussed herein as approved medical devices or solutions. The models and the IR Agent are not medical devices. They have not been cleared, approved, or reviewed by the U.S. Food and Drug Administration (FDA) or any other national or international regulatory agency. This work is not intended to be, and should not be used as, a substitute for professional medical advice, diagnosis, or treatment. Real-world deployment of such technologies would necessitate rigorous testing, validation, and regulatory approval.

The research described here is joint work across Google Research and partnering teams. The following researchers contributed to this work: Ahmed A. Metwally, A. Ali Heydari, Daniel McDuff, Alexandru Solot, Zeinab Esmaeilpour, Anthony Z. Faranesh, Menglian Zhou, David B. Savage, Conor Heneghan, Shwetak Patel, Cathy Speed, and Javier L. Prieto. Google partnered with Quest Diagnostics , the world’s leading provider of diagnostic information, to allow eligible participants to share their biomarker data received as part of a free blood draw, which includes a comprehensive metabolic panel and measuring cholesterol, triglycerides and insulin levels.

November 18, 2025

November 7, 2025

November 6, 2025"
Highly accurate genome polishing with DeepPolisher: Enhancing the foundation of genomic research,https://research.google/blog/highly-accurate-genome-polishing-with-deeppolisher-enhancing-the-foundation-of-genomic-research/,"Kishwar Shafin, Technical Lead, and Andrew Carroll, Product Lead, Google Research

DeepPolisher, is a new deep learning tool that significantly improves the accuracy of genome assemblies by precisely correcting base-level errors, which recently played a key role in enhancing the Human Pangenome Reference.

The key to understanding heredity, disease, and evolution lies in the genome, which is encoded in nucleotides (i.e., the bases A, T, G, and C). DNA sequencers can read these nucleotides, but doing so both accurately and at scale is challenging, due to the very small scale of the base pairs. However, to unlock the secrets hidden within the genome, we must be able to assemble a reference genome as close to perfect as possible.

Errors in assembly can limit the methods used to identify genes and proteins , and can cause later diagnostic processes to miss disease-causing variants. In genome assembly, the same genome is sequenced many times, allowing iterative correction of errors. Still, with the human genome being 3 billion nucleotides, even a small error rate can mean a large total number of errors and can limit the derived genome’s utility.

In an effort to continually improve the resources for genome assembly, we introduce DeepPolisher , an open-source method for genome assembly that we developed in a collaboration with the UC Santa Cruz Genomics Institute . In our recent paper, “ Highly accurate assembly polishing with DeepPolisher ”, published in Genome Research , we describe how this pipeline extends existing methods to improve the accuracy of the genome assembly. DeepPolisher reduces the number of errors in the assembly by 50% and the number of insertion or deletion (“indel”) errors by 70%. This is especially important since indel errors interfere with the identification of genes.

While there are several ways to measure DNA, most typically involve capturing the process of copying DNA. One method for this involves attaching label molecules with different colors to separate building block nucleotides and observing the process of each being added to the DNA molecule being copied. The DNA copying machinery always copies the strand in a particular orientation, so although the information is redundantly encoded on both strands, only nucleotides from one strand are read at a time. Identifying the nucleotides requires detectors that are able to resolve single molecules, which limits the accuracy of measurements.

One breakthrough technology to scale this method, developed by Illumina , copies one molecule of the DNA to be sequenced into a cluster of identical copies. It then monitors as the cluster copies in sync, thus increasing the signal for each base. However, as it is impossible to ensure the cluster copies in perfect unison, the cluster may desynchronize so that the signal of different bases blend together, which limits the lengths of the DNA measured using this method to a few hundred nucleotides.

Although these sequences (called “reads”) are short, they are still useful for analysis. By comparing them to a reference genome, i.e., an existing map of the genome of the species to be sequenced, it is possible to map many of the short reads to that reference, thus building up a more complete genome of the sampled individual. This can then be compared to the reference to better understand how the subject’s genome varies.

The human genome is composed of two strands that redundantly encode information ( left ), organized into chromosomes, with one full copy inherited from each parent ( right ). ( Images from NHGRI )

Even with improved sequencing technology, there remain several challenges. First, the method relies on having a robust reference genome, which is itself exceptionally difficult to create. Even with such a reference, some parts of the genome look more like other parts, making them difficult to confidently map to the reference.

To address those challenges, scientists developed processes that could sequence individual molecules, enabling reads of tens of thousands of nucleotides. Initially, this process had unacceptable error rates (~10%). This was addressed when Pacific Biosciences developed a way to sequence the same molecule in multiple passes, reducing the error rate to only 1%, similar to the short-read methods. Google and Pacific Biosciences worked together on the first demonstration of this on a human genome .

Our team then took this further by developing DeepConsensus, which uses a sequence transformer to more accurately construct the correct sequence from the initial error-prone bases. Today Pacific Biosciences deploys DeepConsensus on their long-read sequencers to reduce the error rate to less than 0.1%. While this error rate is markedly better than the prior state of the art, reaching the accuracy required to construct a new, nearly perfect reference genome, requires combining sequence reads from multiple DNA molecules of the same individual to further correct remaining errors.

This is where DeepPolisher comes in. Adapted from DeepConsensus, DeepPolisher uses a Transformer architecture trained on the genome from a human cell line donated to the Personal Genomes Project . This reference genome has been exhaustively characterized by NIST and NHGRI and sequenced using many different technologies. It is estimated to be ~100% complete with a correctness of 99.99999%. This corresponds to around 300–1000 total errors across the 6 billion nucleotides in the genome (two copies of the 3 billion nucleotide reference inherited from each parent).

By conducting PacBio sequencing and genome assembly, we can identify remaining errors and then train models to learn to correct them. For training, the model takes the sequenced bases, their quality, and how uniquely they map to a given part of the reference assembly. During training, we use only chromosomes 1–19. We hold out chromosomes 20–22, using the performance on chromosomes 21 and 22 to select a model, and we report accuracies using chromosome 20.

Architecture of DeepPolisher. The sequence reads are categorized by parental origin (called “phasing”) and are aligned to the draft genome assembly. The input channels are: the base information, reported quality by the sequencer, the quality of the mapping (ability to place the reads uniquely on the assembly), and annotations of mismatched bases. This is sent to an encoder-only Transformer, which classifies the errors in the assembly and then suggests a fix, which is used to correct the assembly.

DeepPolisher reduces errors in a genome assembly by approximately half, an improvement largely driven by the reduction in insertion–deletion (“indel”) errors, which decrease by more than 70 percent. Reducing these types of errors is especially important, because inserted or deleted bases can shift the reading frame of a gene, causing annotation programs to overlook that gene when labelling the genome and hiding it from reports in clinical analysis or drug discovery.

We quantify the quality of a genome using a “ Q-score ”, which is a base-10 logarithm of the probability that a position in the genome has an error. A Q30 score means 99.9% chance of being correct, while a Q60 means a 99.9999% chance of a base being correct. To assess the improvement of DeepPolisher, we pulled sequencing data being used to assemble new genomes for the Human Pangenome Reference Consortium (HPRC). We looked for potential errors in the assembly by trying to identify combinations of nucleotides in the assembly that don’t occur in other sequencing of the same sample with different sequencing technologies. By doing this analysis in the parts of the genome for which the other sequencing method has no systematic biases (confident region), we can show an improvement of the assembly from Q66.7 to Q70.1 on average. We also show improvement in every single sample assessed.

Assembly qualities before and after polishing for 180 samples. For each sample, the genome is separated by the parental origin (the copy of the genome transmitted by father or mother) indicated as Haplotype (Hap) 1 or 2, and the assessed quality of those haplotypes.

DeepPolisher is already being used to improve genomics resources for the scientific community. In May, the HPRC announced their second data release, which included sequenced genome assemblies on 232 individuals, a fivefold increase over the first release. The data in the second release underwent an additional polishing step with DeepPolisher that reduced single nucleotide and indel errors twofold, leading to an extremely low error rate of less than one base error in half a million assembled bases.

By providing DeepPolisher as an open-source tool, our goal is to make the methods available broadly to the community. Working with the Human Pangenome Reference Consortium, we help enable scientists to more accurately diagnose genetic diseases for individuals of all ancestries.

This blog post demonstrates Google’s contribution to the development of DeepPolisher for improving the quality of genome assemblies. Integrating DeepPolisher in the broader context of generating highly accurate pangenome references involves contributions from nearly 195 authors from 68 different organizations. We thank the research groups from UCSC Genomics Institute (GI) under Professor Benedict Paten and Professor Karen Miga for helping in primary analysis and development directions of DeepPolisher. We acknowledge Mira Mastoras and Mobin Asri for leading the core analysis and integration of DeepPolisher to the pangenome generation pipeline. We thank the Google technical contributors: Pi-Chuan Chang, Daniel E. Cook, Alexey Kolesnikov, Lucas Brambrink, and Maria Nattestad. We thank Lizzie Dorfman, Dale Webster, and Katherine Chou for strategic leadership, and Monique Brouillette for help in writing.

November 4, 2025

October 31, 2025

October 27, 2025"
MLE-STAR: A state-of-the-art machine learning engineering agent,https://research.google/blog/mle-star-a-state-of-the-art-machine-learning-engineering-agents/,"Jinsung Yoon, Research Scientist, and Jaehyun Nam, Student Researcher, Google Cloud

MLE-STAR is a state-of-the-art machine learning engineering agent capable of automating various machine learning tasks across diverse data modalities while achieving top performances.

The rise of machine learning (ML) has fueled the development of high-performance applications across a wide array of real-world scenarios, from tabular classification to image denoising . However, crafting these models remains an arduous endeavor for machine learning engineers, demanding extensive iterative experimentation and data engineering. To streamline these demanding workflows, recent investigations have concentrated on leveraging large language models (LLMs) as machine learning engineering (MLE) agents. By capitalizing on their inherent coding and reasoning skills, these agents conceptualize ML tasks as code optimization challenges. They then explore potential code solutions, ultimately generating executable code (such as a Python script) based on a provided task description and datasets.

ML engineering agents are built to tackle diverse machine learning challenges by analyzing a task description and datasets that can span various modalities. Their ultimate goal is to pinpoint the best solution for the given problem.

Despite their promising initial strides, current MLE agents face several limitations that curtail their efficacy. First, their heavy reliance on pre-existing LLM knowledge often leads to a bias towards familiar and frequently used methods (e.g., the scikit-learn library for tabular data), overlooking potentially superior task-specific approaches. Furthermore, these agents typically employ an exploration strategy that modifies the entire code structure simultaneously in each iteration. This frequently causes agents to prematurely shift focus to other stages (e.g., model selection or hyperparameter tuning) because they lack the capacity for deep, iterative exploration within specific pipeline components, such as exhaustively experimenting with different feature engineering options.

In our recent paper , we introduce MLE-STAR, a novel ML engineering agent that integrates web search and targeted code block refinement. Unlike alternatives, MLE-STAR tackles ML challenges by first searching the web for proper models to get a solid foundation. It then carefully improves this foundation by testing which parts of the code are most important. MLE-STAR also utilizes a new method to blend several models together for even better results. This approach is very successful — it won medals in 63% of the Kaggle competitions in MLE-Bench-Lite, significantly outperforming the alternatives.

To generate initial solution code, MLE-STAR uses web search to retrieve relevant and potentially state-of-the-art approaches that could be effective for building a model. [da8046] To enhance the solution, MLE-STAR extracts a specific code block representing a distinct ML pipeline component, like feature engineering or ensemble building. It then concentrates on exploring strategies tailored to that component, reflecting on previous attempts as feedback. To identify the code block with the most significant impact on performance, MLE-STAR conducts an ablation study that evaluates the contribution of each ML component. This refinement process is repeated, modifying various code blocks.

Overview. ( a ) MLE-STAR begins by using web search to find and incorporate task-specific models into an initial solution. ( b ) For each refinement step, it conducts an ablation study to pinpoint the code block with the most significant impact on performance. ( c ) The identified code block then undergoes iterative refinement based on LLM-suggested plans, which explore various strategies using feedback from prior experiments. This process of selecting and refining target code blocks repeats, where the improved solution from ( c ) becomes the starting point for the next refinement step in ( b ).

Additionally, we present a novel method for generating ensembles. MLE-STAR first proposes multiple candidate solutions. Then, instead of relying on a simple voting mechanism based on validation scores, MLE-STAR merges these candidates into a single, improved solution using an ensemble strategy proposed by the agent itself. This ensemble strategy is iteratively refined based on the performance of the preceding strategies.

Ensembling Solutions: MLE-STAR refines its ensemble strategies over successive attempts, efficiently combining multiple parallel-generated solutions into a single, improved solution.

Last but not least, MLE-STAR incorporates three additional modules to enhance its robustness: (i) a debugging agent, (ii) a data leakage checker, and (iii) a data usage checker. For the debugging agent, if the execution of a Python script triggers an error, leading to a record (such as a traceback), MLE-STAR employs a debugging module to attempt correction. Regarding the data leakage checker, we've observed that LLM-generated Python scripts carry the risk of introducing data leakage, for instance, by improperly accessing information from a test dataset during training data preparation. To address this, we've introduced a checker agent that analyzes the solution script prior to its execution. As for the data usage checker, we've noticed that LLM-generated scripts sometimes neglect to use all provided data sources, focusing solely on simple formats like CSVs. To ensure the utilization of all relevant provided data, MLE-STAR includes a data usage checker agent.

To validate its effectiveness, we conducted comprehensive evaluations of MLE-STAR using the Kaggle competitions within MLE-Bench-Lite . Here, we utilized an additional agent that takes the task description and the final solution as input, and outputs the code that incorporates loading the test sample and creating a submission file.

Main results from MLE-Bench-Lite. Scores represent the average % of achievements in Kaggle competitions in MLE-Bench-Lite.

The experimental results presented in the figure above demonstrate that MLE-STAR, requiring only minimal human effort (e.g., defining initial prompts that are generalizable to any tasks), significantly outperforms previous alternatives, including those necessitating manual labor to collect strategies from Kaggle. Specifically, MLE-STAR achieves a substantial gain in any medal achievement, improving it from 25.8% to 63.6% when compared to the top-performing baseline .

To understand the sources of MLE-STAR's performance gains, we conducted several analyses from various perspectives. Here, we examined (i) the types of ML models that MLE-STAR utilizes, (ii) how MLE-STAR can be extended with human intervention, and (iii) how the additional data leakage and usage checkers further improve MLE-STAR's performance.

Left: Model usage (%) in image classification competitions. Right: Demonstrating human intervention: MLE-STAR integrates a model's training code based on a manual model description.

Left: MLE-STAR's data leakage checker ensures appropriate preprocessing. Right: MLE-STAR's data usage checker identifies and incorporates previously unused information.

We proposed MLE-STAR, a novel machine learning engineering agent designed for diverse ML tasks. Our core idea is to utilize web search to retrieve effective models and then explore various strategies targeting specific ML pipeline components to improve the solution. The effectiveness of MLE-STAR is validated by winning medals in 63% (36% of which are gold medals) of the MLE-Bench-Lite Kaggle competitions.

By automating complex ML tasks, MLE-STAR could lower the barrier to entry for individuals and organizations seeking to leverage ML, potentially fostering innovation across various sectors. Furthermore, as state-of-the-art models are continually updated and improved, the performance of solutions generated by MLE-STAR is expected to automatically boost. This is because our framework leverages a search engine to retrieve effective models from the web to form its solutions. This inherent adaptability ensures that MLE-STAR continues to provide increasingly better solutions as the field of ML advances. Last but not least, developers and researchers can now accelerate their machine learning projects by using our newly released open-source codebase of MLE-STAR, built with the Agent Development Kit (ADK).

We gratefully acknowledge the contributions of Jiefeng Chen, Jinwoo Shin, Sercan O Arik, Raj Sinha, and Tomas Pfister.

The MLE-STAR is currently intended for research purposes only, and the expectation is for the user to verify that the models and other content sourced by MLE-STAR adhere to appropriate licensing restrictions.

November 7, 2025

November 6, 2025

October 29, 2025"
